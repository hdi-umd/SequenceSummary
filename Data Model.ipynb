{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that represents a point event\n",
    "class PointEvent:\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent:\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe(src, local, sep, header=[], encoding=\"UTF-8\"):\n",
    "    if local == False:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') # was data.csv\n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep, encoding=encoding)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header, encoding=encoding)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(src, sep, encoding=encoding)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header, encoding=encoding)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# header is list of column names if they are not provided in the dataset\n",
    "def importPointEvents(src, timestampColumnIdx, timeFormat, sep, local, header=[], encoding=\"UTF-8\"):\n",
    "    events = []\n",
    "    df = get_dataframe(src, local, sep, header, encoding)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "        # for all attributes other tahn time, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != timestampColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = PointEvent(timestamp, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "def importIntervalEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep, local, header=[], encoding=\"UTF-8\"):\n",
    "    events = []\n",
    "    df = get_dataframe(src, local, sep, header, encoding)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        # create datetime object for the start and end times of the event\n",
    "        t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "        t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Given a list of events and an attribute, group the events by the value of said attribute\n",
    "# Returns a dictionary, keys are attribute values and values are lists of events that have that attribute value\n",
    "def groupEventsBy(eventList, attributeName):\n",
    "    grouped_by = {}\n",
    "    for event in eventList:\n",
    "        value = event.attributes[attributeName]\n",
    "        # If have seen this value before, append it the list of events in grouped_by for value\n",
    "        if value in grouped_by:\n",
    "            grouped_by[value].append(event)\n",
    "        # otherwise store a new list with just that event\n",
    "        else:\n",
    "            grouped_by[value] = [event]\n",
    "    return grouped_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-01 00:43:28 {'car-id': '20154301124328-262', 'car-type': '4', 'gate-name': 'entrance3'}\n",
      "2015-05-01 01:03:48 {'car-id': '20154301124328-262', 'car-type': '4', 'gate-name': 'general-gate1'}\n",
      "2015-05-01 01:06:24 {'car-id': '20154301124328-262', 'car-type': '4', 'gate-name': 'ranger-stop2'}\n",
      "2015-05-01 01:09:25 {'car-id': '20154301124328-262', 'car-type': '4', 'gate-name': 'ranger-stop0'}\n",
      "2015-05-01 01:12:36 {'car-id': '20154301124328-262', 'car-type': '4', 'gate-name': 'general-gate2'}\n"
     ]
    }
   ],
   "source": [
    "# VAST mini challenge dataset\n",
    "url = \"http://vacommunity.org/tiki-download_file.php?fileId=492\"\n",
    "vast = importPointEvents(url, 0, '%Y-%m-%d %H:%M:%S', ',', local=False)\n",
    "\n",
    "# print the first few events\n",
    "for e in vast[:5]:\n",
    "    print(e.timestamp, e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-25 00:00:00 {'Glucose': 38, 'Meal': 'Sugar to treat'}\n",
      "2019-06-23 00:00:00 {'Glucose': 233, 'Meal': 'Dinner'}\n",
      "2019-06-23 00:00:00 {'Glucose': 52, 'Meal': 'Lunch'}\n",
      "2019-06-22 00:00:00 {'Glucose': 67, 'Meal': 'Sugar to treat'}\n",
      "2019-06-22 00:00:00 {'Glucose': 309, 'Meal': 'Breakfast'}\n",
      "2019-06-21 00:00:00 {'Glucose': 66, 'Meal': 'Sugar to treat'}\n",
      "2019-06-21 00:00:00 {'Glucose': 80, 'Meal': 'Lunch'}\n",
      "2019-06-21 00:00:00 {'Glucose': 168, 'Meal': 'Breakfast'}\n",
      "2019-06-20 00:00:00 {'Glucose': 171, 'Meal': 'Dinner'}\n",
      "2019-06-20 00:00:00 {'Glucose': 56, 'Meal': 'Sugar to treat'}\n"
     ]
    }
   ],
   "source": [
    "# Sequence braiding\n",
    "# NOTE: I deleted the last 4 rows of the dataset before loading it in since they did not look like evevnts \n",
    "# here is what one looked like \"Stirfry 100 120\"\n",
    "# NOTE: this data set only had dates not times\n",
    "sequence_braiding = importPointEvents('data/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "\n",
    "# print the first few events\n",
    "for e in sequence_braiding[:10]:\n",
    "    print(e.timestamp, e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-04-03 18:00:09 {'User ID': 470, 'Venue ID': '49bbd6c0f964a520f4531fe3', 'Venue category ID': '4bf58dd8d48988d127951735', 'Venue category name': 'Arts & Crafts Store', 'Latitude': 40.71981037548853, 'Longitude': -74.00258103213994, 'Timezone offset (minutes)': -240}\n",
      "2012-04-03 18:00:25 {'User ID': 979, 'Venue ID': '4a43c0aef964a520c6a61fe3', 'Venue category ID': '4bf58dd8d48988d1df941735', 'Venue category name': 'Bridge', 'Latitude': 40.606799581406435, 'Longitude': -74.04416981025437, 'Timezone offset (minutes)': -240}\n",
      "2012-04-03 18:02:24 {'User ID': 69, 'Venue ID': '4c5cc7b485a1e21e00d35711', 'Venue category ID': '4bf58dd8d48988d103941735', 'Venue category name': 'Home (private)', 'Latitude': 40.71616168484322, 'Longitude': -73.88307005845945, 'Timezone offset (minutes)': -240}\n",
      "2012-04-03 18:02:41 {'User ID': 395, 'Venue ID': '4bc7086715a7ef3bef9878da', 'Venue category ID': '4bf58dd8d48988d104941735', 'Venue category name': 'Medical Center', 'Latitude': 40.7451638, 'Longitude': -73.982518775, 'Timezone offset (minutes)': -240}\n",
      "2012-04-03 18:03:00 {'User ID': 87, 'Venue ID': '4cf2c5321d18a143951b5cec', 'Venue category ID': '4bf58dd8d48988d1cb941735', 'Venue category name': 'Food Truck', 'Latitude': 40.74010382743943, 'Longitude': -73.98965835571289, 'Timezone offset (minutes)': -240}\n"
     ]
    }
   ],
   "source": [
    "# Foursquare NYC\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "fs_nyc = importPointEvents('data/foursquare/cities/nyc.txt', 7, foursquare_time_format, sep='\\t', local=True, header=header, encoding='latin1')\n",
    "\n",
    "# print the first few events\n",
    "for e in fs_nyc[:5]:\n",
    "    print(e.timestamp, e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-04-03 18:17:18 {'User ID': 1541, 'Venue ID': '4f0fd5a8e4b03856eeb6c8cb', 'Venue category ID': '4bf58dd8d48988d10c951735', 'Venue category name': 'Cosmetics Shop', 'Latitude': 35.705101088587135, 'Longitude': 139.61959004402158, 'Timezone offset (minutes)': 540}\n",
      "2012-04-03 18:22:04 {'User ID': 868, 'Venue ID': '4b7b884ff964a5207d662fe3', 'Venue category ID': '4bf58dd8d48988d1d1941735', 'Venue category name': 'Ramen /  Noodle House', 'Latitude': 35.715581120393146, 'Longitude': 139.8003172874451, 'Timezone offset (minutes)': 540}\n",
      "2012-04-03 19:12:07 {'User ID': 114, 'Venue ID': '4c16fdda96040f477cc473a5', 'Venue category ID': '4d954b0ea243a5684a65b473', 'Venue category name': 'Convenience Store', 'Latitude': 35.714542173995646, 'Longitude': 139.4800649934587, 'Timezone offset (minutes)': 540}\n",
      "2012-04-03 19:12:13 {'User ID': 868, 'Venue ID': '4c178638c2dfc928651ea869', 'Venue category ID': '4bf58dd8d48988d118951735', 'Venue category name': 'Food & Drink Shop', 'Latitude': 35.72559198908874, 'Longitude': 139.7766325938853, 'Timezone offset (minutes)': 540}\n",
      "2012-04-03 19:18:23 {'User ID': 1458, 'Venue ID': '4f568309e4b071452e447afe', 'Venue category ID': '4f2a210c4b9023bd5841ed28', 'Venue category name': 'Housing Development', 'Latitude': 35.656083091901124, 'Longitude': 139.734045462721, 'Timezone offset (minutes)': 540}\n"
     ]
    }
   ],
   "source": [
    "# Foursquare tokyo\n",
    "fs_tokyo = importPointEvents('data/foursquare/cities/tokyo.txt', 7, foursquare_time_format, sep='\\t', local=True, header=header, encoding='latin1')\n",
    "\n",
    "# print the first few events\n",
    "for e in fs_tokyo[:5]:\n",
    "    print(e.timestamp, e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare global\n",
    "# There 33 million eventt for this dataset and so I was not able to get it the importPointEvents to fully run\n",
    "#header = [\"User ID\", \"Venue ID\", \"UTC time\", \"Timezone offset in minutes\"]\n",
    "#fs_gloabl = importPointEvents(\"data/foursquare/global_scale/checkins.txt\", 2, foursquare_time_format, sep='\\t', local=True, header=header, encoding='latin1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
