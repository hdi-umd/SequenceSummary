{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-452c80cb6755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData_Model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Research/code/data model/Data_Model.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m\"        # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;34m\"        if type(data[endTimeColumnIdx]) is float:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;34m\"            t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;34m\"            event_type = \\\"point\\\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;34m\"        # Otherwise its an interval event\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/code/data model/Data_Model.ipynb\u001b[0m in \u001b[0;36mimportPointEvents\u001b[0;34m(src, timestampColumnIdx, timeFormat, sep, local, header, df)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"            df = pd.read_csv(\\\"data.txt\\\", sep)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m\"        #else use header param for column names\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;34m\"        else:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;34m\"            df = pd.read_csv(\\\"data.txt\\\", sep, names=header)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"        # Delete the csv file\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/code/data model/Data_Model.ipynb\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(src, local, sep, header)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"        self.time = [t1,t2] \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"        # dictionary: key=attribute value=attribute value\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;34m\"        self.attributes = attributes \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m    ]\n\u001b[1;32m     46\u001b[0m   },\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Data_Model import get_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that represents a point event\n",
    "class PointEvent:\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent:\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe(src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# header is list of column names if they are not provided in the dataset\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importPointEvents(src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "        # for all attributes other tahn time, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != timestampColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = PointEvent(timestamp, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importIntervalEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        # create datetime object for the start and end times of the event\n",
    "        t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "        t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Import a dataset that has both interval and point events\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "def importMixedEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    #print(df)\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        #print(data)\n",
    "        attribs = {}\n",
    "        # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "        # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "        if type(data[endTimeColumnIdx]) is float:\n",
    "            t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            event_type = \"point\"\n",
    "        # Otherwise its an interval event\n",
    "        else:\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            event_type = \"interval\"\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "        attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "        for i in attribute_columns:\n",
    "            attribs[cols[i]] = data[i]\n",
    "        # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "        if event_type == \"point\":\n",
    "            e = PointEvent(t, attribs)\n",
    "        else:\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-57d0a50b12ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# VAST mini challenge dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://vacommunity.org/tiki-download_file.php?fileId=492\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportPointEvents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Sequence braiding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-2d8da1fe5a6b>\u001b[0m in \u001b[0;36mimportPointEvents\u001b[0;34m(src, timestampColumnIdx, timeFormat, sep, local, header, df)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# if the df is not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# For each event in the csv construct an event object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-2d8da1fe5a6b>\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(src, local, sep, header)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dl=0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dl=1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Download the CSV at url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0murl_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VAST mini challenge dataset\n",
    "url = \"http://vacommunity.org/tiki-download_file.php?fileId=492\"\n",
    "vast = importPointEvents(url, 0, '%Y-%m-%d %H:%M:%S', sep=',', local=False)\n",
    "\n",
    "# Sequence braiding\n",
    "# NOTE: I deleted the last 4 rows of the dataset before loading it in since they did not look like evevnts \n",
    "# NOTE: this data set only had dates not times\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "\n",
    "# Foursquare NYC\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "    \n",
    "# Foursquare tokyo\n",
    "df = pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "fs_tokyo = importPointEvents('datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "\n",
    "# CHICAGO-SeasonD2O.txt\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "header = [\"Game/Points\", \"EventType\", \"Start time\", \"End time\"]\n",
    "chicago_season_d2o = importMixedEvents(\"../datasets/Chicago_Bulls/CHICAGO-SeasonD2O.txt\", 2, 3, time_format, sep='\\t', local=True, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "# Group events by attributeName, and order them by timestamp\n",
    "def generateSequence(eventList, attributeName):\n",
    "    grouped_by = {}\n",
    "    # Sort the event list\n",
    "    eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "    for event in eventList:\n",
    "        value = event.attributes[attributeName]\n",
    "        # If have seen this value before, append it the list of events in grouped_by for value\n",
    "        if value in grouped_by:\n",
    "            grouped_by[value].append(event)\n",
    "        # otherwise store a new list with just that event\n",
    "        else:\n",
    "            grouped_by[value] = [event]\n",
    "    return list(grouped_by.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n",
    "# Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "# break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "# “week”, “month”, “quarter”, and “year”.\n",
    "# For interval events I used the start time of the event to determine its category when splitting it\n",
    "def splitSequences(sequenceList, timeUnit, record=None):\n",
    "    results = []\n",
    "    timeUnit = timeUnit.lower()\n",
    "    # Check if the time unit is a valid argument\n",
    "    valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "    if timeUnit not in valid_time_units:\n",
    "        raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "    # Sort the events by the timestamp or event start time\n",
    "    sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "    \n",
    "    # Process the event sequence based on the given time unit\n",
    "    # Generally, create a map for that time unit and then add each event into that map \n",
    "    # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "    if timeUnit == \"hour\":\n",
    "        hours = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.hour, time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,hours,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "        results = list(hours.values())\n",
    "    \n",
    "    elif timeUnit == \"day\":\n",
    "        days = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,days,event)\n",
    "            #print(days)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "        results = list(days.values())\n",
    "        \n",
    "    elif timeUnit == \"month\":\n",
    "        months = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.month,time.year)\n",
    "            insert_event_into_dict(key,months,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "        results = list(months.values())\n",
    "        \n",
    "    elif timeUnit == \"week\":\n",
    "        weeks = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            week_num = time.isocalendar()[1]\n",
    "            key = (year,week_num)\n",
    "            insert_event_into_dict(key,weeks,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "        results = list(weeks.values())\n",
    "                                   \n",
    "    elif timeUnit == \"year\":\n",
    "        years = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = time.year\n",
    "            insert_event_into_dict(key,years,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key)\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "        results = list(years.values())\n",
    "            \n",
    "    elif timeUnit == \"quarter\":\n",
    "        quarters = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            month = time.month\n",
    "            # Determine the year, quarter pair/key for quarter dict\n",
    "            # January, February, and March (Q1)\n",
    "            if month in range(1, 4):\n",
    "                key = (year, \"Q1\")\n",
    "            # April, May, and June (Q2)\n",
    "            elif month in range(4, 7):\n",
    "                key = (year, \"Q2\")\n",
    "            # July, August, and September (Q3)\n",
    "            elif month in range(7,10):\n",
    "                key = (year, \"Q3\")\n",
    "            # October, November, and December (Q4)\n",
    "            elif month in range(10,13):\n",
    "                key = (year, \"Q4\")\n",
    "            # Put the event in the dictionary\n",
    "            insert_event_into_dict(key,quarters,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "        results = list(quarters.values())\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to EventFLow Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataModel(eventlist, record_id, event_category,include_attr=None, file_name=\"Eventflow.txt\"):\n",
    "    #columns in EventFlow\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    \n",
    "    Start_time=None\n",
    "    End_time=None\n",
    "    data=[]\n",
    "    \n",
    "    #which keys will go to attributes list\n",
    "    #key_list=[keys for keys in eventlist[0].attributes.keys() if keys not in [record_id,event_category]]\n",
    "    key_list=include_attr\n",
    "    for event in eventlist:\n",
    "        # Get the attribute value of record    \n",
    "        if(event.type==\"point\"):    \n",
    "            Start_time=event.timestamp\n",
    "            End_time=\"\"\n",
    "        else:\n",
    "            Start_time=event.time[0]\n",
    "            End_time=event.time[1]\n",
    "        attr_str=\"\"\n",
    "        if include_attr is not None:\n",
    "            attr_str=\";\".join([str(keys).replace(\" \",\"_\")+\"=\\\"\"+str(event.attributes[keys])+\"\\\"\" for keys in key_list])\n",
    "        \n",
    "        data.append([(re.sub('[(),]','',str(event.attributes[record_id]))).replace(\" \",\"_\"),str(event.attributes[event_category]).replace(\" \",\"_\"),Start_time, End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate method signature\n",
    "#def createEventflowFormatdataframe(src, record_id_idx, event_category_idx, startTimeColumnIdx, endTimeColumnIdx, sep=\"\\t\", local=False, header=[],file_name=\"Eventflow.txt\"):\n",
    "    #events = []\n",
    "    # if the df is not provided\n",
    "    #if df is None:\n",
    "    #    df = get_dataframe(src, local, sep, header)\n",
    "    #cols = df.columns\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataframe(df, record_id_idx, event_category_idx, start_time_idx, end_time_idx,include_attr=False, file_name=\"Eventflow.txt\"):\n",
    "    \n",
    "\n",
    "    #columns in EventFlow\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    data=[]\n",
    "    key_list=[]\n",
    "    cols = df.columns\n",
    "    \n",
    "    for i in range(0,len(cols)):\n",
    "        if i not in [record_id_idx, event_category_idx, start_time_idx, end_time_idx]:\n",
    "            key_list.append(cols[i])\n",
    "    print(key_list)\n",
    "    for row in df.iterrows():\n",
    "        event = row[1]\n",
    "        if type(event[end_time_idx]) is float:\n",
    "            End_time = \"\"\n",
    "        else:\n",
    "            End_time = event[end_time_idx]\n",
    "        attr_str=\"\"\n",
    "        if(include_attr==True):\n",
    "            attr_str=\";\".join([(re.sub('[(),]','',str(keys))).replace(\" \",\"_\")+\"=\\\"\"+str(event[keys])+\"\\\"\" for keys in key_list])\n",
    "        \n",
    "        data.append([(re.sub('[(),]','',str(event[record_id_idx]))).replace(\" \",\"_\"),(re.sub('[(),]','',str(event[event_category_idx]))).replace(\" \",\"_\"),event[start_time_idx], End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, the record distinctions are not present, so we add record_id\n",
    "\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "splitSequences(sequence_braiding, \"week\")\n",
    "event_df=createEventflowFormatdataModel(sequence_braiding, \"record\", \"Meal\", \"EventFlow.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case data doesn't have header, so we add headers\n",
    "header = [\"record_id\", \"EventType\", \"Start time\", \"End time\",\"attribute\"]\n",
    "children_hospital = importMixedEvents('../datasets/Children Hospital/DND-ChildrensDemo-06-26-13.txt', 2,3, \"%Y-%m-%d %H:%M:%S.%f\", sep='\\t', local=True, header= header)\n",
    "splitSequences(children_hospital, \"hour\",\"record_id\")\n",
    "event_df=createEventflowFormatdataModel(children_hospital, \"record_id\", \"EventType\", \"EventFlow_CH.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender', 'Heart Rate', 'Speed', 'SportID']\n"
     ]
    }
   ],
   "source": [
    "#Loading from dataframe\n",
    "endomodo = get_dataframe(\"../datasets/endomondo/Endo_time_clean.txt\", local=True, sep=\" \", header=[])\n",
    "#print(endomodo)\n",
    "event_df=createEventflowFormatdataframe(endomodo, 0, 1,2,3, \"EventFlow_Endo_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../coronanet_shortened.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b1cba709a5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorona_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../coronanet_shortened.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(corona_net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#splitSequences(sequence_braiding, \"day\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mevent_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreateEventflowFormatdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorona_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"EventFlow_corona_subset.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2d8da1fe5a6b>\u001b[0m in \u001b[0;36mget_dataframe\u001b[0;34m(src, local, sep, header)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# If header list is empty, the dataset provides header so ignore param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# else use header param for column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../coronanet_shortened.csv'"
     ]
    }
   ],
   "source": [
    "corona_net = get_dataframe('../coronanet_shortened.csv', local=True, sep=\",\", header=[])\n",
    "#print(corona_net)\n",
    "#splitSequences(sequence_braiding, \"day\")\n",
    "event_df=createEventflowFormatdataframe(corona_net,8,10,6,7,  \"EventFlow_corona_subset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy_id', 'entry_type', 'correct_type', 'update_type', 'update_level', 'date_announced', 'init_country_level', 'type_sub_cat']\n"
     ]
    }
   ],
   "source": [
    "corona_net = get_dataframe('../datasets/coronanet/coronanet_subset_shortened.csv', local=True, sep=\",\", header=[])\n",
    "#print(corona_net)\n",
    "#splitSequences(sequence_braiding, \"day\")\n",
    "event_df=createEventflowFormatdataframe(corona_net,8,10,6,7, file_name= \"EventFlow_corona_subset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Event Category', 'srcdocnum', 'apptype', 'mainclass', 'docval']\n"
     ]
    }
   ],
   "source": [
    "#header = [\"record_id\", \"EventType\", \"Start time\", \"End time\",\"attribute\"]\n",
    "invention_trajectories=get_dataframe('../datasets/Invention Trajectories/invention_expanded.csv', local=True, sep=\",\")\n",
    "invention_trajectories=invention_trajectories[invention_trajectories[\"docsource\"]!='\"\"']\n",
    "#invention_trajectories=invention_trajectories[\"mainclass\"!=\"\"]\n",
    "event_df=createEventflowFormatdataframe(invention_trajectories,0,4,2,3,  \"EventFlow_invention_docsrc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, the record distinctions are not present, so we add record_id\n",
    "\n",
    "endomodo = importMixedEvents('../Endo_time_clean.txt', 2,3, \"%Y-%m-%d %H:%M:%S\", sep=' ', local=True)\n",
    "splitSequences(endomodo, \"month\",\"UserID\")\n",
    "event_df=createEventflowFormatdataModel(endomodo, \"UserID\", \"Sport\", \"_endo_month.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_nyc, \"quarter\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",False, \"EventFlow_fs_nyc_quarter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare NY weekly\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/NYC_shortened.csv', ',', encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/NYC_shortened.csv', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_nyc, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",False, \"shortened_fs_nyc_week.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare tokyo weekly\n",
    "df = pd.read_csv('../datasets/foursquare/TKY_shortened.csv', ',', encoding='latin1')\n",
    "fs_tokyo = importPointEvents('datasets/foursquare/TKY_shortened.csv', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_tokyo, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",False, \"shortened_fs_tky_week.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                                                     Afghanistan\n",
       "type                                  Closure and Regulation of Schools\n",
       "date_start                                                   2020-03-14\n",
       "date_end                                                     2020-04-21\n",
       "policy_id                                                       3558044\n",
       "entry_type                                                       update\n",
       "correct_type                                                   original\n",
       "update_type                                            Change of Policy\n",
       "update_level                                              Strengthening\n",
       "date_announced                                               2020-03-14\n",
       "init_country_level                                             National\n",
       "type_sub_cat          Primary Schools (generally for children ages 1...\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_net.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glucose\n",
      "Meal\n",
      "record\n"
     ]
    }
   ],
   "source": [
    "for key_ in sequence_braiding[0].attributes.keys():\n",
    "    print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Restaurant': 'Restaurant', 'Japanese Restaurant': 'Restaurant', 'French Restaurant': 'Restaurant', 'Chinese Restaurant': 'Restaurant', 'Korean Restaurant': 'Restaurant', 'Asian Restaurant': 'Restaurant', 'Italian Restaurant': 'Restaurant', 'Indian Restaurant': 'Restaurant', 'Sushi Restaurant': 'Restaurant', 'Dumpling Restaurant': 'Restaurant', 'Thai Restaurant': 'Restaurant', 'Seafood Restaurant': 'Restaurant', 'American Restaurant': 'Restaurant', 'Eastern European Restaurant': 'Restaurant', 'Caribbean Restaurant': 'Restaurant', 'Spanish Restaurant': 'Restaurant', 'Vegetarian / Vegan Restaurant': 'Restaurant', 'German Restaurant': 'Restaurant', 'Vietnamese Restaurant': 'Restaurant', 'Swiss Restaurant': 'Restaurant', 'Dim Sum Restaurant': 'Restaurant', 'Tapas Restaurant': 'Restaurant', 'Mexican Restaurant': 'Restaurant', 'Turkish Restaurant': 'Restaurant', 'Mediterranean Restaurant': 'Restaurant', 'Latin American Restaurant': 'Restaurant', 'Brazilian Restaurant': 'Restaurant', 'Ethiopian Restaurant': 'Restaurant', 'Middle Eastern Restaurant': 'Restaurant', 'Peruvian Restaurant': 'Restaurant', 'Moroccan Restaurant': 'Restaurant', 'Cuban Restaurant': 'Restaurant', 'Portuguese Restaurant': 'Restaurant', 'Scandinavian Restaurant': 'Restaurant', 'African Restaurant': 'Restaurant', 'Malaysian Restaurant': 'Restaurant', 'Southern / Soul Food Restaurant': 'Restaurant', 'Argentinian Restaurant': 'Restaurant', 'Cajun / Creole Restaurant': 'Restaurant', 'South American Restaurant': 'Restaurant', 'Arepa Restaurant': 'Restaurant', 'Gluten-free Restaurant': 'Restaurant', 'Afghan Restaurant': 'Restaurant', 'Falafel Restaurant': 'Restaurant', 'Australian Restaurant': 'Restaurant', 'Diner': 'Restaurant', 'Steakhouse': 'Restaurant', 'Store': 'Store', 'Convenience Store': 'Store', 'Furniture / Home Store': 'Store', 'Candy Store': 'Store', 'Toy / Game Store': 'Store', 'Electronics Store': 'Store', 'Department Store': 'Store', 'Arts & Crafts Store': 'Store', 'Bookstore': 'Store', 'Clothing Store': 'Store', 'Drugstore / Pharmacy': 'Store', 'Camera Store': 'Store', 'Music Store': 'Store', 'Hardware Store': 'Store', 'Paper / Office Supplies Store': 'Store', 'Video Game Store': 'Store', 'Pet Store': 'Store', 'Thrift / Vintage Store': 'Store', 'Jewelry Store': 'Store', 'Video Store': 'Store', 'Cosmetics Shop': 'Shop', 'Flower Shop': 'Shop', 'Miscellaneous Shop': 'Shop', 'Smoke Shop': 'Shop', 'Automotive Shop': 'Shop', 'Mobile Phone Shop': 'Shop', 'Shop & Service': 'Shop', 'Hobby Shop': 'Shop', 'Gift Shop': 'Shop', 'Bike Shop': 'Shop', 'Record Shop': 'Shop', 'Sporting Goods Shop': 'Shop', 'Antique Shop': 'Shop', 'Board Shop': 'Shop', 'Motorcycle Shop': 'Shop', 'Bridal Shop': 'Shop', 'Fast Food Restaurant': 'Food and Snack', 'Coffee Shop': 'Food and Snack', 'Donut Shop': 'Food and Snack', 'Dessert Shop': 'Food and Snack', 'Food & Drink Shop': 'Food and Snack', 'Ice Cream Shop': 'Food and Snack', 'Bagel Shop': 'Food and Snack', 'Cupcake Shop': 'Food and Snack', 'Bakery': 'Food and Snack', 'Fish & Chips Shop': 'Food and Snack', 'Breakfast Spot': 'Food and Snack', 'Burrito Place': 'Food and Snack', 'Taco Place': 'Food and Snack', 'Salad Place': 'Food and Snack', 'Pizza Place': 'Food and Snack', 'Soup Place': 'Food and Snack', 'Sandwich Place': 'Food and Snack', 'Snack Place': 'Food and Snack', 'Mac & Cheese Joint': 'Food and Snack', 'Fried Chicken Joint': 'Food and Snack', 'BBQ Joint': 'Food and Snack', 'Burger Joint': 'Food and Snack', 'Hot Dog Joint': 'Food and Snack', 'Wings Joint': 'Food and Snack', 'Deli / Bodega': 'Food and Snack', 'Gaming Cafe': 'Cafe', 'Internet Cafe': 'Cafe', 'Café': 'Cafe', 'Salon / Barbershop': 'Salon', 'Tanning Salon': 'Salon', 'Nail Salon': 'Salon', 'Spa / Massage': 'Salon', 'Fraternity House': 'Fraternity', 'Sorority House': 'Fraternity', 'General College & University': 'General College & University', 'University': 'General College & University', 'Community College': 'General College & University', 'College & University': 'General College & University', 'College Stadium': 'General College & University', 'College Theater': 'General College & University', 'College Academic Building': 'General College & University', 'Student Center': 'General College & University', 'Music School': 'Speciality Schools', 'Law School': 'Speciality Schools', 'Trade School': 'Speciality Schools', 'Medical School': 'Speciality Schools', 'Nursery School': 'Schools', 'Elementary School': 'Schools', 'School': 'Schools', 'Middle School': 'Schools', 'High School': 'Schools', 'Distillery': 'Alcohol', 'Winery': 'Alcohol', 'Brewery': 'Alcohol', 'Gastropub': 'Alcohol', 'Beer Garden': 'Alcohol', 'History Museum': 'Museum', 'Museum': 'Museum', 'Art Museum': 'Museum', 'Science Museum': 'Museum', 'Outdoors & Recreation': 'Outdoor', 'Other Great Outdoors': 'Outdoor', 'Sculpture Garden': 'Outdoor', 'Scenic Lookout': 'Outdoor', 'Campground': 'Outdoor', 'Garden': 'Outdoor', 'Shrine': 'Spiritual', 'Temple': 'Spiritual', 'Mosque': 'Spiritual', 'Synagogue': 'Spiritual', 'Church': 'Spiritual', 'Spiritual Center': 'Spiritual', 'Arts & Entertainment': 'Arts & Entertainment', 'Public Art': 'Arts & Entertainment', 'Performing Arts Venue': 'Arts & Entertainment', 'Art Gallery': 'Arts & Entertainment', 'Music Venue': 'Arts & Entertainment', 'Concert Hall': 'Arts & Entertainment', 'Theater': 'Arts & Entertainment', 'Movie Theater': 'Arts & Entertainment', 'Comedy Club': 'Arts & Entertainment', 'General Entertainment': 'General Entertainment', 'Casino': 'General Entertainment', 'Racetrack': 'General Entertainment', 'Event Space': 'Event Space', 'Convention Center': 'Event Space', 'General Travel': 'Travel & Transport', 'Travel & Transport': 'Travel & Transport', 'Travel Lounge': 'Travel & Transport', 'Taxi': 'Travel & Transport', 'Airport': 'Travel & Transport', 'Bus Station': 'Travel & Transport', 'Light Rail': 'Travel & Transport', 'Ferry': 'Travel & Transport', 'Bridge': 'Travel & Transport', 'Road': 'Travel & Transport', 'Rest Area': 'Travel & Transport', 'Train Station': 'Railway', 'Subway': 'Railway', 'Nightlife Spot': 'Nightlife', 'Other Nightlife': 'Nightlife', 'Plaza': 'Mall', 'Mall': 'Mall', 'Aquarium': 'Recreation', 'Planetarium': 'Recreation', 'Park': 'Recreation', 'Zoo': 'Recreation', 'Playground': 'Recreation', 'Pool Hall': 'Indoor Games', 'Arcade': 'Indoor Games', 'Bowling Alley': 'Indoor Games'}\n"
     ]
    }
   ],
   "source": [
    "#Event aggregation\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df =  pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcategory\"]=df[\"Venue category name\"]\n",
    "fs_tokyo = importPointEvents('../datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_tokyo,\"../datasets/foursquare/tky_dict_final.txt\", \"Venue category name\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",include_attr=[\"subcategory\"], file_name= \"aggr_fs_tky.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Restaurant': 'Restaurant', 'Japanese Restaurant': 'Restaurant', 'French Restaurant': 'Restaurant', 'Chinese Restaurant': 'Restaurant', 'Korean Restaurant': 'Restaurant', 'Asian Restaurant': 'Restaurant', 'Italian Restaurant': 'Restaurant', 'Indian Restaurant': 'Restaurant', 'Sushi Restaurant': 'Restaurant', 'Dumpling Restaurant': 'Restaurant', 'Thai Restaurant': 'Restaurant', 'Seafood Restaurant': 'Restaurant', 'American Restaurant': 'Restaurant', 'Eastern European Restaurant': 'Restaurant', 'Caribbean Restaurant': 'Restaurant', 'Spanish Restaurant': 'Restaurant', 'Vegetarian / Vegan Restaurant': 'Restaurant', 'German Restaurant': 'Restaurant', 'Vietnamese Restaurant': 'Restaurant', 'Swiss Restaurant': 'Restaurant', 'Dim Sum Restaurant': 'Restaurant', 'Tapas Restaurant': 'Restaurant', 'Mexican Restaurant': 'Restaurant', 'Turkish Restaurant': 'Restaurant', 'Mediterranean Restaurant': 'Restaurant', 'Latin American Restaurant': 'Restaurant', 'Brazilian Restaurant': 'Restaurant', 'Ethiopian Restaurant': 'Restaurant', 'Middle Eastern Restaurant': 'Restaurant', 'Peruvian Restaurant': 'Restaurant', 'Moroccan Restaurant': 'Restaurant', 'Cuban Restaurant': 'Restaurant', 'Portuguese Restaurant': 'Restaurant', 'Scandinavian Restaurant': 'Restaurant', 'African Restaurant': 'Restaurant', 'Malaysian Restaurant': 'Restaurant', 'Southern / Soul Food Restaurant': 'Restaurant', 'Argentinian Restaurant': 'Restaurant', 'Cajun / Creole Restaurant': 'Restaurant', 'South American Restaurant': 'Restaurant', 'Arepa Restaurant': 'Restaurant', 'Gluten-free Restaurant': 'Restaurant', 'Afghan Restaurant': 'Restaurant', 'Falafel Restaurant': 'Restaurant', 'Australian Restaurant': 'Restaurant', 'Filipino Restaurant': 'Restaurant', 'Diner': 'Restaurant', 'Steakhouse': 'Restaurant', 'Greek Restaurant': 'Restaurant', 'Molecular Gastronomy Restaurant': 'Restaurant', 'Store': 'Store', 'Convenience Store': 'Store', 'Furniture / Home Store': 'Store', 'Candy Store': 'Store', 'Toy / Game Store': 'Store', 'Electronics Store': 'Store', 'Department Store': 'Store', 'Arts & Crafts Store': 'Store', 'Bookstore': 'Store', 'Clothing Store': 'Store', 'Drugstore / Pharmacy': 'Store', 'Camera Store': 'Store', 'Music Store': 'Store', 'Hardware Store': 'Store', 'Paper / Office Supplies Store': 'Store', 'Video Game Store': 'Store', 'Pet Store': 'Store', 'Thrift / Vintage Store': 'Store', 'Jewelry Store': 'Store', 'Video Store': 'Store', 'Cosmetics Shop': 'Shop', 'Flower Shop': 'Shop', 'Miscellaneous Shop': 'Shop', 'Smoke Shop': 'Shop', 'Automotive Shop': 'Shop', 'Mobile Phone Shop': 'Shop', 'Shop & Service': 'Shop', 'Hobby Shop': 'Shop', 'Gift Shop': 'Shop', 'Bike Shop': 'Shop', 'Record Shop': 'Shop', 'Sporting Goods Shop': 'Shop', 'Antique Shop': 'Shop', 'Board Shop': 'Shop', 'Motorcycle Shop': 'Shop', 'Bridal Shop': 'Shop', 'Fast Food Restaurant': 'Fast Food', 'Coffee Shop': 'Fast Food', 'Donut Shop': 'Fast Food', 'Dessert Shop': 'Fast Food', 'Food & Drink Shop': 'Fast Food', 'Ice Cream Shop': 'Fast Food', 'Bagel Shop': 'Fast Food', 'Cupcake Shop': 'Fast Food', 'Bakery': 'Fast Food', 'Fish & Chips Shop': 'Fast Food', 'Breakfast Spot': 'Fast Food', 'Burrito Place': 'Fast Food', 'Taco Place': 'Fast Food', 'Salad Place': 'Fast Food', 'Pizza Place': 'Fast Food', 'Soup Place': 'Fast Food', 'Sandwich Place': 'Fast Food', 'Snack Place': 'Fast Food', 'Mac & Cheese Joint': 'Fast Food', 'Fried Chicken Joint': 'Fast Food', 'BBQ Joint': 'Fast Food', 'Burger Joint': 'Fast Food', 'Hot Dog Joint': 'Fast Food', 'Wings Joint': 'Fast Food', 'Deli / Bodega': 'Fast Food', 'Gaming Cafe': 'Cafe', 'Internet Cafe': 'Cafe', 'Café': 'Cafe', 'Salon / Barbershop': 'Salon', 'Tanning Salon': 'Salon', 'Nail Salon': 'Salon', 'Spa / Massage': 'Salon', 'Fraternity House': 'Fraternity', 'Sorority House': 'Fraternity', 'General College & University': 'General College & University', 'University': 'General College & University', 'Community College': 'General College & University', 'College & University': 'General College & University', 'College Stadium': 'General College & University', 'College Theater': 'General College & University', 'College Academic Building': 'General College & University', 'Student Center': 'General College & University', 'Music School': 'Speciality Schools', 'Law School': 'Speciality Schools', 'Trade School': 'Speciality Schools', 'Medical School': 'Speciality Schools', 'School': 'Schools', 'Nursery School': 'Schools', 'Elementary School': 'Schools', 'Middle School': 'Schools', 'High School': 'Schools', 'Distillery': 'Alcohol', 'Winery': 'Alcohol', 'Brewery': 'Alcohol', 'Gastropub': 'Alcohol', 'Beer Garden': 'Alcohol', 'Bar': 'Alcohol', 'History Museum': 'Museum', 'Museum': 'Museum', 'Art Museum': 'Museum', 'Science Museum': 'Museum', 'Outdoors & Recreation': 'Outdoor', 'Other Great Outdoors': 'Outdoor', 'Sculpture Garden': 'Outdoor', 'Scenic Lookout': 'Outdoor', 'Campground': 'Outdoor', 'Garden': 'Outdoor', 'Shrine': 'Spiritual', 'Temple': 'Spiritual', 'Mosque': 'Spiritual', 'Synagogue': 'Spiritual', 'Church': 'Spiritual', 'Spiritual Center': 'Spiritual', 'Arts & Entertainment': 'Arts & Entertainment', 'Public Art': 'Arts & Entertainment', 'Performing Arts Venue': 'Arts & Entertainment', 'Art Gallery': 'Arts & Entertainment', 'Music Venue': 'Arts & Entertainment', 'Concert Hall': 'Arts & Entertainment', 'Theater': 'Arts & Entertainment', 'Movie Theater': 'Arts & Entertainment', 'Comedy Club': 'Arts & Entertainment', 'Casino': 'General Entertainment', 'Racetrack': 'General Entertainment', 'Convention Center': 'Event Space', 'Event Space': 'Event Space', 'Travel & Transport': 'Travel & Transport', 'General Travel': 'Travel & Transport', 'Travel Lounge': 'Travel & Transport', 'Taxi': 'Travel & Transport', 'Airport': 'Travel & Transport', 'Bus Station': 'Travel & Transport', 'Light Rail': 'Travel & Transport', 'Ferry': 'Travel & Transport', 'Bridge': 'Travel & Transport', 'Road': 'Travel & Transport', 'Rest Area': 'Travel & Transport', 'Train Station': 'Railway', 'Subway': 'Railway', 'Nightlife Spot': 'Nightlife', 'Other Nightlife': 'Nightlife', 'Plaza': 'Mall', 'Mall': 'Mall', 'Planetarium': 'Recreation', 'Park': 'Recreation', 'Zoo': 'Recreation', 'Playground': 'Recreation', 'Pool Hall': 'Indoor Games', 'Arcade': 'Indoor Games', 'Bowling Alley': 'Indoor Games'}\n"
     ]
    }
   ],
   "source": [
    "#Event aggregation\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', names=header, encoding='latin1', sep='\\t')\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_nyc,\"../datasets/foursquare/ny_dict_final.txt\", \"Venue category name\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",include_attr=[\"subcategory\"],file_name= \"aggr_fs_nyc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID\n",
      "Venue ID\n",
      "Venue category ID\n",
      "Venue category name\n",
      "Latitude\n",
      "Longitude\n",
      "Timezone offset\n"
     ]
    }
   ],
   "source": [
    "for key_ in fs_tokyo[0].attributes.keys():\n",
    "    print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Glucose': 38, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 233, 'Meal': 'Supper'}\n",
      "{'Glucose': 52, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 67, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 309, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 66, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 80, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 168, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 171, 'Meal': 'Supper'}\n",
      "{'Glucose': 56, 'Meal': 'Sugar to treat'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of aggregateEventsDict with test file\n",
    "# Rules in the test file:\n",
    "# Dinner will become Supper \n",
    "# Lunch and Breakfast become Not Dinner\n",
    "sb = aggregateEventsDict(sequence_braiding, \"testFiles/aggregateEventsDictTestFileSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Glucose': 38, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 233, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 52, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 67, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 309, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 66, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 80, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 168, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 171, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 56, 'Meal': 'SNACK TIME!'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of aggregateEventsRegex with test file\n",
    "# Rules in the test file:\n",
    "# Lunch|Dinner|Breakfast -> Must eat times\n",
    "# S.* -> SNACK TIME!\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "sb = aggregateEventsRegex(sequence_braiding, \"testFiles/aggregateEventsRegexSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
