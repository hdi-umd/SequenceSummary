{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that represents a point event\n",
    "class PointEvent:\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent:\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe(src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# header is list of column names if they are not provided in the dataset\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importPointEvents(src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "        # for all attributes other tahn time, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != timestampColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = PointEvent(timestamp, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importIntervalEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        # create datetime object for the start and end times of the event\n",
    "        t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "        t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Import a dataset that has both interval and point events\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "def importMixedEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    #print(df)\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        #print(data)\n",
    "        attribs = {}\n",
    "        # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "        # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "        if type(data[endTimeColumnIdx]) is float:\n",
    "            t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            event_type = \"point\"\n",
    "        # Otherwise its an interval event\n",
    "        else:\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            event_type = \"interval\"\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "        attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "        for i in attribute_columns:\n",
    "            attribs[cols[i]] = data[i]\n",
    "        # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "        if event_type == \"point\":\n",
    "            e = PointEvent(t, attribs)\n",
    "        else:\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-57d0a50b12ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# VAST mini challenge dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://vacommunity.org/tiki-download_file.php?fileId=492\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportPointEvents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Sequence braiding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2d8da1fe5a6b>\u001b[0m in \u001b[0;36mimportPointEvents\u001b[0;34m(src, timestampColumnIdx, timeFormat, sep, local, header, df)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestampColumnIdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# for all attributes other tahn time, add them to attributes dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtimestampColumnIdx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mattribs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \"\"\"\n\u001b[1;32m    597\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VAST mini challenge dataset\n",
    "url = \"http://vacommunity.org/tiki-download_file.php?fileId=492\"\n",
    "vast = importPointEvents(url, 0, '%Y-%m-%d %H:%M:%S', sep=',', local=False)\n",
    "\n",
    "# Sequence braiding\n",
    "# NOTE: I deleted the last 4 rows of the dataset before loading it in since they did not look like evevnts \n",
    "# NOTE: this data set only had dates not times\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "\n",
    "# Foursquare NYC\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "    \n",
    "# Foursquare tokyo\n",
    "df = pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "fs_tokyo = importPointEvents('datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "\n",
    "# CHICAGO-SeasonD2O.txt\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "header = [\"Game/Points\", \"EventType\", \"Start time\", \"End time\"]\n",
    "chicago_season_d2o = importMixedEvents(\"../datasets/Chicago_Bulls/CHICAGO-SeasonD2O.txt\", 2, 3, time_format, sep='\\t', local=True, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "# Group events by attributeName, and order them by timestamp\n",
    "def generateSequence(eventList, attributeName):\n",
    "    grouped_by = {}\n",
    "    # Sort the event list\n",
    "    eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "    for event in eventList:\n",
    "        value = event.attributes[attributeName]\n",
    "        # If have seen this value before, append it the list of events in grouped_by for value\n",
    "        if value in grouped_by:\n",
    "            grouped_by[value].append(event)\n",
    "        # otherwise store a new list with just that event\n",
    "        else:\n",
    "            grouped_by[value] = [event]\n",
    "    return list(grouped_by.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n",
    "# Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "# break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "# “week”, “month”, “quarter”, and “year”.\n",
    "# For interval events I used the start time of the event to determine its category when splitting it\n",
    "def splitSequences(sequenceList, timeUnit):\n",
    "    results = []\n",
    "    timeUnit = timeUnit.lower()\n",
    "    # Check if the time unit is a valid argument\n",
    "    valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "    if timeUnit not in valid_time_units:\n",
    "        raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "    # Sort the events by the timestamp or event start time\n",
    "    sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "    \n",
    "    # Process the event sequence based on the given time unit\n",
    "    # Generally, create a map for that time unit and then add each event into that map \n",
    "    # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "    if timeUnit == \"hour\":\n",
    "        hours = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.hour, time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,hours,event)\n",
    "        results = list(hours.values())\n",
    "    \n",
    "    elif timeUnit == \"day\":\n",
    "        days = {}\n",
    "        for seq_no, event in enumerate(sequenceList):\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,days,event)\n",
    "            #print(days)\n",
    "            event.attributes[\"record\"]=str(key)\n",
    "        results = list(days.values())\n",
    "        \n",
    "    elif timeUnit == \"month\":\n",
    "        months = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.month,time.year)\n",
    "            insert_event_into_dict(key,months,event)\n",
    "        results = list(months.values())\n",
    "        \n",
    "    elif timeUnit == \"week\":\n",
    "        weeks = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            week_num = time.isocalendar()[1]\n",
    "            key = (year,week_num)\n",
    "            insert_event_into_dict(key,weeks,event)\n",
    "        results = list(weeks.values())\n",
    "                                   \n",
    "    elif timeUnit == \"year\":\n",
    "        years = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = time.year\n",
    "            insert_event_into_dict(key,years,event)\n",
    "        results = list(years.values())\n",
    "            \n",
    "    elif timeUnit == \"quarter\":\n",
    "        quarters = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            month = time.month\n",
    "            # Determine the year, quarter pair/key for quarter dict\n",
    "            # January, February, and March (Q1)\n",
    "            if month in range(1, 4):\n",
    "                key = (year, \"Q1\")\n",
    "            # April, May, and June (Q2)\n",
    "            elif month in range(4, 7):\n",
    "                key = (year, \"Q2\")\n",
    "            # July, August, and September (Q3)\n",
    "            elif month in range(7,10):\n",
    "                key = (year, \"Q3\")\n",
    "            # October, November, and December (Q4)\n",
    "            elif month in range(10,13):\n",
    "                key = (year, \"Q4\")\n",
    "            # Put the event in the dictionary\n",
    "            insert_event_into_dict(key,quarters,event)\n",
    "        results = list(quarters.values())\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to EventFLow Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataModel(eventlist, record_id, event_category, file_name=\"Eventflow.txt\"):\n",
    "    #columns in EventFlow\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    \n",
    "    Start_time=None\n",
    "    End_time=None\n",
    "    data=[]\n",
    "    key_list=[]\n",
    "    \n",
    "    #which keys will go to attributes list\n",
    "    for keys in eventlist[0].attributes.keys():\n",
    "        if keys not in [record_id,event_category]:\n",
    "            key_list.append(keys)\n",
    "    for event in eventlist:\n",
    "        # Get the attribute value of record\n",
    "        \n",
    "        if(event.type==\"point\"):    \n",
    "            Start_time=event.timestamp\n",
    "            End_time=\"\"\n",
    "        else:\n",
    "            Start_time=event.time[0]\n",
    "            End_time=event.time[1]\n",
    "        attr_str=\"\"\n",
    "        for keys in key_list:\n",
    "            attr_str+=str(keys)+\"=\\\"\"+str(event.attributes[keys])+\"\\\"\"\n",
    "        \n",
    "        data.append([(re.sub('[(),]','',str(event.attributes[record_id]))).replace(\" \",\"_\"),str(event.attributes[event_category]).replace(\" \",\"_\"),Start_time, End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataframe(df, record_id_idx, event_category_idx, file_name=\"Eventflow.txt\"):\n",
    "    #columns in EventFlow\n",
    "    \"\"\"\n",
    "        events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "        # for all attributes other tahn time, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != timestampColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = PointEvent(timestamp, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "    \"\"\"\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    \n",
    "    Start_time=None\n",
    "    End_time=None\n",
    "    data=[]\n",
    "    key_list=[]\n",
    "    \n",
    "    #which keys will go to attributes list\n",
    "    for keys in eventlist[0].attributes.keys():\n",
    "        if keys not in [record_id,event_category]:\n",
    "            key_list.append(keys)\n",
    "    for event in eventlist:\n",
    "        # Get the attribute value of record\n",
    "        \n",
    "        if(event.type==\"point\"):    \n",
    "            Start_time=event.timestamp\n",
    "            End_time=\"\"\n",
    "        else:\n",
    "            Start_time=event.time[0]\n",
    "            End_time=event.time[1]\n",
    "        attr_str=\"\"\n",
    "        for keys in key_list:\n",
    "            attr_str+=str(keys)+\"=\\\"\"+str(event.attributes[keys])+\"\\\"\"\n",
    "        \n",
    "        data.append([(re.sub('[(),]','',str(event.attributes[record_id]))).replace(\" \",\"_\"),str(event.attributes[event_category]).replace(\" \",\"_\"),Start_time, End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in this case, the record distinctions are not present, so we add record_id\n",
    "\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "splitSequences(sequence_braiding, \"day\")\n",
    "event_df=createEventflowFormatdataModel(sequence_braiding, \"record\", \"Meal\", \"EventFlow.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case data doesn't have header, so we add headers\n",
    "header = [\"record_id\", \"EventType\", \"Start time\", \"End time\",\"attribute\"]\n",
    "children_hospital = importMixedEvents('../datasets/Children Hospital/DND-ChildrensDemo-06-26-13.txt', 2,3, \"%Y-%m-%d %H:%M:%S.%f\", sep='\\t', local=True, header= header)\n",
    "event_df=createEventflowFormatdataModel(children_hospital, \"record_id\", \"EventType\", \"EventFlow_CH.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glucose\n",
      "Meal\n",
      "record\n"
     ]
    }
   ],
   "source": [
    "for key_ in sequence_braiding[0].attributes.keys():\n",
    "    print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Glucose': 38, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 233, 'Meal': 'Supper'}\n",
      "{'Glucose': 52, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 67, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 309, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 66, 'Meal': 'Sugar to treat'}\n",
      "{'Glucose': 80, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 168, 'Meal': 'Not Dinner'}\n",
      "{'Glucose': 171, 'Meal': 'Supper'}\n",
      "{'Glucose': 56, 'Meal': 'Sugar to treat'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of aggregateEventsDict with test file\n",
    "# Rules in the test file:\n",
    "# Dinner will become Supper \n",
    "# Lunch and Breakfast become Not Dinner\n",
    "sb = aggregateEventsDict(sequence_braiding, \"testFiles/aggregateEventsDictTestFileSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Glucose': 38, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 233, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 52, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 67, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 309, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 66, 'Meal': 'SNACK TIME!'}\n",
      "{'Glucose': 80, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 168, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 171, 'Meal': 'Must eat times'}\n",
      "{'Glucose': 56, 'Meal': 'SNACK TIME!'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of aggregateEventsRegex with test file\n",
    "# Rules in the test file:\n",
    "# Lunch|Dinner|Breakfast -> Must eat times\n",
    "# S.* -> SNACK TIME!\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "sb = aggregateEventsRegex(sequence_braiding, \"testFiles/aggregateEventsRegexSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
