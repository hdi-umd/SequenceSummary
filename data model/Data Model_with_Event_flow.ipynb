{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipynb.fs.full.Data_Model import get_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that represents a point event\n",
    "class PointEvent:\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent:\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe(src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# header is list of column names if they are not provided in the dataset\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importPointEvents(src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "        # for all attributes other tahn time, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != timestampColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = PointEvent(timestamp, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "# for those cases\n",
    "def importIntervalEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        attribs = {}\n",
    "        # create datetime object for the start and end times of the event\n",
    "        t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "        t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        for i in range(len(data)):\n",
    "            if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                attribs[cols[i]] = data[i]\n",
    "        # use time stamp and attributes map to construct event object\n",
    "        e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events\n",
    "\n",
    "# Import a dataset that has both interval and point events\n",
    "# Returns a list of event objects\n",
    "# src is a url or directory path, if local is false its url else its path\n",
    "# The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "# I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "def importMixedEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "    events = []\n",
    "    # if the df is not provided\n",
    "    if df is None:\n",
    "        df = get_dataframe(src, local, sep, header)\n",
    "    cols = df.columns\n",
    "    #print(df)\n",
    "    # For each event in the csv construct an event object\n",
    "    for row in df.iterrows():\n",
    "        data = row[1]\n",
    "        #print(data)\n",
    "        attribs = {}\n",
    "        # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "        # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "        if type(data[endTimeColumnIdx]) is float:\n",
    "            t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            event_type = \"point\"\n",
    "        # Otherwise its an interval event\n",
    "        else:\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            event_type = \"interval\"\n",
    "        # for all attributes other than times, add them to attributes dict\n",
    "        ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "        attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "        for i in attribute_columns:\n",
    "            attribs[cols[i]] = data[i]\n",
    "        # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "        if event_type == \"point\":\n",
    "            e = PointEvent(t, attribs)\n",
    "        else:\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "        events.append(e)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAST mini challenge dataset\n",
    "url = \"http://vacommunity.org/tiki-download_file.php?fileId=492\"\n",
    "vast = importPointEvents(url, 0, '%Y-%m-%d %H:%M:%S', sep=',', local=False)\n",
    "\n",
    "# Sequence braiding\n",
    "# NOTE: I deleted the last 4 rows of the dataset before loading it in since they did not look like evevnts \n",
    "# NOTE: this data set only had dates not times\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "\n",
    "# Foursquare NYC\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "    \n",
    "# Foursquare tokyo\n",
    "df = pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "fs_tokyo = importPointEvents('datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "\n",
    "# CHICAGO-SeasonD2O.txt\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "header = [\"Game/Points\", \"EventType\", \"Start time\", \"End time\"]\n",
    "chicago_season_d2o = importMixedEvents(\"../datasets/Chicago_Bulls/CHICAGO-SeasonD2O.txt\", 2, 3, time_format, sep='\\t', local=True, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "# Group events by attributeName, and order them by timestamp\n",
    "def generateSequence(eventList, attributeName):\n",
    "    grouped_by = {}\n",
    "    # Sort the event list\n",
    "    eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "    for event in eventList:\n",
    "        value = event.attributes[attributeName]\n",
    "        # If have seen this value before, append it the list of events in grouped_by for value\n",
    "        if value in grouped_by:\n",
    "            grouped_by[value].append(event)\n",
    "        # otherwise store a new list with just that event\n",
    "        else:\n",
    "            grouped_by[value] = [event]\n",
    "    return list(grouped_by.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n",
    "# Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "# break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "# “week”, “month”, “quarter”, and “year”.\n",
    "# For interval events I used the start time of the event to determine its category when splitting it\n",
    "def splitSequences(sequenceList, timeUnit, record=None):\n",
    "    results = []\n",
    "    timeUnit = timeUnit.lower()\n",
    "    # Check if the time unit is a valid argument\n",
    "    valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "    if timeUnit not in valid_time_units:\n",
    "        raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "    # Sort the events by the timestamp or event start time\n",
    "    sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "    \n",
    "    # Process the event sequence based on the given time unit\n",
    "    # Generally, create a map for that time unit and then add each event into that map \n",
    "    # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "    if timeUnit == \"hour\":\n",
    "        hours = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.hour, time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,hours,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "        results = list(hours.values())\n",
    "    \n",
    "    elif timeUnit == \"day\":\n",
    "        days = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.day, time.month, time.year)\n",
    "            insert_event_into_dict(key,days,event)\n",
    "            #print(days)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "        results = list(days.values())\n",
    "        \n",
    "    elif timeUnit == \"month\":\n",
    "        months = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = (time.month,time.year)\n",
    "            insert_event_into_dict(key,months,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "        results = list(months.values())\n",
    "        \n",
    "    elif timeUnit == \"week\":\n",
    "        weeks = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            week_num = time.isocalendar()[1]\n",
    "            key = (year,week_num)\n",
    "            insert_event_into_dict(key,weeks,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "        results = list(weeks.values())\n",
    "                                   \n",
    "    elif timeUnit == \"year\":\n",
    "        years = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            key = time.year\n",
    "            insert_event_into_dict(key,years,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key)\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "        results = list(years.values())\n",
    "            \n",
    "    elif timeUnit == \"quarter\":\n",
    "        quarters = {}\n",
    "        for event in sequenceList:\n",
    "            time = get_time_to_sort_by(event)\n",
    "            year = time.year\n",
    "            month = time.month\n",
    "            # Determine the year, quarter pair/key for quarter dict\n",
    "            # January, February, and March (Q1)\n",
    "            if month in range(1, 4):\n",
    "                key = (year, \"Q1\")\n",
    "            # April, May, and June (Q2)\n",
    "            elif month in range(4, 7):\n",
    "                key = (year, \"Q2\")\n",
    "            # July, August, and September (Q3)\n",
    "            elif month in range(7,10):\n",
    "                key = (year, \"Q3\")\n",
    "            # October, November, and December (Q4)\n",
    "            elif month in range(10,13):\n",
    "                key = (year, \"Q4\")\n",
    "            # Put the event in the dictionary\n",
    "            insert_event_into_dict(key,quarters,event)\n",
    "            if record is None:\n",
    "                event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "            else:\n",
    "                event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "        results = list(quarters.values())\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to EventFLow Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataModel(eventlist, record_id, event_category,include_attr=None, file_name=\"Eventflow.txt\"):\n",
    "    #columns in EventFlow\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    \n",
    "    data=[]\n",
    "    \n",
    "    #which keys will go to attributes list\n",
    "    #key_list=[keys for keys in eventlist[0].attributes.keys() if keys not in [record_id,event_category]]\n",
    "    key_list=include_attr\n",
    "    print(key_list)\n",
    "    for event in eventlist:\n",
    "        # Get the attribute value of record    \n",
    "        if(event.type==\"point\"):    \n",
    "            Start_time=event.timestamp\n",
    "            End_time=\"\"\n",
    "        else:\n",
    "            Start_time=event.time[0]\n",
    "            End_time=event.time[1]\n",
    "        attr_str=\"\"\n",
    "        if include_attr is not None:\n",
    "            attr_str=\";\".join([str(keys).replace(\" \",\"_\")+\"=\\\"\"+str(event.attributes[keys])+\"\\\"\" for keys in key_list])\n",
    "        \n",
    "        data.append([(re.sub('[(),]','',str(event.attributes[record_id]))).replace(\" \",\"_\"),(re.sub('[(),]','',str(event.attributes[event_category]))).replace(\" \",\"_\"),Start_time, End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate method signature\n",
    "#def createEventflowFormatdataframe(src, record_id_idx, event_category_idx, startTimeColumnIdx, endTimeColumnIdx, sep=\"\\t\", local=False, header=[],file_name=\"Eventflow.txt\"):\n",
    "    #events = []\n",
    "    # if the df is not provided\n",
    "    #if df is None:\n",
    "    #    df = get_dataframe(src, local, sep, header)\n",
    "    #cols = df.columns\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEventflowFormatdataframe(df, record_id_idx, event_category_idx, start_time_idx, end_time_idx,include_attr=None, file_name=\"Eventflow.txt\"):\n",
    "    \n",
    "\n",
    "    #columns in EventFlow\n",
    "    column_names=[\"record_id\",\"event_category\",\"Start_time\", \"end_time\", \"event_attributes\"]\n",
    "    data=[]\n",
    "    \n",
    "    key_list=df.columns[include_attr].values\n",
    "    print(key_list)\n",
    "    for row in df.iterrows():\n",
    "        event = row[1]\n",
    "        if type(event[end_time_idx]) is float:\n",
    "            End_time = \"\"\n",
    "        else:\n",
    "            End_time = event[end_time_idx]\n",
    "        attr_str=\"\"\n",
    "        if include_attr is not None:\n",
    "            attr_str=\";\".join([(re.sub('[(),]','',str(keys))).replace(\" \",\"_\")+\"=\\\"\"+str(event[keys])+\"\\\"\" for keys in key_list])\n",
    "        data.append([(re.sub('[(),]','',str(event[record_id_idx]))).replace(\" \",\"_\"),(re.sub('[(),]','',str(event[event_category_idx]))).replace(\" \",\"_\"),event[start_time_idx], End_time,attr_str])\n",
    "        \n",
    "    event_df=pd.DataFrame(data,columns = column_names)\n",
    "    \n",
    "    event_df.to_csv(\"../datasets/\"+file_name,sep=\"\\t\",header=False,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glucose']\n"
     ]
    }
   ],
   "source": [
    "# in this case, the record distinctions are not present, so we add record_id\n",
    "\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "splitSequences(sequence_braiding, \"week\")\n",
    "sequence_braiding[0].attributes\n",
    "event_df=createEventflowFormatdataModel(sequence_braiding, \"record\", \"Meal\", [\"Glucose\"],\"EventFlow.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case data doesn't have header, so we add headers\n",
    "header = [\"record_id\", \"EventType\", \"Start time\", \"End time\",\"attribute\"]\n",
    "children_hospital = importMixedEvents('../datasets/Children Hospital/DND-ChildrensDemo-06-26-13.txt', 2,3, \"%Y-%m-%d %H:%M:%S.%f\", sep='\\t', local=True, header= header)\n",
    "splitSequences(children_hospital, \"hour\",\"record_id\")\n",
    "event_df=createEventflowFormatdataModel(children_hospital, \"record_id\", \"EventType\", \"EventFlow_CH.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender' 'Speed']\n"
     ]
    }
   ],
   "source": [
    "#Loading from dataframe\n",
    "endomodo = get_dataframe(\"../datasets/endomondo/Endo_time_clean.txt\", local=True, sep=\" \", header=[])\n",
    "#print(endomodo)\n",
    "event_df=createEventflowFormatdataframe(endomodo, 0, 1,2,3,[4,6], \"EventFlow_Endo_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corona_net = get_dataframe('../coronanet_shortened.csv', local=True, sep=\",\", header=[])\n",
    "#print(corona_net)\n",
    "#splitSequences(sequence_braiding, \"day\")\n",
    "event_df=createEventflowFormatdataframe(corona_net,8,10,6,7,  \"EventFlow_corona_subset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy_id', 'entry_type', 'correct_type', 'update_type', 'update_level', 'date_announced', 'init_country_level', 'type_sub_cat']\n"
     ]
    }
   ],
   "source": [
    "corona_net = get_dataframe('../datasets/coronanet/coronanet_subset_shortened_Feb.csv', local=True, sep=\",\", header=[])\n",
    "len(corona_net)\n",
    "#print(corona_net)\n",
    "#splitSequences(sequence_braiding, \"day\")\n",
    "event_df=createEventflowFormatdataframe(corona_net,8,10,6,7, file_name= \"EventFlow_corona_subset_Feb.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header = [\"record_id\", \"EventType\", \"Start time\", \"End time\",\"attribute\"]\n",
    "invention_trajectories=get_dataframe('../datasets/Invention Trajectories/invention_expanded.csv', local=True, sep=\",\")\n",
    "invention_trajectories=invention_trajectories[invention_trajectories[\"docsource\"]!='\"\"']\n",
    "#invention_trajectories=invention_trajectories[\"mainclass\"!=\"\"]\n",
    "event_df=createEventflowFormatdataframe(invention_trajectories,0,4,2,3,  \"EventFlow_invention_docsrc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender', 'Speed']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "endomodo = importMixedEvents('../Endo_time_clean.txt', 2,3, \"%Y-%m-%d %H:%M:%S\", sep=' ', local=True)\n",
    "splitSequences(endomodo, \"week\",\"UserID\")\n",
    "event_df=createEventflowFormatdataModel(endomodo, \"UserID\", \"Sport\", [\"Gender\",\"Speed\"],\"endo_week.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_nyc, \"quarter\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",False, \"EventFlow_fs_nyc_quarter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare NY weekly\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "df = pd.read_csv('../datasets/foursquare/NYC_shortened.csv', ',', encoding=\"latin1\")\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/NYC_shortened.csv', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_nyc, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",None, \"shortened_fs_nyc_week.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'foursquare_time_format' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-326f6af165e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Foursquare tokyo weekly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../datasets/foursquare/TKY_shortened.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfs_tokyo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportPointEvents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/foursquare/TKY_shortened.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoursquare_time_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msplitSequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_tokyo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"week\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"User ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevent_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreateEventflowFormatdataModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_tokyo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"User ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Venue category name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shortened_fs_tky_week.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'foursquare_time_format' is not defined"
     ]
    }
   ],
   "source": [
    "# Foursquare tokyo weekly\n",
    "df = pd.read_csv('../datasets/foursquare/TKY_shortened.csv', ',', encoding='latin1')\n",
    "fs_tokyo = importPointEvents('datasets/foursquare/TKY_shortened.csv', 7, foursquare_time_format, df=df)\n",
    "splitSequences(fs_tokyo, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",None, \"shortened_fs_tky_week.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User ID': '1541_2012W14',\n",
       " 'Venue category name': 'Shop',\n",
       " 'UTC time_end': nan,\n",
       " 'attr': nan}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weekly Foursquare TOkyo\n",
    "header = [\"User ID\", \"Venue category name\", \"UTC time_start\",\"UTC time_end\",\"attr\"]\n",
    "foursquare_time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "df = pd.read_csv('../datasets/saggr_fs_tky.txt', '\\t', names=header, encoding=\"latin1\")\n",
    "fs_tky = importPointEvents('../datasets/saggr_fs_tky.txt', 2, foursquare_time_format, df=df)\n",
    "splitSequences(fs_tky, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_tky, \"User ID\", \"Venue category name\",None, \"EventFlow_fs_tky_day.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corona_net.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anti-Disinformation Measures': 'Measures', 'Public Awareness Measures': 'Measures', 'External Border Restrictions': 'Border restriction', 'Internal Border Restrictions': 'Border restriction', 'Health Monitoring': 'Health and Hygiene', 'Health Resources': 'Health and Hygiene', 'Health Testing': 'Health and Hygiene', 'Hygiene': 'Health and Hygiene', 'Restriction and Regulation of Businesses': 'Restriction and Regulation', 'Restriction and Regulation of Government Services': 'Restriction and Regulation', 'Restrictions of Mass Gatherings': 'Restriction and Regulation', 'Closure and Regulation of Schools': 'Restriction and Regulation', 'Lockdown': 'Lockdown curfew emergency', 'Curfew': 'Lockdown curfew emergency', 'Declaration of Emergency': 'Lockdown curfew emergency'}\n"
     ]
    }
   ],
   "source": [
    "df = get_dataframe('../datasets/coronanet/coronanet_subset_shortened.csv', local=True, sep=\",\", header=[])\n",
    "coronanet = importIntervalEvents('../datasets/coronanet/coronanet_subset_shortened.csv', 6,7, \"%Y-%m-%d\", df=df)\n",
    "corona_aggr=aggregateEventsDict(coronanet,\"../datasets/coronanet/coronadict_final.txt\", \"type\")\n",
    "event_df=createEventflowFormatdataModel(coronanet, \"country\", \"type\",include_attr=None, file_name=\"aggr_corona_subset_without_russia.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_ in sequence_braiding[0].attributes.keys():\n",
    "    print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Restaurant': 'Restaurant', 'Japanese Restaurant': 'Restaurant', 'French Restaurant': 'Restaurant', 'Chinese Restaurant': 'Restaurant', 'Korean Restaurant': 'Restaurant', 'Asian Restaurant': 'Restaurant', 'Italian Restaurant': 'Restaurant', 'Indian Restaurant': 'Restaurant', 'Sushi Restaurant': 'Restaurant', 'Dumpling Restaurant': 'Restaurant', 'Thai Restaurant': 'Restaurant', 'Seafood Restaurant': 'Restaurant', 'American Restaurant': 'Restaurant', 'Eastern European Restaurant': 'Restaurant', 'Caribbean Restaurant': 'Restaurant', 'Spanish Restaurant': 'Restaurant', 'Vegetarian / Vegan Restaurant': 'Restaurant', 'German Restaurant': 'Restaurant', 'Vietnamese Restaurant': 'Restaurant', 'Swiss Restaurant': 'Restaurant', 'Dim Sum Restaurant': 'Restaurant', 'Tapas Restaurant': 'Restaurant', 'Mexican Restaurant': 'Restaurant', 'Turkish Restaurant': 'Restaurant', 'Mediterranean Restaurant': 'Restaurant', 'Latin American Restaurant': 'Restaurant', 'Brazilian Restaurant': 'Restaurant', 'Ethiopian Restaurant': 'Restaurant', 'Middle Eastern Restaurant': 'Restaurant', 'Peruvian Restaurant': 'Restaurant', 'Moroccan Restaurant': 'Restaurant', 'Cuban Restaurant': 'Restaurant', 'Portuguese Restaurant': 'Restaurant', 'Scandinavian Restaurant': 'Restaurant', 'African Restaurant': 'Restaurant', 'Malaysian Restaurant': 'Restaurant', 'Southern / Soul Food Restaurant': 'Restaurant', 'Argentinian Restaurant': 'Restaurant', 'Cajun / Creole Restaurant': 'Restaurant', 'South American Restaurant': 'Restaurant', 'Arepa Restaurant': 'Restaurant', 'Gluten-free Restaurant': 'Restaurant', 'Afghan Restaurant': 'Restaurant', 'Falafel Restaurant': 'Restaurant', 'Australian Restaurant': 'Restaurant', 'Diner': 'Restaurant', 'Steakhouse': 'Restaurant', 'Store': 'Store', 'Convenience Store': 'Store', 'Furniture / Home Store': 'Store', 'Candy Store': 'Store', 'Toy / Game Store': 'Store', 'Electronics Store': 'Store', 'Department Store': 'Store', 'Arts & Crafts Store': 'Store', 'Bookstore': 'Store', 'Clothing Store': 'Store', 'Drugstore / Pharmacy': 'Store', 'Camera Store': 'Store', 'Music Store': 'Store', 'Hardware Store': 'Store', 'Paper / Office Supplies Store': 'Store', 'Video Game Store': 'Store', 'Pet Store': 'Store', 'Thrift / Vintage Store': 'Store', 'Jewelry Store': 'Store', 'Video Store': 'Store', 'Cosmetics Shop': 'Shop', 'Flower Shop': 'Shop', 'Miscellaneous Shop': 'Shop', 'Smoke Shop': 'Shop', 'Automotive Shop': 'Shop', 'Mobile Phone Shop': 'Shop', 'Shop & Service': 'Shop', 'Hobby Shop': 'Shop', 'Gift Shop': 'Shop', 'Bike Shop': 'Shop', 'Record Shop': 'Shop', 'Sporting Goods Shop': 'Shop', 'Antique Shop': 'Shop', 'Board Shop': 'Shop', 'Motorcycle Shop': 'Shop', 'Bridal Shop': 'Shop', 'Fast Food Restaurant': 'Food and Snack', 'Coffee Shop': 'Food and Snack', 'Donut Shop': 'Food and Snack', 'Dessert Shop': 'Food and Snack', 'Food & Drink Shop': 'Food and Snack', 'Ice Cream Shop': 'Food and Snack', 'Bagel Shop': 'Food and Snack', 'Cupcake Shop': 'Food and Snack', 'Bakery': 'Food and Snack', 'Fish & Chips Shop': 'Food and Snack', 'Breakfast Spot': 'Food and Snack', 'Burrito Place': 'Food and Snack', 'Taco Place': 'Food and Snack', 'Salad Place': 'Food and Snack', 'Pizza Place': 'Food and Snack', 'Soup Place': 'Food and Snack', 'Sandwich Place': 'Food and Snack', 'Snack Place': 'Food and Snack', 'Mac & Cheese Joint': 'Food and Snack', 'Fried Chicken Joint': 'Food and Snack', 'BBQ Joint': 'Food and Snack', 'Burger Joint': 'Food and Snack', 'Hot Dog Joint': 'Food and Snack', 'Wings Joint': 'Food and Snack', 'Deli / Bodega': 'Food and Snack', 'Gaming Cafe': 'Cafe', 'Internet Cafe': 'Cafe', 'Café': 'Cafe', 'Salon / Barbershop': 'Salon', 'Tanning Salon': 'Salon', 'Nail Salon': 'Salon', 'Spa / Massage': 'Salon', 'Fraternity House': 'Fraternity', 'Sorority House': 'Fraternity', 'General College & University': 'General College & University', 'University': 'General College & University', 'Community College': 'General College & University', 'College & University': 'General College & University', 'College Stadium': 'General College & University', 'College Theater': 'General College & University', 'College Academic Building': 'General College & University', 'Student Center': 'General College & University', 'Music School': 'Speciality Schools', 'Law School': 'Speciality Schools', 'Trade School': 'Speciality Schools', 'Medical School': 'Speciality Schools', 'Nursery School': 'Schools', 'Elementary School': 'Schools', 'School': 'Schools', 'Middle School': 'Schools', 'High School': 'Schools', 'Distillery': 'Alcohol', 'Winery': 'Alcohol', 'Brewery': 'Alcohol', 'Gastropub': 'Alcohol', 'Beer Garden': 'Alcohol', 'Bar': 'Alcohol', 'History Museum': 'Museum', 'Museum': 'Museum', 'Art Museum': 'Museum', 'Science Museum': 'Museum', 'Outdoors & Recreation': 'Outdoor', 'Other Great Outdoors': 'Outdoor', 'Sculpture Garden': 'Outdoor', 'Scenic Lookout': 'Outdoor', 'Campground': 'Outdoor', 'Garden': 'Outdoor', 'Shrine': 'Spiritual', 'Temple': 'Spiritual', 'Mosque': 'Spiritual', 'Synagogue': 'Spiritual', 'Church': 'Spiritual', 'Spiritual Center': 'Spiritual', 'Arts & Entertainment': 'Arts & Entertainment', 'Public Art': 'Arts & Entertainment', 'Performing Arts Venue': 'Arts & Entertainment', 'Art Gallery': 'Arts & Entertainment', 'Music Venue': 'Arts & Entertainment', 'Concert Hall': 'Arts & Entertainment', 'Theater': 'Arts & Entertainment', 'Movie Theater': 'Arts & Entertainment', 'Comedy Club': 'Arts & Entertainment', 'General Entertainment': 'General Entertainment', 'Casino': 'General Entertainment', 'Racetrack': 'General Entertainment', 'Event Space': 'Event Space', 'Convention Center': 'Event Space', 'General Travel': 'Travel & Transport', 'Travel & Transport': 'Travel & Transport', 'Travel Lounge': 'Travel & Transport', 'Taxi': 'Travel & Transport', 'Airport': 'Travel & Transport', 'Bus Station': 'Travel & Transport', 'Light Rail': 'Travel & Transport', 'Ferry': 'Travel & Transport', 'Bridge': 'Travel & Transport', 'Road': 'Travel & Transport', 'Rest Area': 'Travel & Transport', 'Train Station': 'Railway', 'Subway': 'Railway', 'Nightlife Spot': 'Nightlife', 'Other Nightlife': 'Nightlife', 'Plaza': 'Mall', 'Mall': 'Mall', 'Aquarium': 'Recreation', 'Planetarium': 'Recreation', 'Park': 'Recreation', 'Zoo': 'Recreation', 'Playground': 'Recreation', 'Pool Hall': 'Indoor Games', 'Arcade': 'Indoor Games', 'Bowling Alley': 'Indoor Games'}\n"
     ]
    }
   ],
   "source": [
    "#Event aggregation\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df =  pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_tokyo = importPointEvents('../datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_tokyo,\"../datasets/foursquare/tky_dict_final.txt\", \"Venue category name\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",include_attr=[\"subcat\"], file_name= \"saggr_fs_tky.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df = pd.read_csv('../datasets/saggr_fs_nyc.txt'', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_nyc,\"../datasets/foursquare/ny_dict_final.txt\", \"Venue category name\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",include_attr=[\"subcat\"],file_name= \"saggr_fs_nyc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation_weekly_nyc\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df = pd.read_csv('../datasets/foursquare/nyc.txt', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/nyc.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_nyc,\"../datasets/foursquare/ny_dict_final.txt\", \"Venue category name\")\n",
    "splitSequences(fs_nyc, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",include_attr=None,file_name= \"aggr_fs_nyc_week_without_subcat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation_weekly_tky\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df =  pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_tokyo = importPointEvents('../datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_tokyo,\"../datasets/foursquare/tky_dict_final.txt\", \"Venue category name\")\n",
    "splitSequences(fs_tokyo, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",include_attr=None, file_name= \"aggr_fs_tky_week_without_subcat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "df =  pd.read_csv('../datasets/foursquare/tokyo.txt', names=header, encoding='latin1', sep='\\t')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_tokyo = importPointEvents('../datasets/foursquare/tokyo.txt', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_tokyo,\"../datasets/foursquare/tky_dict_final.txt\", \"Venue category name\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",include_attr=[\"subcat\"], file_name= \"saggr_fs_tky.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation_weekly_tky_shortened\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%Y-%m-%d %H:%M:%S+%f:00\"\n",
    "df =  pd.read_csv('../datasets/foursquare/TKY_Apr.csv',  encoding='latin1', sep=',')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_tokyo = importPointEvents('../datasets/foursquare/TKY_Apr.csv', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_tokyo,\"../datasets/foursquare/tky_dict_final.txt\", \"Venue category name\")\n",
    "splitSequences(fs_tokyo, \"day\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_tokyo, \"User ID\", \"Venue category name\",include_attr=[\"subcat\"], file_name= \"aggr_fs_tky_day_Apr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event aggregation_weekly_tky_shortened\n",
    "header = [\"User ID\", \"Venue ID\", \"Venue category ID\", \"Venue category name\", \"Latitude\", \"Longitude\", \n",
    "          \"Timezone offset (minutes)\", \"UTC time\"]\n",
    "foursquare_time_format = \"%Y-%m-%d %H:%M:%S+%f:00\"\n",
    "df =  pd.read_csv('../datasets/foursquare/NYC_Apr.csv',  encoding='latin1', sep=',')\n",
    "df[\"subcat\"]=df[\"Venue category name\"]\n",
    "fs_nyc = importPointEvents('../datasets/foursquare/NYC_Apr.csv', 7, foursquare_time_format, df=df)\n",
    "fs_aggr=aggregateEventsDict(fs_nyc,\"../datasets/foursquare/ny_dict_final.txt\", \"Venue category name\")\n",
    "splitSequences(fs_nyc, \"week\",\"User ID\")\n",
    "event_df=createEventflowFormatdataModel(fs_nyc, \"User ID\", \"Venue category name\",include_attr=[\"subcat\"], file_name= \"aggr_fs_nyc_week_Apr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_ in fs_tokyo[0].attributes.keys():\n",
    "    print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of aggregateEventsDict with test file\n",
    "# Rules in the test file:\n",
    "# Dinner will become Supper \n",
    "# Lunch and Breakfast become Not Dinner\n",
    "sb = aggregateEventsDict(sequence_braiding, \"testFiles/aggregateEventsDictTestFileSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of aggregateEventsRegex with test file\n",
    "# Rules in the test file:\n",
    "# Lunch|Dinner|Breakfast -> Must eat times\n",
    "# S.* -> SNACK TIME!\n",
    "sequence_braiding = importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "sb = aggregateEventsRegex(sequence_braiding, \"testFiles/aggregateEventsRegexSequenceBraidings.txt\", \"Meal\")\n",
    "for e in sb[:10]:\n",
    "    print(e.attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
