{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common class for all Events\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, eventtype):\n",
    "        self.type=eventtype\n",
    "    \n",
    "    #Return Attribute value given attribute name\n",
    "    def getAttrVal(self, attrName):\n",
    "        return self.attributes.get(attrName,\"Not found\")\n",
    "\n",
    "    \n",
    "# A class that represents a point event\n",
    "class PointEvent(Event):\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        Event.__init__(self,\"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "        \n",
    "    \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent(Event):\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        Event.__init__(self,\"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence:\n",
    "    _ids = count(0)\n",
    "    attrdict={}\n",
    "    def __init__(self, events, sid=None):\n",
    "        if sid is None:\n",
    "            self.sid = next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "        \n",
    "        self.events = events\n",
    "        self.volume=1\n",
    "        self.seqAttributes={}\n",
    "    def getEventPosition(self, attr, hash_val):\n",
    "        for count,event in enumerate(self.events):\n",
    "            if event.getAttrVal(attr)==hash_val:\n",
    "                return count\n",
    "        return -1\n",
    "    \n",
    "    def setVolume(self, intValue):\n",
    "        self.volume=intValue\n",
    "        \n",
    "    def getVolume():\n",
    "        return self.volume\n",
    "    \n",
    "    def increaseVolume():\n",
    "        self.volume += 1 \n",
    "        \n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return l\n",
    "    \n",
    "    #Not sure this will always result in same index, will change if \n",
    "    #dictionary is updated\n",
    "    #since python is unordered\n",
    "    \n",
    "    def getHashList(self, attr):\n",
    "        l=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        return l\n",
    "    \n",
    "    def getValueHashes(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        return l\n",
    "    \n",
    "    def getEventsHashString(self, attr):\n",
    "        s=attr+\": \"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\" \".join(str(event.getAttrVal(attr)) for event in self.events)\n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        s=\" -1 \".join(str(event.getAttrVal(attr)) for event in self.events)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\" -2\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def getPathID(self):\n",
    "        return self.sid\n",
    "    \n",
    "    def matchPathAttribute(self, attr, val):\n",
    "        # should i use eq?!\n",
    "        if this.seqAttributes.get(attr)==(val):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def setSequenceAttribute(self,attr, value):\n",
    "        self.seqAttributes[attr]=value\n",
    "        \n",
    "    \n",
    "    def create_attr_dict(seqList):\n",
    "        attr_list=seqList[0].events[0].attributes.keys()\n",
    "        print(attr_list)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            a=0\n",
    "            unique_list=[]\n",
    "            for sequences in seqList:\n",
    "                unique_list.extend(sequences.getUniqueValueHashes(attr))\n",
    "            unique_list=list(set(unique_list))\n",
    "            unique_list.clear()\n",
    "            \n",
    "            unicode_dict={}\n",
    "            for uniques in unique_list:\n",
    "                unicode_dict[uniques]=chr(a)\n",
    "                a=a+1\n",
    "            self.attrdict[attr]=unicode_dict\n",
    "            unicode_dict.clear()    \n",
    "            \n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)    \n",
    "    def getSeqVolume(seqlist):\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattern:\n",
    "    _pids = count(1)\n",
    "\n",
    "    def __init__(self, events=[]):\n",
    "        #pattern id\n",
    "        self.id = next(self._pids)\n",
    "        \n",
    "        self.keyEvts = events\n",
    "        \n",
    "        self.medianPos=[]\n",
    "        self.meanPos=[]\n",
    "        \n",
    "        self.sids=[]\n",
    "        \n",
    "        self.support=0\n",
    "        self.supPercent=None\n",
    "        self.cluster=None\n",
    "        self.medianPathLength=0\n",
    "        self.meanPathLength=0\n",
    "        \n",
    "        self.parentSegment=None\n",
    "        self.segSizes=None\n",
    "        \n",
    "    def filterPaths(self, paths, evtType):\n",
    "        print(\"filtering \"+ str(len(paths))+\" paths by \"+str(len(self.keyEvts))+\" checkpoints\")\n",
    "        \n",
    "        for sequences in paths:\n",
    "            if(self.matchMilestones(sequences.getValueHashes(evtType),self.keyEvts)==False):\n",
    "                continue\n",
    "            self.sids.append(sequences)\n",
    "            \n",
    "        print(str(len(self.sids))+\" matching paths\")\n",
    "\n",
    "        \n",
    "    def matchMilestones(self, arr, milestones):\n",
    "        ja=arr\n",
    "        idx=-1\n",
    "        for elems in milestones:\n",
    "            try:\n",
    "                idx=arr[idx+1:].index(elems)\n",
    "                print(idx)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def getMedianSpacing(self):\n",
    "        l=[y - x for x,y in zip(self.medianPos,self.medianPos[1:])]\n",
    "        if(len(l)<=1):\n",
    "            return 100\n",
    "        l=l.sort()\n",
    "        middle=int(len(l)/2)\n",
    "        if(len(l)%2==0):\n",
    "            return ((l[middle-1]+l[middle])/2.0)\n",
    "        else:\n",
    "            return l[middle]\n",
    "        return np.median(np.asarray(l))\n",
    "    \n",
    "    def addKeyEvent(self, hashval):\n",
    "        self.keyEvts.append(hashval)\n",
    "        \n",
    "    def addToSupportSet(self, seq):\n",
    "        self.sids.append(seq)\n",
    "        self.support+=seq.getVolume()\n",
    "        \n",
    "    def getSequences(self):\n",
    "        return self.sids\n",
    "    \n",
    "    def setMedianPathLength(self, median):\n",
    "        self.medianPathLength=median\n",
    "    \n",
    "    def setMeanPathLength(self, mean):\n",
    "        self.meanPathLength=mean\n",
    "        \n",
    "    def getMedianPathLength(self):\n",
    "        return self.medianPathLength\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "    \n",
    "    def getEvents(self):\n",
    "        return self.keyEvts\n",
    "    \n",
    "    def getEventMeanPos(self):\n",
    "        return self.meanPos\n",
    "    \n",
    "    def getEventMedianPos(self):\n",
    "        return self.medianPos\n",
    "    \n",
    "    #Do we need to preserve order here??\n",
    "\n",
    "    def getUniqueEventsString(self):\n",
    "        #return \"-\".join(str(x) for x in list(set(self.keyEvts)))\n",
    "        #return \"-\".join(str(x) for x in list(dict.fromkeys(self.keyEvts)))\n",
    "        return \"-\".join(str(x) for x in self.keyEvts)\n",
    "    \n",
    "    def getPositions(self, events, path):\n",
    "        sequence=path\n",
    "        pos=[]\n",
    "        idx=-1\n",
    "        offset=0\n",
    "        \n",
    "        for elems in events:\n",
    "            \n",
    "            offset+=idx+1\n",
    "            try:\n",
    "                idx=path[offset:].index(elems)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            pos.append(offset+idx)\n",
    "        return pos\n",
    "    \n",
    "    def getMedian(self, data):\n",
    "        #middle=len(data)/2\n",
    "        #if(len(data)%2==0 and len(data)>1):\n",
    "        #    return (data[middle-1]+data[middle])/2.0\n",
    "        #else: \n",
    "        #    return data[middle]\n",
    "        return np.median(data)\n",
    "    \n",
    "    def computePatternStats(self, evtAttr):\n",
    "        pathsOfStrings=[]\n",
    "        for path in self.sids:\n",
    "            pageSequence=path.getHashList(evtAttr)\n",
    "            pathsOfStrings.append(pageSequence)\n",
    "            \n",
    "        medians=[]\n",
    "        means=[]\n",
    "        \n",
    "        for i,events in enumerate(self.keyEvts):\n",
    "            numSteps=[]\n",
    "            \n",
    "            for idx,paths in enumerate(pathsOfStrings):\n",
    "                if(self.matchMilestones(paths, self.keyEvts[0,i+1])):\n",
    "                    pos=self.getPositions(self.keyEvts[0,i+1], paths)\n",
    "                    if i==0:\n",
    "                        #add position value of first element id sequence\n",
    "                        numSteps.append(pos[i])\n",
    "                    else:\n",
    "                        #in other cases add the difference\n",
    "                        numSteps.append(pos[i]-pos[i-1])\n",
    "            sum_steps=sum(numSteps)\n",
    "            \n",
    "            median= self.getMedian(numsteps)\n",
    "            \n",
    "            medians.append(median)\n",
    "            means.append(sum_steps*1.0/ numSteps.size())\n",
    "                \n",
    "            \n",
    "                \n",
    "        #list(accumulate(means))\n",
    "        means=np.cumsum(np.asarray(means))\n",
    "        medians=np.cumsum(np.asarray(medians))\n",
    "        \n",
    "        self.setMedianPositions(medians)\n",
    "        self.setMeanPositions(means)\n",
    "        \n",
    "        trailingSteps=[0]*len(self.sids)\n",
    "        for i,path in enumerate(self.sids):\n",
    "            pos=self.getPositions(self.keyEvts, path.getHashList(evtAttr))\n",
    "            trailingSteps[i]= len(path.events)- pos[-1]\n",
    "        \n",
    "        trailStepSum=sum(trailingSteps)\n",
    "        median= self.getMedian(trailingSteps)\n",
    "        mean= trailStepSum/len(trailingSteps)\n",
    "        \n",
    "        self.setMedianPathLength(median+medians[-1])\n",
    "        self.setMeanPathLength(mean+means[-1])\n",
    "                                  \n",
    "    def getMedianPositions(self, allPos, pids):\n",
    "        median=[]\n",
    "        for k in range(0, len(pid)):\n",
    "            posInPaths=allPos[k]\n",
    "            median.append(self.getMedian(posInPaths))\n",
    "        #return list(self.getMedian(posInPaths) for posInPaths in allPos)\n",
    "        return median\n",
    "    \n",
    "    def getMeanPositions(self, allPos, pids):\n",
    "        mean=[]\n",
    "        for k in range(0, len(allPos)):\n",
    "            mean.append(sum(allPos[k])*1.0/(len(allPos[k])))\n",
    "        return mean\n",
    "    \n",
    "    def setMedianPositions(self, median):\n",
    "        self.medianPos=median\n",
    "        \n",
    "    def setMeanPositions(self, mean):\n",
    "        self.meanPos=mean\n",
    "        \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4)\n",
    "    \n",
    "    def getSupport(self):\n",
    "        return this.support\n",
    "    \n",
    "    def setCluster(self, cluster):\n",
    "        self.cluster=cluster\n",
    "        \n",
    "    def setParent(self, parent, segment):\n",
    "        self.parent=parent\n",
    "        self.parentSegment=segment\n",
    "    \n",
    "    \n",
    "    # How to implement this with BitArray?\n",
    "    #def getEventBitSet(self)\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    \n",
    "    def getParentSegment(self):\n",
    "        return self.parentSegment\n",
    "    \n",
    "    def setMeanPathLength(self,d):\n",
    "        self.meanPathLength=d\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "        \n",
    "    def setSupport(self, sup, total):\n",
    "        self.support=sup\n",
    "        self.supPercent= sup*1.0/total\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowNode Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowNode:\n",
    "    NID=count(1)\n",
    "    nodeHash={}\n",
    "    \n",
    "    def __init__(self, name=\"\", count=0, value=\"\"):\n",
    "        self.nid=next(self.NID)\n",
    "        self.name=name\n",
    "        self.seqCount=count\n",
    "        self.value=value\n",
    "        self.hash=-1\n",
    "        self.pos=[]\n",
    "        self.meanStep=0\n",
    "        self.medianStep=0\n",
    "        self.zipCompressRatio=0\n",
    "        self.incomingBranchUniqueEvts=None\n",
    "        self.incomingBranchSimMean=None\n",
    "        self.incomingBranchSimMedian=None\n",
    "        self.incomingBranchSimVariance=None\n",
    "        \n",
    "        self.incomingSequences=[]\n",
    "        self.outgoingSequences=[]\n",
    "        \n",
    "        self.meanRelTimestamp=0\n",
    "        self.medianRelTimestamp=0\n",
    "        \n",
    "        nodeHash[self.nid]=self\n",
    "        \n",
    "        \n",
    "    def getNode(self, node_id):\n",
    "        return nodeHash[node_id]\n",
    "    \n",
    "    def clearHash(self):\n",
    "        nodeHash.clear()\n",
    "        \n",
    "    def getIncomingSequences(self):\n",
    "        return self.incomingSequences\n",
    "    \n",
    "    def getSeqCount(self):\n",
    "        return self.seqCount\n",
    "    \n",
    "    def setSeqCount(self, seqCount):\n",
    "        self.seqCount=seqCount\n",
    "        \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def setName(self, name):\n",
    "        self.name=name\n",
    "        \n",
    "    def getMeanStep(self):\n",
    "        return self.meanStep\n",
    "    \n",
    "    def toString(self):\n",
    "        return self.name+\": \"+self.seqCount\n",
    "    \n",
    "    def setPositions(self, l):\n",
    "        self.pos=l\n",
    "        self.sort(l)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "\n",
    "    # Helper function to return a data frame\n",
    "    # Local is boolean, if local then source should be path to the file\n",
    "    # Otherwise it should be a URL to the the file\n",
    "\n",
    "    def get_dataframe( src, local=False, sep=\"\\t\", header=[]):\n",
    "        if not local:\n",
    "            # To force a dropbox link to download change the dl=0 to 1\n",
    "            if \"dropbox\" in src:\n",
    "                src = src.replace('dl=0', 'dl=1')\n",
    "            # Download the CSV at url\n",
    "            req = requests.get(src)\n",
    "            url_content = req.content\n",
    "            csv_file = open('data.txt', 'wb') \n",
    "            csv_file.write(url_content)\n",
    "            csv_file.close()\n",
    "            # Read the CSV into pandas\n",
    "            # If header list is empty, the dataset provides header so ignore param\n",
    "            if not header:\n",
    "                df = pd.read_csv(\"data.txt\", sep)\n",
    "            #else use header param for column names\n",
    "            else:\n",
    "                df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "            # Delete the csv file\n",
    "            os.remove(\"data.txt\")\n",
    "            return df\n",
    "        # Dataset is local\n",
    "        else:\n",
    "            # If header list is empty, the dataset provides header so ignore param\n",
    "            if not header:\n",
    "                df = pd.read_csv(src, sep)\n",
    "            # else use header param for column names\n",
    "            else:\n",
    "                df = pd.read_csv(src, sep, names=header)\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    \n",
    "    def importPointEvents(src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = EventStore.get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != timestampColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = PointEvent(timestamp, attribs)\n",
    "            events.append(e)\n",
    "        return events\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    \n",
    "    def importIntervalEvents( src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for the start and end times of the event\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        return events\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "\n",
    "    def importMixedEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "            if type(data[endTimeColumnIdx]) is float:\n",
    "                t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                event_type = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "                event_type = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "            attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "            for i in attribute_columns:\n",
    "                attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            if event_type == \"point\":\n",
    "                e = PointEvent(t, attribs)\n",
    "            else:\n",
    "                e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        return events\n",
    "    \n",
    "    \n",
    "    # Group events by attributeName, and order them by timestamp\n",
    "    \n",
    "    def generateSequence(eventList, attributeName):\n",
    "        grouped_by = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in grouped_by for value\n",
    "            if value in grouped_by:\n",
    "                grouped_by[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                grouped_by[value] = [event]\n",
    "        return list(grouped_by.values())\n",
    "    \n",
    "    # Helper to insert an event into a map\n",
    "    # Params are key=unique id for that time, map of key to event list, event object\n",
    "    \n",
    "    def insert_event_into_dict(key, dictionary, event):\n",
    "        if key in dictionary:\n",
    "            dictionary[key].append(event)\n",
    "        else:\n",
    "            dictionary[key] = [event]\n",
    "            \n",
    "    # Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "    # Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "    def get_time_to_sort_by(e):\n",
    "        # Sort by starting time of event if its an interval event\n",
    "        if type(e) == IntervalEvent:\n",
    "            return e.time[0]\n",
    "        # Otherwise use the timestamp\n",
    "        else:\n",
    "            return e.timestamp\n",
    "\n",
    "\n",
    "    # Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "    # break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "    # “week”, “month”, “quarter”, and “year”.\n",
    "    # For interval events I used the start time of the event to determine its category when splitting it\n",
    "    \n",
    "    def splitSequences(sequenceList, timeUnit, record=None):\n",
    "        results = []\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in valid_time_units:\n",
    "            raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "        # Sort the events by the timestamp or event start time\n",
    "        sequenceList = sorted(sequenceList, key=EventStore.get_time_to_sort_by)\n",
    "\n",
    "        # Process the event sequence based on the given time unit\n",
    "        # Generally, create a map for that time unit and then add each event into that map \n",
    "        # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "        if timeUnit == \"hour\":\n",
    "            hours = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                key = (time.hour, time.day, time.month, time.year)\n",
    "                EventStore.insert_event_into_dict(key,hours,event)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "            results = list(hours.values())\n",
    "\n",
    "        elif timeUnit == \"day\":\n",
    "            days = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                key = (time.day, time.month, time.year)\n",
    "                EventStore.insert_event_into_dict(key,days,event)\n",
    "                #print(days)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "            results = list(days.values())\n",
    "\n",
    "        elif timeUnit == \"month\":\n",
    "            months = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                key = (time.month,time.year)\n",
    "                EventStore.insert_event_into_dict(key,months,event)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "            results = list(months.values())\n",
    "\n",
    "        elif timeUnit == \"week\":\n",
    "            weeks = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                year = time.year\n",
    "                week_num = time.isocalendar()[1]\n",
    "                key = (year,week_num)\n",
    "                EventStore.insert_event_into_dict(key,weeks,event)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "            results = list(weeks.values())\n",
    "\n",
    "        elif timeUnit == \"year\":\n",
    "            years = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                key = time.year\n",
    "                EventStore.insert_event_into_dict(key,years,event)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=str(key)\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "            results = list(years.values())\n",
    "\n",
    "        elif timeUnit == \"quarter\":\n",
    "            quarters = {}\n",
    "            for event in sequenceList:\n",
    "                time = EventStore.get_time_to_sort_by(event)\n",
    "                year = time.year\n",
    "                month = time.month\n",
    "                # Determine the year, quarter pair/key for quarter dict\n",
    "                # January, February, and March (Q1)\n",
    "                if month in range(1, 4):\n",
    "                    key = (year, \"Q1\")\n",
    "                # April, May, and June (Q2)\n",
    "                elif month in range(4, 7):\n",
    "                    key = (year, \"Q2\")\n",
    "                # July, August, and September (Q3)\n",
    "                elif month in range(7,10):\n",
    "                    key = (year, \"Q3\")\n",
    "                # October, November, and December (Q4)\n",
    "                elif month in range(10,13):\n",
    "                    key = (year, \"Q4\")\n",
    "                # Put the event in the dictionary\n",
    "                EventStore.insert_event_into_dict(key,quarters,event)\n",
    "                if record is None:\n",
    "                    event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                else:\n",
    "                    event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "            results = list(quarters.values())\n",
    "\n",
    "        return results   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 233, 52, 67, 309, 66, 80, 168, 171, 56, 116, 64, 172, 68, 69, 131, 74, 161, 189, 181, 173, 198, 217, 84, 93, 141, 55, 73, 191, 82, 102, 288, 69, 239, 199, 89, 64, 63, 78, 77, 104, 67, 102, 76, 76, 147, 69, 61, 68, 44, 172, 74, 64, 65, 59, 59, 76, 88, 73, 94, 58, 69, 89, 149, 175, 138, 154, 63, 67, 171, 60, 109, 71, 72, 82, 56, 80, 80, 65, 134, 112, 65, 75, 102, 153, 103, 83, 76, 134, 81, 75, 98, 76, 113, 69, 78, 80, 66, 81, 81, 122, 77, 106, 95, 73, 58, 56, 87, 174, 147, 114, 153, 115, 126, 76, 67, 128, 65, 56, 96, 65, 71, 109, 58, 176, 151, 172, 214, 78, 199, 234, 57, 123, 140, 224, 148, 69, 132, 66, 49, 46, 59, 66, 357, 137, 155, 142, 156, 98, 84, 118, 111, 108, 76, 116, 137, 90, 100, 87, 105, 58, 113, 140, 198, 162, 180, 90, 66, 83, 51, 158, 57, 85, 57, 50, 204, 121, 112, 54, 62, 72, 172, 178, 84, 145, 77, 198, 175, 68, 107, 88, 125, 58, 175, 172, 174, 124, 205, 69, 66, 56, 94, 264, 237, 136, 121, 66, 173, 113, 49, 151, 84, 135, 162, 259, 67, 93, 318, 48, 132, 76, 72, 75, 71, 208, 56, 47, 76, 58, 67, 78, 69, 75, 187, 56, 68, 66, 67, 94, 64, 60, 61, 111, 85, 171, 222, 222, 140, 96, 105, 78, 148, 86, 116, 129, 141, 226, 117, 349, 225, 106, 157, 67, 139, 130, 69, 52, 69, 138, 168, 104, 175, 59, 172, 166, 166, 126, 95, 108, 78, 119, 211, 283, 214, 81, 65, 81, 163, 173, 203, 80, 113, 186, 64, 68, 73, 139, 95, 62, 66, 54, 53, 102, 89, 101, 253, 94, 50, 110, 105, 59, 148, 182, 132, 78, 98, 107, 176, 64, 127, 93, 82, 74, 53, 101, 106, 110, 69, 79, 130, 154, 70, 50, 89, 82, 71, 89, 124, 87, 65, 126, 87, 105, 123, 73, 98, 73, 74, 235, 55, 47, 184, 63, 76, 59, 71, 111, 62, 67, 67, 67, 104, 123, 63, 103, 51, 248, 82, 137, 94, 105, 261, 174, 60, 60, 69, 60, 56, 42, 85, 248, 114, 142, 155, 191, 192, 105, 165, 132, 154, 153, 167, 126, 139, 155, 100, 113, 378, 253, 210, 74, 91, 154, 174, 190, 119, 74, 235, 290, 248, 82, 159, 246, 292, 148, 86, 70, 82, 67, 73, 55, 63, 61, 108, 239, 109, 158, 321, 258, 241, 141, 87, 318, 77, 90, 219, 161, 145, 160, 103, 64, 226, 118, 52, 122, 156, 74, 227, 154, 58, 275, 133, 231, 70, 86, 175, 130, 113, 150, 131, 75, 217, 74, 193, 82, 82, 94, 122, 182, 90, 200, 61, 61, 117, 82, 86, 135, 76, 135, 190, 240, 63, 174, 232, 306, 306, 227, 140, 196, 53, 59, 104, 58, 81, 66, 102, 297, 61, 68, 68, 164, 124, 74, 70, 106, 78, 46, 54, 152, 63, 62, 161, 176, 81, 68, 53, 76, 258]\n",
      "38 -1 233 -1 52 -1 67 -1 309 -1 66 -1 80 -1 168 -1 171 -1 56 -1 116 -1 64 -1 172 -1 68 -1 69 -1 131 -1 74 -1 161 -1 189 -1 181 -1 173 -1 198 -1 217 -1 84 -1 93 -1 141 -1 55 -1 73 -1 191 -1 82 -1 102 -1 288 -1 69 -1 239 -1 199 -1 89 -1 64 -1 63 -1 78 -1 77 -1 104 -1 67 -1 102 -1 76 -1 76 -1 147 -1 69 -1 61 -1 68 -1 44 -1 172 -1 74 -1 64 -1 65 -1 59 -1 59 -1 76 -1 88 -1 73 -1 94 -1 58 -1 69 -1 89 -1 149 -1 175 -1 138 -1 154 -1 63 -1 67 -1 171 -1 60 -1 109 -1 71 -1 72 -1 82 -1 56 -1 80 -1 80 -1 65 -1 134 -1 112 -1 65 -1 75 -1 102 -1 153 -1 103 -1 83 -1 76 -1 134 -1 81 -1 75 -1 98 -1 76 -1 113 -1 69 -1 78 -1 80 -1 66 -1 81 -1 81 -1 122 -1 77 -1 106 -1 95 -1 73 -1 58 -1 56 -1 87 -1 174 -1 147 -1 114 -1 153 -1 115 -1 126 -1 76 -1 67 -1 128 -1 65 -1 56 -1 96 -1 65 -1 71 -1 109 -1 58 -1 176 -1 151 -1 172 -1 214 -1 78 -1 199 -1 234 -1 57 -1 123 -1 140 -1 224 -1 148 -1 69 -1 132 -1 66 -1 49 -1 46 -1 59 -1 66 -1 357 -1 137 -1 155 -1 142 -1 156 -1 98 -1 84 -1 118 -1 111 -1 108 -1 76 -1 116 -1 137 -1 90 -1 100 -1 87 -1 105 -1 58 -1 113 -1 140 -1 198 -1 162 -1 180 -1 90 -1 66 -1 83 -1 51 -1 158 -1 57 -1 85 -1 57 -1 50 -1 204 -1 121 -1 112 -1 54 -1 62 -1 72 -1 172 -1 178 -1 84 -1 145 -1 77 -1 198 -1 175 -1 68 -1 107 -1 88 -1 125 -1 58 -1 175 -1 172 -1 174 -1 124 -1 205 -1 69 -1 66 -1 56 -1 94 -1 264 -1 237 -1 136 -1 121 -1 66 -1 173 -1 113 -1 49 -1 151 -1 84 -1 135 -1 162 -1 259 -1 67 -1 93 -1 318 -1 48 -1 132 -1 76 -1 72 -1 75 -1 71 -1 208 -1 56 -1 47 -1 76 -1 58 -1 67 -1 78 -1 69 -1 75 -1 187 -1 56 -1 68 -1 66 -1 67 -1 94 -1 64 -1 60 -1 61 -1 111 -1 85 -1 171 -1 222 -1 222 -1 140 -1 96 -1 105 -1 78 -1 148 -1 86 -1 116 -1 129 -1 141 -1 226 -1 117 -1 349 -1 225 -1 106 -1 157 -1 67 -1 139 -1 130 -1 69 -1 52 -1 69 -1 138 -1 168 -1 104 -1 175 -1 59 -1 172 -1 166 -1 166 -1 126 -1 95 -1 108 -1 78 -1 119 -1 211 -1 283 -1 214 -1 81 -1 65 -1 81 -1 163 -1 173 -1 203 -1 80 -1 113 -1 186 -1 64 -1 68 -1 73 -1 139 -1 95 -1 62 -1 66 -1 54 -1 53 -1 102 -1 89 -1 101 -1 253 -1 94 -1 50 -1 110 -1 105 -1 59 -1 148 -1 182 -1 132 -1 78 -1 98 -1 107 -1 176 -1 64 -1 127 -1 93 -1 82 -1 74 -1 53 -1 101 -1 106 -1 110 -1 69 -1 79 -1 130 -1 154 -1 70 -1 50 -1 89 -1 82 -1 71 -1 89 -1 124 -1 87 -1 65 -1 126 -1 87 -1 105 -1 123 -1 73 -1 98 -1 73 -1 74 -1 235 -1 55 -1 47 -1 184 -1 63 -1 76 -1 59 -1 71 -1 111 -1 62 -1 67 -1 67 -1 67 -1 104 -1 123 -1 63 -1 103 -1 51 -1 248 -1 82 -1 137 -1 94 -1 105 -1 261 -1 174 -1 60 -1 60 -1 69 -1 60 -1 56 -1 42 -1 85 -1 248 -1 114 -1 142 -1 155 -1 191 -1 192 -1 105 -1 165 -1 132 -1 154 -1 153 -1 167 -1 126 -1 139 -1 155 -1 100 -1 113 -1 378 -1 253 -1 210 -1 74 -1 91 -1 154 -1 174 -1 190 -1 119 -1 74 -1 235 -1 290 -1 248 -1 82 -1 159 -1 246 -1 292 -1 148 -1 86 -1 70 -1 82 -1 67 -1 73 -1 55 -1 63 -1 61 -1 108 -1 239 -1 109 -1 158 -1 321 -1 258 -1 241 -1 141 -1 87 -1 318 -1 77 -1 90 -1 219 -1 161 -1 145 -1 160 -1 103 -1 64 -1 226 -1 118 -1 52 -1 122 -1 156 -1 74 -1 227 -1 154 -1 58 -1 275 -1 133 -1 231 -1 70 -1 86 -1 175 -1 130 -1 113 -1 150 -1 131 -1 75 -1 217 -1 74 -1 193 -1 82 -1 82 -1 94 -1 122 -1 182 -1 90 -1 200 -1 61 -1 61 -1 117 -1 82 -1 86 -1 135 -1 76 -1 135 -1 190 -1 240 -1 63 -1 174 -1 232 -1 306 -1 306 -1 227 -1 140 -1 196 -1 53 -1 59 -1 104 -1 58 -1 81 -1 66 -1 102 -1 297 -1 61 -1 68 -1 68 -1 164 -1 124 -1 74 -1 70 -1 106 -1 78 -1 46 -1 54 -1 152 -1 63 -1 62 -1 161 -1 176 -1 81 -1 68 -1 53 -1 76 -1 258 -2\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding = EventStore.importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding)\n",
    "#seq.getEventPosition('Meal','Lunch')\n",
    "#print(seq.getUniqueValueHashes('Meal'))\n",
    "#print(seq.getHashList('Glucose'))\n",
    "print(seq.getValueHashes('Glucose'))\n",
    "#print(seq.getEventsHashString('Glucose'))\n",
    "raw_seq=seq.convertToVMSPReadable('Glucose')\n",
    "print(seq.convertToVMSPReadable('Glucose'))\n",
    "#print(seq.getPathID())\n",
    "#sequence_braiding[0].attributes.keys()\n",
    "#print(sequence_braiding[0].getAttrVal('Meals'))\n",
    "#print(sequence_braiding[0].type)\n",
    "#for events in sequence_braiding:\n",
    "#    print(events.getAttrVal('Meal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding_split=EventStore.splitSequences(sequence_braiding, \"week\")\n",
    "seq_list=[]\n",
    "for seqs in sequence_braiding_split:\n",
    "    seq_list.append(Sequence(seqs))\n",
    "    \n",
    "raw_seq=\"\\n\".join( seqs.convertToVMSPReadable('Glucose') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 -1 62 -1 161 -1 176 -1 81 -1 68 -1 53 -1 76 -1 258 -1 124 -1 74 -1 70 -1 106 -1 78 -1 46 -1 54 -1 152 -1 61 -1 68 -1 68 -1 164 -2\n",
      "66 -1 102 -1 297 -1 59 -1 104 -1 58 -1 81 -1 140 -1 196 -1 53 -1 174 -1 232 -1 306 -1 306 -1 227 -1 190 -1 240 -1 63 -1 76 -1 135 -1 135 -2\n",
      "82 -1 86 -1 200 -1 61 -1 61 -1 117 -1 122 -1 182 -1 90 -1 193 -1 82 -1 82 -1 94 -1 150 -1 131 -1 75 -1 217 -1 74 -1 130 -1 113 -1 175 -2\n",
      "133 -1 231 -1 70 -1 86 -1 154 -1 58 -1 275 -1 74 -1 227 -1 118 -1 52 -1 122 -1 156 -1 103 -1 64 -1 226 -1 160 -1 145 -2\n",
      "77 -1 90 -1 219 -1 161 -1 241 -1 141 -1 87 -1 318 -1 109 -1 158 -1 321 -1 258 -1 55 -1 63 -1 61 -1 108 -1 239 -1 82 -1 67 -1 73 -1 86 -1 70 -1 292 -1 148 -2\n",
      "82 -1 159 -1 246 -1 119 -1 74 -1 235 -1 290 -1 248 -1 210 -1 74 -1 91 -1 154 -1 174 -1 190 -1 100 -1 113 -1 378 -1 253 -1 167 -1 126 -1 139 -1 155 -1 154 -1 153 -1 105 -1 165 -1 132 -2\n",
      "142 -1 155 -1 191 -1 192 -1 69 -1 60 -1 56 -1 42 -1 85 -1 248 -1 114 -1 137 -1 94 -1 105 -1 261 -1 174 -1 60 -1 60 -1 123 -1 63 -1 103 -1 51 -1 248 -1 82 -1 63 -1 76 -1 59 -1 71 -1 111 -1 62 -1 67 -1 67 -1 67 -1 104 -1 74 -1 235 -1 55 -1 47 -1 184 -1 98 -1 73 -2\n",
      "87 -1 105 -1 123 -1 73 -1 71 -1 89 -1 124 -1 87 -1 65 -1 126 -1 50 -1 89 -1 82 -1 110 -1 69 -1 79 -1 130 -1 154 -1 70 -1 93 -1 82 -1 74 -1 53 -1 101 -1 106 -1 176 -1 64 -1 127 -1 98 -1 107 -2\n",
      "59 -1 148 -1 182 -1 132 -1 78 -1 94 -1 50 -1 110 -1 105 -1 89 -1 101 -1 253 -1 54 -1 53 -1 102 -1 139 -1 95 -1 62 -1 66 -1 64 -1 68 -1 73 -1 186 -2\n",
      "65 -1 81 -1 163 -1 173 -1 203 -1 80 -1 113 -1 211 -1 283 -1 214 -1 81 -1 95 -1 108 -1 78 -1 119 -1 59 -1 172 -1 166 -1 166 -1 126 -1 138 -1 168 -1 104 -1 175 -1 67 -1 139 -1 130 -1 69 -1 52 -1 69 -1 225 -1 106 -1 157 -2\n",
      "141 -1 226 -1 117 -1 349 -1 148 -1 86 -1 116 -1 129 -1 105 -1 78 -1 171 -1 222 -1 222 -1 140 -1 96 -1 60 -1 61 -1 111 -1 85 -1 67 -1 94 -1 64 -1 187 -1 56 -1 68 -1 66 -2\n",
      "56 -1 47 -1 76 -1 58 -1 67 -1 78 -1 69 -1 75 -1 48 -1 132 -1 76 -1 72 -1 75 -1 71 -1 208 -1 162 -1 259 -1 67 -1 93 -1 318 -1 49 -1 151 -1 84 -1 135 -1 66 -1 173 -1 113 -1 264 -1 237 -1 136 -1 121 -1 56 -1 94 -2\n",
      "174 -1 124 -1 205 -1 69 -1 66 -1 125 -1 58 -1 175 -1 172 -1 107 -1 88 -1 145 -1 77 -1 198 -1 175 -1 68 -1 72 -1 172 -1 178 -1 84 -1 112 -1 54 -1 62 -1 121 -2\n",
      "57 -1 50 -1 204 -1 57 -1 85 -1 66 -1 83 -1 51 -1 158 -1 140 -1 198 -1 162 -1 180 -1 90 -1 105 -1 58 -1 113 -1 100 -1 87 -1 90 -2\n",
      "76 -1 116 -1 137 -1 118 -1 111 -1 108 -1 156 -1 98 -1 84 -1 137 -1 155 -1 142 -1 49 -1 46 -1 59 -1 66 -1 357 -1 69 -1 132 -1 66 -1 148 -2\n",
      "123 -1 140 -1 224 -1 234 -1 57 -1 199 -1 151 -1 172 -1 214 -1 78 -1 56 -1 96 -1 65 -1 71 -1 109 -1 58 -1 176 -1 76 -1 67 -1 128 -1 65 -1 115 -1 126 -2\n",
      "174 -1 147 -1 114 -1 153 -1 73 -1 58 -1 56 -1 87 -1 106 -1 95 -1 122 -1 77 -1 69 -1 78 -1 80 -1 66 -1 81 -1 81 -1 76 -1 113 -1 98 -2\n",
      "81 -1 75 -1 76 -1 134 -1 153 -1 103 -1 83 -1 112 -1 65 -1 75 -1 102 -1 72 -1 82 -1 56 -1 80 -1 80 -1 65 -1 134 -1 60 -1 109 -1 71 -2\n",
      "175 -1 138 -1 154 -1 63 -1 67 -1 171 -1 69 -1 89 -1 149 -1 94 -1 58 -1 59 -1 59 -1 76 -1 88 -1 73 -1 74 -1 64 -1 65 -1 68 -1 44 -1 172 -1 61 -2\n",
      "102 -1 76 -1 76 -1 147 -1 69 -1 89 -1 64 -1 63 -1 78 -1 77 -1 104 -1 67 -1 288 -1 69 -1 239 -1 199 -1 55 -1 73 -1 191 -1 82 -1 102 -1 84 -1 93 -1 141 -1 198 -1 217 -1 173 -2\n",
      "74 -1 161 -1 189 -1 181 -1 68 -1 69 -1 131 -1 64 -1 172 -1 171 -1 56 -1 116 -1 66 -1 80 -1 168 -1 67 -1 309 -1 233 -1 52 -2\n",
      "38 -2\n"
     ]
    }
   ],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233, 309, 106, 166]\n",
      "[1, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "pat=Pattern([233,309,106,166])\n",
    "print(pat.keyEvts)\n",
    "#print(pat.filterPaths([seq],'Glucose'))\n",
    "#print(pat.getUniqueEventsString())\n",
    "print(pat.getPositions([233,309,80,168],seq.getValueHashes('Glucose')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example_raw = \"\"\"1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2\n",
    "1 4 -1 3 -1 2 3 -1 1 5 -1 -2\n",
    "5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2\n",
    "5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "spmf.jar not found. Please use the spmf_bin_location_dir argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9eae219c2d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m spmf = Spmf(\"VMSP\", spmf_bin_location_dir=\"./test_files/\", input_direct=raw_seq,\n\u001b[0;32m----> 2\u001b[0;31m             output_filename=\"output.txt\", arguments=[0.2])\n\u001b[0m",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/site-packages/spmf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, algorithm_name, input_direct, input_type, input_filename, output_filename, arguments, spmf_bin_location_dir, memory)\u001b[0m\n\u001b[1;32m     34\u001b[0m                                             self.executable_))):\n\u001b[1;32m     35\u001b[0m             raise FileNotFoundError(self.executable_ + \" not found. Please\" +\n\u001b[0;32m---> 36\u001b[0;31m                                     \" use the spmf_bin_location_dir argument.\")\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magorithm_name_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: spmf.jar not found. Please use the spmf_bin_location_dir argument."
     ]
    }
   ],
   "source": [
    "spmf = Spmf(\"VMSP\", spmf_bin_location_dir=\"./test_files/\", input_direct=raw_seq,\n",
    "            output_filename=\"output.txt\", arguments=[0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spmf.to_pandas_dataframe(pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spmf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
