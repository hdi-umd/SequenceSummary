{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf\n",
    "import json\n",
    "import jsonpickle\n",
    "import heapq\n",
    "\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common class for all Events\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, eventtype):\n",
    "        self.type=eventtype\n",
    "    \n",
    "    #Return Attribute value given attribute name\n",
    "    def getAttrVal(self, attrName):\n",
    "        return self.attributes.get(attrName,None)\n",
    "\n",
    "    \n",
    "# A class that represents a point event\n",
    "class PointEvent(Event):\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        super().__init__(\"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "        \n",
    "    \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent(Event):\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        super().__init__(\"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    \n",
    "    def __init__(self, eventlist=[]):\n",
    "        self.attrdict={}\n",
    "        self.reverseatttrdict={}\n",
    "        self.events=eventlist\n",
    "\n",
    "    #should be moved to EventStore\n",
    "    # hold the list of events, also the dictionaries\n",
    "    \n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importPointEvents(self, src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != timestampColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = PointEvent(timestamp, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events\n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importIntervalEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for the start and end times of the event\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events    \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    #@staticmethod\n",
    "    def importMixedEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "            if type(data[endTimeColumnIdx]) is float:\n",
    "                t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                event_type = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "                event_type = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "            attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "            for i in attribute_columns:\n",
    "                attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            if event_type == \"point\":\n",
    "                e = PointEvent(t, attribs)\n",
    "            else:\n",
    "                e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events   \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    #should take an eventlist as input\n",
    "    # Group events by attributeName, and order them by timestamp\n",
    "    #@staticmethod\n",
    "    #should return a list of sequences\n",
    "    def generateSequence(self, attributeName):\n",
    "        eventList=self.events\n",
    "        grouped_by = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in grouped_by for value\n",
    "            if value in grouped_by:\n",
    "                grouped_by[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                grouped_by[value] = [event]\n",
    "        sequences= list(grouped_by.values())\n",
    "        seqlist=[]\n",
    "        for seq in sequences:\n",
    "            seqlist.append(Sequence(seq, self))\n",
    "        return seqlist\n",
    "    \n",
    "    # Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "    # break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "    # “week”, “month”, “quarter”, and “year”.\n",
    "    # For interval events I used the start time of the event to determine its category when splitting it\n",
    "    \n",
    "    #ZINAT- changes\n",
    "    #SequenceList represents a list of objects of type Sequence. The sequences are further splitted into\n",
    "    #sequence objects, this way we can use generate sequences and then splitSequences \n",
    "    @staticmethod\n",
    "    def splitSequences(sequenceLists, timeUnit, record=None):\n",
    "        if not isinstance(sequenceLists, list):\n",
    "            sequenceLists=[sequenceLists]\n",
    "        eventstore=sequenceLists[0].eventstore\n",
    "        results = []\n",
    "        resultlist=[]\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in valid_time_units:\n",
    "            raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "        \n",
    "        for sequence in sequenceLists:\n",
    "            # Sort the events by the timestamp or event start time\n",
    "            sequenceList= sequence.events\n",
    "            sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "\n",
    "            # Process the event sequence based on the given time unit\n",
    "            # Generally, create a map for that time unit and then add each event into that map \n",
    "            # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "            if timeUnit == \"hour\":\n",
    "                hours = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.hour, time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,hours,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "                results = list(hours.values())\n",
    "\n",
    "            elif timeUnit == \"day\":\n",
    "                days = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,days,event)\n",
    "                    #print(days)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                results = list(days.values())\n",
    "\n",
    "            elif timeUnit == \"month\":\n",
    "                months = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.month,time.year)\n",
    "                    insert_event_into_dict(key,months,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(months.values())\n",
    "\n",
    "            elif timeUnit == \"week\":\n",
    "                weeks = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    week_num = time.isocalendar()[1]\n",
    "                    key = (year,week_num)\n",
    "                    insert_event_into_dict(key,weeks,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "                results = list(weeks.values())\n",
    "\n",
    "            elif timeUnit == \"year\":\n",
    "                years = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = time.year\n",
    "                    insert_event_into_dict(key,years,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key)\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "                results = list(years.values())\n",
    "\n",
    "            elif timeUnit == \"quarter\":\n",
    "                quarters = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    month = time.month\n",
    "                    # Determine the year, quarter pair/key for quarter dict\n",
    "                    # January, February, and March (Q1)\n",
    "                    if month in range(1, 4):\n",
    "                        key = (year, \"Q1\")\n",
    "                    # April, May, and June (Q2)\n",
    "                    elif month in range(4, 7):\n",
    "                        key = (year, \"Q2\")\n",
    "                    # July, August, and September (Q3)\n",
    "                    elif month in range(7,10):\n",
    "                        key = (year, \"Q3\")\n",
    "                    # October, November, and December (Q4)\n",
    "                    elif month in range(10,13):\n",
    "                        key = (year, \"Q4\")\n",
    "                    # Put the event in the dictionary\n",
    "                    insert_event_into_dict(key,quarters,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(quarters.values())\n",
    "            resultlist.extend(results)\n",
    "        resultlists= [Sequence(x, eventstore) for x in resultlist]\n",
    "\n",
    "        return resultlists\n",
    "    \n",
    "    def getUniqueValues(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return l\n",
    "    \n",
    "    #Assuming we are given a list of events and from those events we create \n",
    "    #the mapping and reverse mapping dictionary\n",
    "    def create_attr_dict(self):\n",
    "        attr_list=self.events[0].attributes.keys()\n",
    "        print(attr_list)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            a=48\n",
    "            unique_list=[]\n",
    "            unique_list.extend(self.getUniqueValues(attr))\n",
    "            unique_list=list(set(unique_list))\n",
    "            #unique_list.clear()\n",
    "            \n",
    "            unicode_dict={}\n",
    "            reverse_dict={}\n",
    "            for uniques in unique_list:\n",
    "                unicode_dict[uniques]=chr(a)\n",
    "                reverse_dict[chr(a)]=uniques\n",
    "                a=a+1\n",
    "            self.attrdict[attr]=unicode_dict\n",
    "            self.reverseatttrdict[attr]=reverse_dict\n",
    "            #unicode_dict.clear()                    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence():\n",
    "    _ids = count(0)\n",
    "    \n",
    "\n",
    "    def __init__(self,  eventlist, eventstore,sid=None):\n",
    "        # sequence id\n",
    "        if sid is None:\n",
    "            self.sid = next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "        \n",
    "        self.events = eventlist\n",
    "        self.eventstore=eventstore\n",
    "        self.volume=1\n",
    "        self.seqAttributes={}\n",
    "        self.seqIndices=[]\n",
    "    def getEventPosition(self, attr, hash_val):\n",
    "        for count,event in enumerate(self.events):\n",
    "            #if event.getAttrVal(attr)==hash_val:\n",
    "            if self.eventstore.attrdict[attr][event.getAttrVal(attr)]==hash_val:\n",
    "                return count\n",
    "        return -1\n",
    "    \n",
    "    def setVolume(self, intValue):\n",
    "        self.volume=intValue\n",
    "        \n",
    "    def getVolume(self):\n",
    "        return self.volume\n",
    "    \n",
    "    def increaseVolume(self):\n",
    "        self.volume += 1 \n",
    "    \n",
    "    \n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        uniquelist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        return uniquelist\n",
    "    \n",
    "    #Not sure this will always result in same index, will change if \n",
    "    #dictionary is updated\n",
    "    #since python is unordered\n",
    "    \n",
    "    def getHashList(self, attr):\n",
    "        #l=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        l=[event.getAttrVal(attr) for event in self.events]\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "    \n",
    "    def getValueHashes(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "        \n",
    "    \n",
    "    def getEventsHashString(self, attr):\n",
    "        s=\"\"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"\".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #print(s)\n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadablenum(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" -1 \".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\" -2\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" \".join(self.eventstore.attrdict[attr][elem] for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\".\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def getPathID(self):\n",
    "        return self.sid\n",
    "    \n",
    "    def matchPathAttribute(self, attr, val):\n",
    "        # should i use eq?!\n",
    "        if this.seqAttributes.get(attr)==(val):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def setSequenceAttribute(self,attr, value):\n",
    "        self.seqAttributes[attr]=value\n",
    "        \n",
    "         \n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)    \n",
    "    def getSeqVolume(seqlist):\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "    \n",
    "    \n",
    "    # Method equivalent to public String getEvtAttrValue(String attr, int hash) in DataManager.java\n",
    "    def getEvtAttrValue(self, attr, hashval):\n",
    "        return self.eventstore.reverseatttrdict[attr][hashval]\n",
    "        \n",
    "    # Method equivalent to public List<String> getEvtAttrValues(String attr) in DataManager.java    \n",
    "    def getEvtAttrValues(self, attr):\n",
    "        return list(self.eventstore.reverseatttrdict[attr].values())\n",
    "    \n",
    "    # Method equivalent to int getEvtAttrValueCount(String attr) in DataManager.java    \n",
    "    def getEvtAttrValueCount(self, attr):\n",
    "        return len(self.eventstore.reverseatttrdict[attr])\n",
    "    \n",
    "    def getEventsString(self, attr):\n",
    "        s=\"\"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"\".join(elem for elem in l)\n",
    "        #print(s)\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    \n",
    "    def getUniqueEvents(seqlist):\n",
    "        l=list(set(event.getAttrVal(attr) for event in seq for seq in seqlist))\n",
    "        return l\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    NID=count(1)\n",
    "    nodeHash={}\n",
    "    \n",
    "    \n",
    "    def __init__(self, name=\"\", count=0, value=\"\", attr=\"\"):\n",
    "        self.nid=next(self.NID)\n",
    "        self.name=name\n",
    "        self.seqCount=count\n",
    "        ## What's the difference between name and value?\n",
    "        self.value=value\n",
    "        self.hash=-1\n",
    "        self.pos=[]\n",
    "        self.meanStep=0\n",
    "        self.medianStep=0\n",
    "        #self.zipCompressRatio=0\n",
    "        self.incomingBranchUniqueEvts=None\n",
    "        #self.incomingBranchSimMean=None\n",
    "        #self.incomingBranchSimMedian=None\n",
    "        #self.incomingBranchSimVariance=None\n",
    "        self.keyevts=[]\n",
    "        self.incomingSequences=[]\n",
    "        self.outgoingSequences=[]\n",
    "        \n",
    "        self.meanRelTimestamp=0\n",
    "        self.medianRelTimestamp=0\n",
    "        self.attr=attr\n",
    "        TreeNode.nodeHash[self.nid]=self\n",
    "        \n",
    "        \n",
    "    def getNode(self, node_id):\n",
    "        return nodeHash[node_id]\n",
    "    \n",
    "    def clearHash(self):\n",
    "        nodeHash.clear()\n",
    "        \n",
    "    def getIncomingSequences(self):\n",
    "        return self.incomingSequences\n",
    "    \n",
    "    def getSeqCount(self):\n",
    "        return self.seqCount\n",
    "    \n",
    "    def setSeqCount(self, seqCount):\n",
    "        self.seqCount=seqCount\n",
    "        \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def setName(self, name):\n",
    "        self.name=name\n",
    "        \n",
    "    def getMeanStep(self):\n",
    "        return self.meanStep\n",
    "    \n",
    "    #need a better implementation\n",
    "    def toJSONObject(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4) \n",
    "    \n",
    "    def toString(self):\n",
    "        return self.name+\": \"+self.seqCount\n",
    "    \n",
    "    def setPositions(self, l):\n",
    "        self.pos=l\n",
    "        self.pos.sort()\n",
    "        d=sum(self.pos)+len(self.pos)\n",
    "        mid=len(self.pos)/2\n",
    "        \n",
    "        if len(self.pos)==0:\n",
    "            self.meanStep=0\n",
    "            slf.medianStep=0\n",
    "        else:\n",
    "            #WHY WE ARE ADDING 1 to mean and medianStep?\n",
    "            self.meanStep=d/len(self.pos)\n",
    "            self.medianStep= np.median(self.pos)+1#((self.pos[mid-1]+self.pos[mid])/2.0)+1 if len(self.pos)%2==0 else self.pos[mid]+1\n",
    "            \n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "    def setValue(self, value):\n",
    "        self.value=value\n",
    "        \n",
    "    def getMedianStep(self):\n",
    "        return self.medianStep\n",
    "    \n",
    "    #def getZipCompressRatio(self):\n",
    "    #    return self.zipCompressRatio\n",
    "    \n",
    "    #def setZipCompressRatio(self, zipcompressratio):\n",
    "    #    self.zipCompressRatio=zipcompressratio\n",
    "        \n",
    "    def getIncomingBranchUniqueEvts(self):\n",
    "        return self.incomingBranchUniqueEvts\n",
    "    \n",
    "    def setIncomingBranchUniqueEvts(self, incomingbranchuniqueevts):\n",
    "        self.incomingBranchUniqueEvts=incomingbranchuniqueevts\n",
    "        \n",
    "    #def setIncomingBranchSimilarityStats(self, mean, median, variance):\n",
    "    #    self.incomingBranchSimMean=mean\n",
    "    #    self.incomingBranchSimMedian=median\n",
    "    #    self.incomingBranchSimVariance=variance\n",
    "        \n",
    "    \n",
    "    def setIncomingSequences(self, incomingbrancseqs, evtattr):\n",
    "        self.incomingSequences=incomingbrancseqs\n",
    "        \n",
    "    def setRelTimeStamps(self, reltimestamps):\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        reltimestamps.sort()\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        \n",
    "        mid=len(reltimestamps)/2\n",
    "        \n",
    "        if(len(reltimestamps)==0):\n",
    "            self.meanRelTimestamp=0\n",
    "            self.medianRelTimestamp=0\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            self.meanRelTimestamp=d*1.0/len(reltimestamps)\n",
    "            self.medianRelTimestamp=np.median(reltimestamps) #(reltimestamps[mid-1]+reltimestamps[mid])/2.0 if len(reltimestamps%2==0) else reltimestamps[mid]\n",
    "        \n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "    \n",
    "    \n",
    "    def getPatternString(self):\n",
    "        return \"-\".join(str(self.incomingSequences[0].eventstore.reverseatttrdict[self.attr][hashval]) for hashval in self.keyevts if self.incomingSequences)\n",
    "    def getHash():\n",
    "        return self.hash\n",
    "        d=sum(reltimestamps, timedelta())\n",
    "        \n",
    "        \n",
    "    def setHash(self, value):\n",
    "        self.hash=value\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def json_serialize(self):\n",
    "    #    json.dump(self, indent=4, default= TreeNode.json_default_dump)\n",
    "    def json_default_dump(self)-> dict:\n",
    "        pass\n",
    "    \n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(Node):\n",
    "    def __init__(self, name=\"\", count=0, value=\"\", attr=\"\"):\n",
    "        super().__init__(name, count, value,attr)\n",
    "        self.children = []\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"event_attribute\": self.hash,\n",
    "            \"value\": self.seqCount,\n",
    "            \"median_index\": self.medianStep,\n",
    "            \"average_index\":self.meanStep,\n",
    "\n",
    "            \"children\":[TreeNode.json_serialize_dump(x) for x in self.children]\n",
    "            \n",
    "        }\n",
    "    \n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=TreeNode.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(Node):\n",
    "    def __init__(self, name=\"\", count=0, value=\"\", attr=\"\"):\n",
    "        super().__init__(name, count, value, attr)\n",
    "        self.before = []\n",
    "        self.after = []\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"before\": GraphNode.json_serialize_dump(self.before),\n",
    "            \"event_attribute\": self.value,\n",
    "            \"Pattern\": self.getPatternString(),\n",
    "            \"value\": self.seqCount,\n",
    "            \"After\": GraphNode.json_serialize_dump(self.after)\n",
    "\n",
    "        }\n",
    "\n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=GraphNode.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rawnode:\n",
    "    def __init__ (self, node):\n",
    "        self.nid=node.nid\n",
    "        self.seqCount=node.seqCount\n",
    "        self.value=node.value \n",
    "        self.pattern=node.getPatternString()\n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"node id\": self.nid,\n",
    "            \"count\":self.seqCount,\n",
    "            \"Value\": self.value,\n",
    "            \"Pattern\": self.pattern\n",
    "        }\n",
    "\n",
    "class Links:\n",
    "    def __init__ (self, node1, node2):\n",
    "        self.source=node1\n",
    "        self.target=node2\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"source\": self.source,\n",
    "            \"target\":self.target\n",
    "        }\n",
    "\n",
    "        \n",
    "class Graph():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.links= []#defaultdict(set)\n",
    "        self.nodes=[]\n",
    "    \n",
    "    \n",
    "    def add(self, node1, node2):\n",
    "        self.links.append(Links(node1,node2))\n",
    "        #self.links[node2].add(node1)\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"links\":self.links\n",
    "\n",
    "        }\n",
    "\n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=Graph.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return None #obj.__dict__\n",
    "\n",
    "    \n",
    "        \n",
    "    def print_graph(self):\n",
    "        for node in self.nodes:\n",
    "            print(self.nid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenTree Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenTreeMiner:\n",
    "    \n",
    "    def expandSeqTree(self, attr, rootNode,  expandCnt, minSupport, maxSupport, graph):\n",
    "        \n",
    "        #if len(rootSeq.eventlist>0):\n",
    "        expandCnt-=len(rootNode.keyevts)\n",
    "        \n",
    "        seqs = []\n",
    "        seqs.append(rootNode)\n",
    "        rootNode.setSeqCount(Sequence.getSeqVolume(rootNode.incomingSequences))\n",
    "        rootNode.attr=attr\n",
    "        leafSeqs = []\n",
    "        \n",
    "        graph.nodes.append(Rawnode(rootNode))\n",
    "        while seqs and expandCnt > 0:\n",
    "            s = max(seqs,key=lambda x: x.seqCount) \n",
    "            print(f'seqCount: {s.seqCount}')\n",
    "            #print(f'this: {s}')\n",
    "\n",
    "            s0 = s.after\n",
    "            s1 = s.before\n",
    "            \n",
    "            #print(f' s : {s}')\n",
    "            #print(f' s0 : {s0}')\n",
    "            #print(f' s1: {s1}')\n",
    "            \n",
    "            print(f'this.pattern s : {s.keyevts}')\n",
    "            #print(f'this.pattern s0 : {s0.keyevts}')\n",
    "            #print(f'this.pattern s1: {s1.keyevts}')\n",
    "        \n",
    "        \n",
    "            if not s1 and not s0:\n",
    "                word, pos, count, s0, s1= self.growSeq(attr, s,  minSupport, maxSupport)\n",
    "                print(f'word: {word}, pos: {pos}, count: {count}')\n",
    "                \n",
    "                \n",
    "                if count < minSupport:\n",
    "                    leafSeqs.append(s)\n",
    "                else:\n",
    "                    \n",
    "                    s1.setHash(word)\n",
    "                    s1.setValue(s.incomingSequences[0].getEvtAttrValue(attr, word))\n",
    "                    s1.keyevts=s.keyevts[:] #deep copy\n",
    "                    s0.keyevts=s.keyevts[:]\n",
    "                    #for i,x in enumerate(s.pattern.keyEvts):\n",
    "                    #    print(s.pattern.keyEvts)\n",
    "                    #    s1.pattern.addKeyEvent(x)\n",
    "                    #    s0.pattern.addKeyEvent(x)\n",
    "                        \n",
    "                    s1.keyevts.insert(pos,word) \n",
    "                \n",
    "                #print(f'this.pattern s after: {s.keyevts}')\n",
    "                #print(f'this.pattern s0 after: {s0.keyevts}')\n",
    "                #print(f'this.pattern s1 after: {s1.keyevts}')\n",
    "        \n",
    "                    \n",
    "            if s1 and s1.seqCount>= minSupport:\n",
    "                expandCnt-=1\n",
    "                seqs.append(s1)\n",
    "                graph.nodes.append(Rawnode(s1))\n",
    "                graph.add(s.nid,s1.nid)\n",
    "            \n",
    "                #s1.after=s\n",
    "            s.before=s1\n",
    "            s.after=s0\n",
    "            \n",
    "            \n",
    "            \n",
    "            if s0 and s0.seqCount>= minSupport:\n",
    "                seqs.append(s0)\n",
    "                #graph.nodes.append(Rawnode(s0))\n",
    "            \n",
    "                #graph.add(s.nid,s0.nid)\n",
    "            \n",
    "                #s0.before=s\n",
    "            print(f'seqCount: {[s.seqCount for s in seqs]}')\n",
    "            #print(f'before: {s.before}')\n",
    "            #print(f'after: {s.after}')\n",
    "            #print(f'this: {s}')\n",
    "            #print(f' s after: {s}')\n",
    "            #print(f' s0 after: {s0}')\n",
    "            #print(f' s1 after: {s1}')\n",
    "        \n",
    "            del seqs[seqs.index(s)]\n",
    "            #print(f'seqCount: {[s.seqCount for s in seqs]}')\n",
    "            #print(f'before: {seqs[0].before}')\n",
    "            #print(f'after: {seqs[0].after}')\n",
    "            #print(f'this: {seqs[0]}')\n",
    "            \n",
    "            #print(f' s : {s}')\n",
    "            #print(f' s0 : {s0}')\n",
    "            #print(f' s1: {s1}')\n",
    "        \n",
    "        return leafSeqs.append(seqs)\n",
    "    \n",
    "    \n",
    "    def growSeq(self, attr, seq,  minSupport, maxSupport) :\n",
    "        #this is not right\n",
    "        pos=-1\n",
    "        word=\"\"\n",
    "        count=0\n",
    "        #print(f'this.pattern in growseq: {seq.pattern}')\n",
    "        #eventcol=Sequence.getUniqueEvents(seq.incomingSequences)\n",
    "        #print(f'seq pattern len {seq.keyevts}')\n",
    "        for i in range (0,len(seq.keyevts)+1):\n",
    "            fdist={}\n",
    "            #print(f'i: {i}, len {len(seq.keyevts)}')\n",
    "            for  ind, s in enumerate(seq.incomingSequences):\n",
    "                #print(f's.seqIndices: {s.seqIndices}')\n",
    "                evtHashes= s.getHashList(attr)\n",
    "                l=0 if i==0 else   s.seqIndices[i - 1] + 1\n",
    "                r=len(evtHashes) if i==len(seq.keyevts) else s.seqIndices[i]\n",
    "                \n",
    "                \n",
    "                #print(f'l index: {l}, r index {r}')\n",
    "                #print(f'evt Hash: {evtHashes}')\n",
    "                duplicate=[]\n",
    "                for j in range (l,r):\n",
    "                    w=evtHashes[j]\n",
    "                    #print(w)\n",
    "                    if w in duplicate:\n",
    "                        continue\n",
    "                    duplicate.append(w)\n",
    "                    if w not in fdist:\n",
    "                        fdist[w] = s.getVolume()\n",
    "                    else:\n",
    "                        fdist[w]+= s.getVolume()\n",
    "                \n",
    "            maxw=\"\"\n",
    "            maxc=0\n",
    "            for w in fdist.keys():\n",
    "                value= fdist[w]\n",
    "\n",
    "                if value < maxSupport and value > maxc:\n",
    "                    maxw= str(w)\n",
    "                    maxc= value\n",
    "\n",
    "            if maxc > count:\n",
    "                pos=i\n",
    "                word=maxw\n",
    "                count=maxc\n",
    "        #print(f'{word}: word')\n",
    "        #print(f'{maxc}: count')\n",
    "                    \n",
    "        s0=GraphNode(attr=attr)\n",
    "        s1=GraphNode(attr=attr)\n",
    "        \n",
    "        #print(f'this.pattern s0 in growseq: {s0.pattern}')\n",
    "        #print(f'this.pattern s1 in growseq: {s1.pattern}')\n",
    "        \n",
    "        #print(f'minSupport {minSupport} count {count}')    \n",
    "        if count >= minSupport:\n",
    "            words=seq.keyevts\n",
    "            for t in seq.incomingSequences:\n",
    "                l=0 if pos==0 else t.seqIndices[pos - 1] + 1\n",
    "                r= len(t.events) if pos == len(words) else  t.seqIndices[pos]\n",
    "                try:\n",
    "                    i = t.getHashList(attr).index(word,l,r)\n",
    "                    #print(f'position: {i}')\n",
    "                    #i+=l\n",
    "                    \n",
    "                    t.seqIndices.insert(pos,i)\n",
    "                    s1.incomingSequences.append(t)\n",
    "                    s1.seqCount+=t.getVolume()\n",
    "\n",
    "                except ValueError:\n",
    "                    #print(f'Value error')\n",
    "                    s0.incomingSequences.append(t)\n",
    "                    s0.seqCount+=t.getVolume()\n",
    "                \n",
    "        s0.setSeqCount(Sequence.getSeqVolume(s0.incomingSequences))\n",
    "        s1.setSeqCount(Sequence.getSeqVolume(s1.incomingSequences))\n",
    "        print(f'Not contain: {len(s0.incomingSequences)}')\n",
    "        print(f'contain: {len(s1.incomingSequences)}')\n",
    "        return word, pos, count, s0, s1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe( src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            print(src)\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "\n",
    "    \n",
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/sequence_braiding_refined.csv\n",
      "dict_keys(['Glucose', 'Meal'])\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "#Sequence.create_attr_dict([seq])\n",
    "#seq.getEventPosition('Meal','Lunch')\n",
    "#print(seq.getUniqueValueHashes('Meal'))\n",
    "#print(seq.getHashList('Glucose'))\n",
    "#print(seq.getValueHashes('Glucose'))\n",
    "#print(seq.getEventsHashString('Glucose'))\n",
    "#raw_seq=seq.convertToVMSPReadable('Meal')\n",
    "#print(raw_seq)\n",
    "#print(seq.getPathID())\n",
    "#sequence_braiding[0].attributes.keys()\n",
    "#print(sequence_braiding[0].getAttrVal('Meals'))\n",
    "#print(sequence_braiding[0].type)\n",
    "#for events in sequence_braiding:\n",
    "#    print(events.getAttrVal('Meal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"week\")\n",
    "#seq_list=[]\n",
    "#for seqs in sequence_braiding_split:\n",
    "#    seq_list.append(Sequence(seqs))\n",
    "    \n",
    "#Sequence.create_attr_dict(seq_list)\n",
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Meal') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Nothing', '1': 'Lunch', '2': 'Dinner', '3': 'Other', '4': 'Sugar to treat', '5': 'Breakfast', '6': 'Afternoon snack', '7': 'Exercise snack', '8': 'Bedtime snack'}\n"
     ]
    }
   ],
   "source": [
    "print(sequence_braiding_Es.reverseatttrdict['Meal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442634415244144454445\n",
      "415434526420001214155\n",
      "452145215821526405215\n",
      "261521515241521551\n",
      "371526106105444158411520\n",
      "775231052461052105821525215\n",
      "67154444105631054424741544482441452144525\n",
      "207124134541524371582441524525\n",
      "40015241521544582454450\n",
      "426001500152615407158215421444215\n",
      "20152615152001544154142144\n",
      "442441454274145204554215415261545\n",
      "271548425251827442151445\n",
      "44145444120715845154\n",
      "315215104071444404651\n",
      "07174107154241045445415\n",
      "071544411515423415456\n",
      "151521504154242241454\n",
      "01044541514447152414454\n",
      "243140441304241542215821252\n",
      "4715415452454154521\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabularies- can be emulated from attrdict\n",
    "# itemset- keys of vocabularies\n",
    "#count- seq volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sugar to treat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_list[0].events[0].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[0,1,5]\n",
    "seq_sublist=[seq_list[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breakfast'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_sublist[2].events[7].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq= [seqs.getEventsHashString('Meal') for seqs in seq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqCount: 22\n",
      "this.pattern s : []\n",
      "Not contain: 1\n",
      "contain: 21\n",
      "word: 2, pos: 0, count: 21\n",
      "seqCount: [22, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['2']\n",
      "Not contain: 0\n",
      "contain: 21\n",
      "word: 4, pos: 1, count: 21\n",
      "seqCount: [21, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['2', '4']\n",
      "Not contain: 0\n",
      "contain: 21\n",
      "word: 1, pos: 2, count: 21\n",
      "seqCount: [21, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['2', '4', '1']\n",
      "Not contain: 0\n",
      "contain: 21\n",
      "word: 5, pos: 3, count: 21\n",
      "seqCount: [21, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['2', '4', '1', '5']\n",
      "Not contain: 4\n",
      "contain: 17\n",
      "word: 4, pos: 4, count: 17\n",
      "seqCount: [21, 17]\n",
      "seqCount: 17\n",
      "this.pattern s : ['2', '4', '1', '5', '4']\n",
      "Not contain: 3\n",
      "contain: 14\n",
      "word: 5, pos: 5, count: 14\n",
      "seqCount: [17, 14]\n",
      "seqCount: 14\n",
      "this.pattern s : ['2', '4', '1', '5', '4', '5']\n",
      "Not contain: 5\n",
      "contain: 9\n",
      "word: 4, pos: 0, count: 9\n",
      "seqCount: [14, 9, 5]\n",
      "seqCount: 9\n",
      "this.pattern s : ['4', '2', '4', '1', '5', '4', '5']\n",
      "Not contain: 3\n",
      "contain: 6\n",
      "word: 4, pos: 1, count: 6\n",
      "seqCount: [9, 5, 6]\n",
      "seqCount: 6\n",
      "this.pattern s : ['4', '4', '2', '4', '1', '5', '4', '5']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 4, pos: 4, count: 4\n",
      "seqCount: [5, 6]\n",
      "seqCount: 5\n",
      "this.pattern s : ['2', '4', '1', '5', '4', '5']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 1, pos: 1, count: 4\n",
      "seqCount: [5]\n"
     ]
    }
   ],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Meal\",root,  expandCnt=30, minSupport=5, maxSupport=len(raw_seq),graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"before\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": {\n",
      "      \"before\": {\n",
      "       \"before\": {\n",
      "        \"before\": {\n",
      "         \"before\": {\n",
      "          \"before\": null,\n",
      "          \"event_attribute\": \"\",\n",
      "          \"Pattern\": \"\",\n",
      "          \"value\": 0,\n",
      "          \"After\": null\n",
      "         },\n",
      "         \"event_attribute\": \"Sugar to treat\",\n",
      "         \"Pattern\": \"Sugar to treat-Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\",\n",
      "         \"value\": 6,\n",
      "         \"After\": {\n",
      "          \"before\": null,\n",
      "          \"event_attribute\": \"\",\n",
      "          \"Pattern\": \"\",\n",
      "          \"value\": 0,\n",
      "          \"After\": null\n",
      "         }\n",
      "        },\n",
      "        \"event_attribute\": \"Sugar to treat\",\n",
      "        \"Pattern\": \"Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\",\n",
      "        \"value\": 9,\n",
      "        \"After\": {\n",
      "         \"before\": null,\n",
      "         \"event_attribute\": \"\",\n",
      "         \"Pattern\": \"Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\",\n",
      "         \"value\": 3,\n",
      "         \"After\": null\n",
      "        }\n",
      "       },\n",
      "       \"event_attribute\": \"Breakfast\",\n",
      "       \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\",\n",
      "       \"value\": 14,\n",
      "       \"After\": {\n",
      "        \"before\": {\n",
      "         \"before\": null,\n",
      "         \"event_attribute\": \"\",\n",
      "         \"Pattern\": \"\",\n",
      "         \"value\": 0,\n",
      "         \"After\": null\n",
      "        },\n",
      "        \"event_attribute\": \"\",\n",
      "        \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\",\n",
      "        \"value\": 5,\n",
      "        \"After\": {\n",
      "         \"before\": null,\n",
      "         \"event_attribute\": \"\",\n",
      "         \"Pattern\": \"\",\n",
      "         \"value\": 0,\n",
      "         \"After\": null\n",
      "        }\n",
      "       }\n",
      "      },\n",
      "      \"event_attribute\": \"Sugar to treat\",\n",
      "      \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat\",\n",
      "      \"value\": 17,\n",
      "      \"After\": {\n",
      "       \"before\": null,\n",
      "       \"event_attribute\": \"\",\n",
      "       \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat\",\n",
      "       \"value\": 3,\n",
      "       \"After\": null\n",
      "      }\n",
      "     },\n",
      "     \"event_attribute\": \"Breakfast\",\n",
      "     \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast\",\n",
      "     \"value\": 21,\n",
      "     \"After\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast\",\n",
      "      \"value\": 4,\n",
      "      \"After\": null\n",
      "     }\n",
      "    },\n",
      "    \"event_attribute\": \"Lunch\",\n",
      "    \"Pattern\": \"Dinner-Sugar to treat-Lunch\",\n",
      "    \"value\": 21,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": \"\",\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"Sugar to treat\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat\",\n",
      "   \"value\": 21,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": \"\",\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"Dinner\",\n",
      "  \"Pattern\": \"Dinner\",\n",
      "  \"value\": 21,\n",
      "  \"After\": {\n",
      "   \"before\": null,\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": \"\",\n",
      "   \"value\": 0,\n",
      "   \"After\": null\n",
      "  }\n",
      " },\n",
      " \"event_attribute\": \"\",\n",
      " \"Pattern\": \"\",\n",
      " \"value\": 22,\n",
      " \"After\": {\n",
      "  \"before\": null,\n",
      "  \"event_attribute\": \"\",\n",
      "  \"Pattern\": \"\",\n",
      "  \"value\": 1,\n",
      "  \"After\": null\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON Encode data into Python String\n",
      "\"{\\\"links\\\": [{\\\"source\\\": 1, \\\"target\\\": 3}, {\\\"source\\\": 3, \\\"target\\\": 5}, {\\\"source\\\": 5, \\\"target\\\": 7}, {\\\"source\\\": 7, \\\"target\\\": 9}, {\\\"source\\\": 9, \\\"target\\\": 11}, {\\\"source\\\": 11, \\\"target\\\": 13}, {\\\"source\\\": 13, \\\"target\\\": 15}, {\\\"source\\\": 15, \\\"target\\\": 17}], \\\"nodes\\\": [{\\\"nid\\\": 1, \\\"seqCount\\\": 22, \\\"value\\\": \\\"\\\", \\\"pattern\\\": \\\"\\\"}, {\\\"nid\\\": 3, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Dinner\\\", \\\"pattern\\\": \\\"Dinner\\\"}, {\\\"nid\\\": 5, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Sugar to treat\\\", \\\"pattern\\\": \\\"Dinner-Sugar to treat\\\"}, {\\\"nid\\\": 7, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Lunch\\\", \\\"pattern\\\": \\\"Dinner-Sugar to treat-Lunch\\\"}, {\\\"nid\\\": 9, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Breakfast\\\", \\\"pattern\\\": \\\"Dinner-Sugar to treat-Lunch-Breakfast\\\"}, {\\\"nid\\\": 11, \\\"seqCount\\\": 17, \\\"value\\\": \\\"Sugar to treat\\\", \\\"pattern\\\": \\\"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat\\\"}, {\\\"nid\\\": 13, \\\"seqCount\\\": 14, \\\"value\\\": \\\"Breakfast\\\", \\\"pattern\\\": \\\"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\\\"}, {\\\"nid\\\": 15, \\\"seqCount\\\": 9, \\\"value\\\": \\\"Sugar to treat\\\", \\\"pattern\\\": \\\"Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\\\"}, {\\\"nid\\\": 17, \\\"seqCount\\\": 6, \\\"value\\\": \\\"Sugar to treat\\\", \\\"pattern\\\": \\\"Sugar to treat-Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\\\"}]}\"\n"
     ]
    }
   ],
   "source": [
    "empJSON = jsonpickle.encode(graph, unpicklable=False)\n",
    "#empJSON = jsonpickle.encode(graph)\n",
    "print(\"Writing JSON Encode data into Python String\")\n",
    "employeeJSONData = json.dumps(empJSON, indent=4)\n",
    "print(employeeJSONData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"nodes\": [\n",
      "  {\n",
      "   \"node id\": 1,\n",
      "   \"count\": 22,\n",
      "   \"Value\": \"\",\n",
      "   \"Pattern\": \"\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 3,\n",
      "   \"count\": 21,\n",
      "   \"Value\": \"Dinner\",\n",
      "   \"Pattern\": \"Dinner\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 5,\n",
      "   \"count\": 21,\n",
      "   \"Value\": \"Sugar to treat\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 7,\n",
      "   \"count\": 21,\n",
      "   \"Value\": \"Lunch\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat-Lunch\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 9,\n",
      "   \"count\": 21,\n",
      "   \"Value\": \"Breakfast\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 11,\n",
      "   \"count\": 17,\n",
      "   \"Value\": \"Sugar to treat\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 13,\n",
      "   \"count\": 14,\n",
      "   \"Value\": \"Breakfast\",\n",
      "   \"Pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 15,\n",
      "   \"count\": 9,\n",
      "   \"Value\": \"Sugar to treat\",\n",
      "   \"Pattern\": \"Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 17,\n",
      "   \"count\": 6,\n",
      "   \"Value\": \"Sugar to treat\",\n",
      "   \"Pattern\": \"Sugar to treat-Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"\n",
      "  }\n",
      " ],\n",
      " \"links\": [\n",
      "  {\n",
      "   \"source\": 1,\n",
      "   \"target\": 3\n",
      "  },\n",
      "  {\n",
      "   \"source\": 3,\n",
      "   \"target\": 5\n",
      "  },\n",
      "  {\n",
      "   \"source\": 5,\n",
      "   \"target\": 7\n",
      "  },\n",
      "  {\n",
      "   \"source\": 7,\n",
      "   \"target\": 9\n",
      "  },\n",
      "  {\n",
      "   \"source\": 9,\n",
      "   \"target\": 11\n",
      "  },\n",
      "  {\n",
      "   \"source\": 11,\n",
      "   \"target\": 13\n",
      "  },\n",
      "  {\n",
      "   \"source\": 13,\n",
      "   \"target\": 15\n",
      "  },\n",
      "  {\n",
      "   \"source\": 15,\n",
      "   \"target\": 17\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(graph, ensure_ascii=False, default=Graph.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmy_dict = {'name': 'flare',\\n           'children': [{'name': k,\\n                         'children': [{'name': child} for child in v]}\\n                            for k, v in my_defaultdict.items()]}\\n\\njson_data = json.dumps(my_dict, indent=2)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "my_dict = {'name': 'flare',\n",
    "           'children': [{'name': k,\n",
    "                         'children': [{'name': child} for child in v]}\n",
    "                            for k, v in my_defaultdict.items()]}\n",
    "\n",
    "json_data = json.dumps(my_dict, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"links\": [{\"source\": 1, \"target\": 3}, {\"source\": 3, \"target\": 5}, {\"source\": 5, \"target\": 7}, {\"source\": 7, \"target\": 9}, {\"source\": 9, \"target\": 11}, {\"source\": 11, \"target\": 13}, {\"source\": 13, \"target\": 15}, {\"source\": 15, \"target\": 17}], \"nodes\": [{\"nid\": 1, \"seqCount\": 22, \"value\": \"\", \"pattern\": \"\"}, {\"nid\": 3, \"seqCount\": 21, \"value\": \"Dinner\", \"pattern\": \"Dinner\"}, {\"nid\": 5, \"seqCount\": 21, \"value\": \"Sugar to treat\", \"pattern\": \"Dinner-Sugar to treat\"}, {\"nid\": 7, \"seqCount\": 21, \"value\": \"Lunch\", \"pattern\": \"Dinner-Sugar to treat-Lunch\"}, {\"nid\": 9, \"seqCount\": 21, \"value\": \"Breakfast\", \"pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast\"}, {\"nid\": 11, \"seqCount\": 17, \"value\": \"Sugar to treat\", \"pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat\"}, {\"nid\": 13, \"seqCount\": 14, \"value\": \"Breakfast\", \"pattern\": \"Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"}, {\"nid\": 15, \"seqCount\": 9, \"value\": \"Sugar to treat\", \"pattern\": \"Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"}, {\"nid\": 17, \"seqCount\": 6, \"value\": \"Sugar to treat\", \"pattern\": \"Sugar to treat-Sugar to treat-Dinner-Sugar to treat-Lunch-Breakfast-Sugar to treat-Breakfast\"}]}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(graph, default=lambda o: o.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Links at 0x7fab9e070450>,\n",
       " <__main__.Links at 0x7fab9e070710>,\n",
       " <__main__.Links at 0x7fab9e070910>,\n",
       " <__main__.Links at 0x7fab9e0701d0>,\n",
       " <__main__.Links at 0x7fab9e070c10>,\n",
       " <__main__.Links at 0x7fab9e072d50>,\n",
       " <__main__.Links at 0x7fab9e0778d0>,\n",
       " <__main__.Links at 0x7fab9e0702d0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Rawnode at 0x7fab9e072bd0>,\n",
       " <__main__.Rawnode at 0x7fab9e070410>,\n",
       " <__main__.Rawnode at 0x7fab9e0706d0>,\n",
       " <__main__.Rawnode at 0x7fab9e070810>,\n",
       " <__main__.Rawnode at 0x7fab9e070190>,\n",
       " <__main__.Rawnode at 0x7fab9e070350>,\n",
       " <__main__.Rawnode at 0x7fab9e070310>,\n",
       " <__main__.Rawnode at 0x7fab9e077890>,\n",
       " <__main__.Rawnode at 0x7fab9e077950>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "h=[]\n",
    "#merge(h,key=lambda e:e[0],reverse=True)\n",
    "heapq.heappush(h, (200, 1))\n",
    "heapq.heappush(h, (300,2))\n",
    "heapq.heappush(h, (400,3))\n",
    "print(heapq.heappop(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"abcd\".index('c',2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Sample_Dataset.csv\n",
      "dict_keys(['Events', 'Counts'])\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../Sample_Dataset.csv', 0, \"%m/%d/%Y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"day\")\n",
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Events') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'E', '1': 'B', '2': 'L', '3': 'A', '4': 'D', '5': 'J', '6': 'F', '7': 'C', '8': 'G', '9': 'K', ':': 'I', ';': 'H'}\n"
     ]
    }
   ],
   "source": [
    "print(sequence_braiding_Es.reverseatttrdict['Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqCount: 6\n",
      "this.pattern s : []\n",
      "Not contain: 2\n",
      "contain: 4\n",
      "word: 3, pos: 0, count: 4\n",
      "seqCount: [6, 4, 2]\n",
      "seqCount: 4\n",
      "this.pattern s : ['3']\n",
      "Not contain: 2\n",
      "contain: 2\n",
      "word: 7, pos: 1, count: 2\n",
      "seqCount: [4, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : []\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 7, pos: 0, count: 2\n",
      "seqCount: [2, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3', '7']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 7, pos: 2, count: 2\n",
      "seqCount: [2, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 4, pos: 0, count: 1\n",
      "seqCount: [2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['7']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 6, pos: 1, count: 2\n",
      "seqCount: [2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3', '7', '7']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 7, pos: 0, count: 1\n",
      "seqCount: [2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['7', '6']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 8, pos: 2, count: 2\n",
      "seqCount: [2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['7', '6', '8']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 2, pos: 1, count: 1\n",
      "seqCount: [2]\n"
     ]
    }
   ],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Events\",root,  expandCnt=30, minSupport=2, maxSupport=len(seq_list),graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"before\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": \"\",\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    },\n",
      "    \"event_attribute\": \"C\",\n",
      "    \"Pattern\": \"A-C-C\",\n",
      "    \"value\": 2,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": \"\",\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"C\",\n",
      "   \"Pattern\": \"A-C\",\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": \"\",\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"A\",\n",
      "  \"Pattern\": \"A\",\n",
      "  \"value\": 4,\n",
      "  \"After\": {\n",
      "   \"before\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": \"\",\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   },\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": \"A\",\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": \"\",\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  }\n",
      " },\n",
      " \"event_attribute\": \"\",\n",
      " \"Pattern\": \"\",\n",
      " \"value\": 6,\n",
      " \"After\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": \"\",\n",
      "      \"value\": 0,\n",
      "      \"After\": null\n",
      "     },\n",
      "     \"event_attribute\": \"G\",\n",
      "     \"Pattern\": \"C-F-G\",\n",
      "     \"value\": 2,\n",
      "     \"After\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": \"\",\n",
      "      \"value\": 0,\n",
      "      \"After\": null\n",
      "     }\n",
      "    },\n",
      "    \"event_attribute\": \"F\",\n",
      "    \"Pattern\": \"C-F\",\n",
      "    \"value\": 2,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": \"\",\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"C\",\n",
      "   \"Pattern\": \"C\",\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": \"\",\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"\",\n",
      "  \"Pattern\": \"\",\n",
      "  \"value\": 2,\n",
      "  \"After\": {\n",
      "   \"before\": null,\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": \"\",\n",
      "   \"value\": 0,\n",
      "   \"After\": null\n",
      "  }\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"nodes\": [\n",
      "  {\n",
      "   \"node id\": 22,\n",
      "   \"count\": 6,\n",
      "   \"Value\": \"\",\n",
      "   \"Pattern\": \"\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 24,\n",
      "   \"count\": 4,\n",
      "   \"Value\": \"A\",\n",
      "   \"Pattern\": \"A\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 26,\n",
      "   \"count\": 2,\n",
      "   \"Value\": \"C\",\n",
      "   \"Pattern\": \"A-C\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 28,\n",
      "   \"count\": 2,\n",
      "   \"Value\": \"C\",\n",
      "   \"Pattern\": \"C\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 30,\n",
      "   \"count\": 2,\n",
      "   \"Value\": \"C\",\n",
      "   \"Pattern\": \"A-C-C\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 34,\n",
      "   \"count\": 2,\n",
      "   \"Value\": \"F\",\n",
      "   \"Pattern\": \"C-F\"\n",
      "  },\n",
      "  {\n",
      "   \"node id\": 38,\n",
      "   \"count\": 2,\n",
      "   \"Value\": \"G\",\n",
      "   \"Pattern\": \"C-F-G\"\n",
      "  }\n",
      " ],\n",
      " \"links\": [\n",
      "  {\n",
      "   \"source\": 22,\n",
      "   \"target\": 24\n",
      "  },\n",
      "  {\n",
      "   \"source\": 24,\n",
      "   \"target\": 26\n",
      "  },\n",
      "  {\n",
      "   \"source\": 23,\n",
      "   \"target\": 28\n",
      "  },\n",
      "  {\n",
      "   \"source\": 26,\n",
      "   \"target\": 30\n",
      "  },\n",
      "  {\n",
      "   \"source\": 28,\n",
      "   \"target\": 34\n",
      "  },\n",
      "  {\n",
      "   \"source\": 34,\n",
      "   \"target\": 38\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(graph, ensure_ascii=False, default=Graph.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Rawnode is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-970ac78a9e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Rawnode is not JSON serializable"
     ]
    }
   ],
   "source": [
    "json.dumps(graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(vars(graph))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
