{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf\n",
    "import json\n",
    "import jsonpickle\n",
    "import heapq\n",
    "\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common class for all Events\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, eventtype):\n",
    "        self.type=eventtype\n",
    "    \n",
    "    #Return Attribute value given attribute name\n",
    "    def getAttrVal(self, attrName):\n",
    "        return self.attributes.get(attrName,None)\n",
    "\n",
    "    \n",
    "# A class that represents a point event\n",
    "class PointEvent(Event):\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        super().__init__(\"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "        \n",
    "    \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent(Event):\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        super().__init__(\"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    \n",
    "    def __init__(self, eventlist=[]):\n",
    "        self.attrdict={}\n",
    "        self.reverseatttrdict={}\n",
    "        self.events=eventlist\n",
    "\n",
    "    #should be moved to EventStore\n",
    "    # hold the list of events, also the dictionaries\n",
    "    \n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importPointEvents(self, src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != timestampColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = PointEvent(timestamp, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events\n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importIntervalEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for the start and end times of the event\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events    \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    #@staticmethod\n",
    "    def importMixedEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "            if type(data[endTimeColumnIdx]) is float:\n",
    "                t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                event_type = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "                event_type = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "            attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "            for i in attribute_columns:\n",
    "                attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            if event_type == \"point\":\n",
    "                e = PointEvent(t, attribs)\n",
    "            else:\n",
    "                e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events   \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    #should take an eventlist as input\n",
    "    # Group events by attributeName, and order them by timestamp\n",
    "    #@staticmethod\n",
    "    #should return a list of sequences\n",
    "    def generateSequence(self, attributeName):\n",
    "        eventList=self.events\n",
    "        grouped_by = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in grouped_by for value\n",
    "            if value in grouped_by:\n",
    "                grouped_by[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                grouped_by[value] = [event]\n",
    "        sequences= list(grouped_by.values())\n",
    "        seqlist=[]\n",
    "        for seq in sequences:\n",
    "            seqlist.append(Sequence(seq, self))\n",
    "        return seqlist\n",
    "    \n",
    "    # Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "    # break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "    # “week”, “month”, “quarter”, and “year”.\n",
    "    # For interval events I used the start time of the event to determine its category when splitting it\n",
    "    \n",
    "    #ZINAT- changes\n",
    "    #SequenceList represents a list of objects of type Sequence. The sequences are further splitted into\n",
    "    #sequence objects, this way we can use generate sequences and then splitSequences \n",
    "    @staticmethod\n",
    "    def splitSequences(sequenceLists, timeUnit, record=None):\n",
    "        if not isinstance(sequenceLists, list):\n",
    "            sequenceLists=[sequenceLists]\n",
    "        eventstore=sequenceLists[0].eventstore\n",
    "        results = []\n",
    "        resultlist=[]\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in valid_time_units:\n",
    "            raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "        \n",
    "        for sequence in sequenceLists:\n",
    "            # Sort the events by the timestamp or event start time\n",
    "            sequenceList= sequence.events\n",
    "            sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "\n",
    "            # Process the event sequence based on the given time unit\n",
    "            # Generally, create a map for that time unit and then add each event into that map \n",
    "            # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "            if timeUnit == \"hour\":\n",
    "                hours = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.hour, time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,hours,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "                results = list(hours.values())\n",
    "\n",
    "            elif timeUnit == \"day\":\n",
    "                days = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,days,event)\n",
    "                    #print(days)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                results = list(days.values())\n",
    "\n",
    "            elif timeUnit == \"month\":\n",
    "                months = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.month,time.year)\n",
    "                    insert_event_into_dict(key,months,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(months.values())\n",
    "\n",
    "            elif timeUnit == \"week\":\n",
    "                weeks = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    week_num = time.isocalendar()[1]\n",
    "                    key = (year,week_num)\n",
    "                    insert_event_into_dict(key,weeks,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "                results = list(weeks.values())\n",
    "\n",
    "            elif timeUnit == \"year\":\n",
    "                years = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = time.year\n",
    "                    insert_event_into_dict(key,years,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key)\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "                results = list(years.values())\n",
    "\n",
    "            elif timeUnit == \"quarter\":\n",
    "                quarters = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    month = time.month\n",
    "                    # Determine the year, quarter pair/key for quarter dict\n",
    "                    # January, February, and March (Q1)\n",
    "                    if month in range(1, 4):\n",
    "                        key = (year, \"Q1\")\n",
    "                    # April, May, and June (Q2)\n",
    "                    elif month in range(4, 7):\n",
    "                        key = (year, \"Q2\")\n",
    "                    # July, August, and September (Q3)\n",
    "                    elif month in range(7,10):\n",
    "                        key = (year, \"Q3\")\n",
    "                    # October, November, and December (Q4)\n",
    "                    elif month in range(10,13):\n",
    "                        key = (year, \"Q4\")\n",
    "                    # Put the event in the dictionary\n",
    "                    insert_event_into_dict(key,quarters,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(quarters.values())\n",
    "            resultlist.extend(results)\n",
    "        resultlists= [Sequence(x, eventstore) for x in resultlist]\n",
    "\n",
    "        return resultlists\n",
    "    \n",
    "    def getUniqueValues(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return l\n",
    "    \n",
    "    #Assuming we are given a list of events and from those events we create \n",
    "    #the mapping and reverse mapping dictionary\n",
    "    def create_attr_dict(self):\n",
    "        attr_list=self.events[0].attributes.keys()\n",
    "        print(attr_list)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            a=48\n",
    "            unique_list=[]\n",
    "            unique_list.extend(self.getUniqueValues(attr))\n",
    "            unique_list=list(set(unique_list))\n",
    "            #unique_list.clear()\n",
    "            \n",
    "            unicode_dict={}\n",
    "            reverse_dict={}\n",
    "            for uniques in unique_list:\n",
    "                unicode_dict[uniques]=chr(a)\n",
    "                reverse_dict[chr(a)]=uniques\n",
    "                a=a+1\n",
    "            self.attrdict[attr]=unicode_dict\n",
    "            self.reverseatttrdict[attr]=reverse_dict\n",
    "            #unicode_dict.clear()                    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence():\n",
    "    _ids = count(0)\n",
    "    \n",
    "\n",
    "    def __init__(self,  eventlist, eventstore,sid=None):\n",
    "        # sequence id\n",
    "        if sid is None:\n",
    "            self.sid = next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "        \n",
    "        self.events = eventlist\n",
    "        self.eventstore=eventstore\n",
    "        self.volume=1\n",
    "        self.seqAttributes={}\n",
    "        self.seqIndices=[]\n",
    "    def getEventPosition(self, attr, hash_val):\n",
    "        for count,event in enumerate(self.events):\n",
    "            #if event.getAttrVal(attr)==hash_val:\n",
    "            if self.eventstore.attrdict[attr][event.getAttrVal(attr)]==hash_val:\n",
    "                return count\n",
    "        return -1\n",
    "    \n",
    "    def setVolume(self, intValue):\n",
    "        self.volume=intValue\n",
    "        \n",
    "    def getVolume(self):\n",
    "        return self.volume\n",
    "    \n",
    "    def increaseVolume(self):\n",
    "        self.volume += 1 \n",
    "    \n",
    "    \n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        uniquelist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        return uniquelist\n",
    "    \n",
    "    #Not sure this will always result in same index, will change if \n",
    "    #dictionary is updated\n",
    "    #since python is unordered\n",
    "    \n",
    "    def getHashList(self, attr):\n",
    "        #l=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        l=[event.getAttrVal(attr) for event in self.events]\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "    \n",
    "    def getValueHashes(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "        \n",
    "    \n",
    "    def getEventsHashString(self, attr):\n",
    "        s=\"\"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"\".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #print(s)\n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadablenum(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" -1 \".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\" -2\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" \".join(self.eventstore.attrdict[attr][elem] for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\".\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def getPathID(self):\n",
    "        return self.sid\n",
    "    \n",
    "    def matchPathAttribute(self, attr, val):\n",
    "        # should i use eq?!\n",
    "        if this.seqAttributes.get(attr)==(val):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def setSequenceAttribute(self,attr, value):\n",
    "        self.seqAttributes[attr]=value\n",
    "        \n",
    "         \n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)    \n",
    "    def getSeqVolume(seqlist):\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "    \n",
    "    \n",
    "    # Method equivalent to public String getEvtAttrValue(String attr, int hash) in DataManager.java\n",
    "    def getEvtAttrValue(self, attr, hashval):\n",
    "        return self.eventstore.reverseatttrdict[attr][hashval]\n",
    "        \n",
    "    # Method equivalent to public List<String> getEvtAttrValues(String attr) in DataManager.java    \n",
    "    def getEvtAttrValues(self, attr):\n",
    "        return list(self.eventstore.reverseatttrdict[attr].values())\n",
    "    \n",
    "    # Method equivalent to int getEvtAttrValueCount(String attr) in DataManager.java    \n",
    "    def getEvtAttrValueCount(self, attr):\n",
    "        return len(self.eventstore.reverseatttrdict[attr])\n",
    "    \n",
    "    @staticmethod\n",
    "    \n",
    "    def getUniqueEvents(seqlist):\n",
    "        l=list(set(event.getAttrVal(attr) for event in seq for seq in seqlist))\n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    NID=count(1)\n",
    "    nodeHash={}\n",
    "    \n",
    "    \n",
    "    def __init__(self, name=\"\", count=0, value=\"\"):\n",
    "        self.nid=next(self.NID)\n",
    "        self.name=name\n",
    "        self.seqCount=count\n",
    "        ## What's the difference between name and value?\n",
    "        self.value=value\n",
    "        self.hash=-1\n",
    "        self.pos=[]\n",
    "        self.meanStep=0\n",
    "        self.medianStep=0\n",
    "        #self.zipCompressRatio=0\n",
    "        self.incomingBranchUniqueEvts=None\n",
    "        #self.incomingBranchSimMean=None\n",
    "        #self.incomingBranchSimMedian=None\n",
    "        #self.incomingBranchSimVariance=None\n",
    "        self.keyevts=[]\n",
    "        self.incomingSequences=[]\n",
    "        self.outgoingSequences=[]\n",
    "        \n",
    "        self.meanRelTimestamp=0\n",
    "        self.medianRelTimestamp=0\n",
    "        \n",
    "        TreeNode.nodeHash[self.nid]=self\n",
    "        \n",
    "        \n",
    "    def getNode(self, node_id):\n",
    "        return nodeHash[node_id]\n",
    "    \n",
    "    def clearHash(self):\n",
    "        nodeHash.clear()\n",
    "        \n",
    "    def getIncomingSequences(self):\n",
    "        return self.incomingSequences\n",
    "    \n",
    "    def getSeqCount(self):\n",
    "        return self.seqCount\n",
    "    \n",
    "    def setSeqCount(self, seqCount):\n",
    "        self.seqCount=seqCount\n",
    "        \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def setName(self, name):\n",
    "        self.name=name\n",
    "        \n",
    "    def getMeanStep(self):\n",
    "        return self.meanStep\n",
    "    \n",
    "    #need a better implementation\n",
    "    def toJSONObject(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4) \n",
    "    \n",
    "    def toString(self):\n",
    "        return self.name+\": \"+self.seqCount\n",
    "    \n",
    "    def setPositions(self, l):\n",
    "        self.pos=l\n",
    "        self.pos.sort()\n",
    "        d=sum(self.pos)+len(self.pos)\n",
    "        mid=len(self.pos)/2\n",
    "        \n",
    "        if len(self.pos)==0:\n",
    "            self.meanStep=0\n",
    "            slf.medianStep=0\n",
    "        else:\n",
    "            #WHY WE ARE ADDING 1 to mean and medianStep?\n",
    "            self.meanStep=d/len(self.pos)\n",
    "            self.medianStep= np.median(self.pos)+1#((self.pos[mid-1]+self.pos[mid])/2.0)+1 if len(self.pos)%2==0 else self.pos[mid]+1\n",
    "            \n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "    def setValue(self, value):\n",
    "        self.value=value\n",
    "        \n",
    "    def getMedianStep(self):\n",
    "        return self.medianStep\n",
    "    \n",
    "    #def getZipCompressRatio(self):\n",
    "    #    return self.zipCompressRatio\n",
    "    \n",
    "    #def setZipCompressRatio(self, zipcompressratio):\n",
    "    #    self.zipCompressRatio=zipcompressratio\n",
    "        \n",
    "    def getIncomingBranchUniqueEvts(self):\n",
    "        return self.incomingBranchUniqueEvts\n",
    "    \n",
    "    def setIncomingBranchUniqueEvts(self, incomingbranchuniqueevts):\n",
    "        self.incomingBranchUniqueEvts=incomingbranchuniqueevts\n",
    "        \n",
    "    #def setIncomingBranchSimilarityStats(self, mean, median, variance):\n",
    "    #    self.incomingBranchSimMean=mean\n",
    "    #    self.incomingBranchSimMedian=median\n",
    "    #    self.incomingBranchSimVariance=variance\n",
    "        \n",
    "    \n",
    "    def setIncomingSequences(self, incomingbrancseqs, evtattr):\n",
    "        self.incomingSequences=incomingbrancseqs\n",
    "        \n",
    "    def setRelTimeStamps(self, reltimestamps):\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        reltimestamps.sort()\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        \n",
    "        mid=len(reltimestamps)/2\n",
    "        \n",
    "        if(len(reltimestamps)==0):\n",
    "            self.meanRelTimestamp=0\n",
    "            self.medianRelTimestamp=0\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            self.meanRelTimestamp=d*1.0/len(reltimestamps)\n",
    "            self.medianRelTimestamp=np.median(reltimestamps) #(reltimestamps[mid-1]+reltimestamps[mid])/2.0 if len(reltimestamps%2==0) else reltimestamps[mid]\n",
    "        \n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        \n",
    "    def getHash():\n",
    "        return self.hash\n",
    "        d=sum(reltimestamps, timedelta())\n",
    "        \n",
    "        \n",
    "    def setHash(self, value):\n",
    "        self.hash=value\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def json_serialize(self):\n",
    "    #    json.dump(self, indent=4, default= TreeNode.json_default_dump)\n",
    "    def json_default_dump(self)-> dict:\n",
    "        pass\n",
    "    \n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(Node):\n",
    "    def __init__(self, name=\"\", count=0, value=\"\"):\n",
    "        super().__init__(name, count, value)\n",
    "        self.children = []\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"event_attribute\": self.hash,\n",
    "            \"value\": self.seqCount,\n",
    "            \"median_index\": self.medianStep,\n",
    "            \"average_index\":self.meanStep,\n",
    "\n",
    "            \"children\":[TreeNode.json_serialize_dump(x) for x in self.children]\n",
    "            \n",
    "        }\n",
    "    \n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=TreeNode.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(Node):\n",
    "    def __init__(self, name=\"\", count=0, value=\"\"):\n",
    "        super().__init__(name, count, value)\n",
    "        self.before = []\n",
    "        self.after = []\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"before\": GraphNode.json_serialize_dump(self.before),\n",
    "            \"event_attribute\": self.value,\n",
    "            \"Pattern\": self.keyevts,\n",
    "            \"value\": self.seqCount,\n",
    "            \"After\": GraphNode.json_serialize_dump(self.after)\n",
    "\n",
    "        }\n",
    "\n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=GraphNode.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rawnode:\n",
    "    def __init__ (self, node):\n",
    "        self.nid=node.nid\n",
    "        self.seqCount=node.seqCount\n",
    "        self.value=node.value\n",
    "        \n",
    "class Graph():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.links= defaultdict(set)\n",
    "        self.nodes=[]\n",
    "    \n",
    "    \n",
    "    def add(self, node1, node2):\n",
    "        self.links[node1].add(node2)\n",
    "        #self.links[node2].add(node1)\n",
    "        \n",
    "    def json_default_dump(self)-> dict:\n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"links\":self.links\n",
    "\n",
    "        }\n",
    "\n",
    "    def json_serialize(self) -> None:\n",
    "    \n",
    "        json.dump(self,  indent=4, default=Graph.json_serialize_dump)\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_serialize_dump(obj):\n",
    "    \n",
    "        if hasattr(obj, \"json_default_dump\"):\n",
    "            \n",
    "            return obj.json_default_dump()\n",
    "        return obj\n",
    "\n",
    "    \n",
    "        \n",
    "    def print_graph(self):\n",
    "        for node in self.nodes:\n",
    "            print(self.nid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenTree Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenTreeMiner:\n",
    "    \n",
    "    def expandSeqTree(self, attr, rootNode,  expandCnt, minSupport, maxSupport, graph):\n",
    "        \n",
    "        #if len(rootSeq.eventlist>0):\n",
    "        expandCnt-=len(rootNode.keyevts)\n",
    "        \n",
    "        seqs = []\n",
    "        seqs.append(rootNode)\n",
    "        rootNode.setSeqCount(Sequence.getSeqVolume(rootNode.incomingSequences))\n",
    "        leafSeqs = []\n",
    "        \n",
    "        graph.nodes.append(Rawnode(rootNode))\n",
    "        while seqs and expandCnt > 0:\n",
    "            s = max(seqs,key=lambda x: x.seqCount) \n",
    "            print(f'seqCount: {s.seqCount}')\n",
    "            #print(f'this: {s}')\n",
    "\n",
    "            s0 = s.after\n",
    "            s1 = s.before\n",
    "            \n",
    "            #print(f' s : {s}')\n",
    "            #print(f' s0 : {s0}')\n",
    "            #print(f' s1: {s1}')\n",
    "            \n",
    "            print(f'this.pattern s : {s.keyevts}')\n",
    "            #print(f'this.pattern s0 : {s0.keyevts}')\n",
    "            #print(f'this.pattern s1: {s1.keyevts}')\n",
    "        \n",
    "        \n",
    "            if not s1 and not s0:\n",
    "                word, pos, count, s0, s1= self.growSeq(attr, s,  minSupport, maxSupport)\n",
    "                print(f'word: {word}, pos: {pos}, count: {count}')\n",
    "                \n",
    "                \n",
    "                if count < minSupport:\n",
    "                    leafSeqs.append(s)\n",
    "                else:\n",
    "                    \n",
    "                    s1.setHash(word)\n",
    "                    s1.setValue(s.incomingSequences[0].getEvtAttrValue(attr, word))\n",
    "                    s1.keyevts=s.keyevts[:] #deep copy\n",
    "                    s0.keyevts=s.keyevts[:]\n",
    "                    #for i,x in enumerate(s.pattern.keyEvts):\n",
    "                    #    print(s.pattern.keyEvts)\n",
    "                    #    s1.pattern.addKeyEvent(x)\n",
    "                    #    s0.pattern.addKeyEvent(x)\n",
    "                        \n",
    "                    s1.keyevts.append(word) \n",
    "                \n",
    "                #print(f'this.pattern s after: {s.keyevts}')\n",
    "                #print(f'this.pattern s0 after: {s0.keyevts}')\n",
    "                #print(f'this.pattern s1 after: {s1.keyevts}')\n",
    "        \n",
    "                    \n",
    "            if s1 and s1.seqCount>= minSupport:\n",
    "                expandCnt-=1\n",
    "                seqs.append(s1)\n",
    "                #s1.after=s\n",
    "            s.before=s1\n",
    "            s.after=s0\n",
    "            \n",
    "            graph.nodes.append(Rawnode(s1))\n",
    "            graph.nodes.append(Rawnode(s0))\n",
    "            \n",
    "            graph.add(s.nid,s1.nid)\n",
    "            graph.add(s.nid,s0.nid)\n",
    "            \n",
    "            if s0 and s0.seqCount>= minSupport:\n",
    "                seqs.append(s0)\n",
    "                #s0.before=s\n",
    "            print(f'seqCount: {[s.seqCount for s in seqs]}')\n",
    "            #print(f'before: {s.before}')\n",
    "            #print(f'after: {s.after}')\n",
    "            #print(f'this: {s}')\n",
    "            #print(f' s after: {s}')\n",
    "            #print(f' s0 after: {s0}')\n",
    "            #print(f' s1 after: {s1}')\n",
    "        \n",
    "            del seqs[seqs.index(s)]\n",
    "            #print(f'seqCount: {[s.seqCount for s in seqs]}')\n",
    "            #print(f'before: {seqs[0].before}')\n",
    "            #print(f'after: {seqs[0].after}')\n",
    "            #print(f'this: {seqs[0]}')\n",
    "            \n",
    "            #print(f' s : {s}')\n",
    "            #print(f' s0 : {s0}')\n",
    "            #print(f' s1: {s1}')\n",
    "        \n",
    "        return leafSeqs.append(seqs)\n",
    "    \n",
    "    \n",
    "    def growSeq(self, attr, seq,  minSupport, maxSupport) :\n",
    "        #this is not right\n",
    "        pos=-1\n",
    "        word=\"\"\n",
    "        count=0\n",
    "        #print(f'this.pattern in growseq: {seq.pattern}')\n",
    "        #eventcol=Sequence.getUniqueEvents(seq.incomingSequences)\n",
    "        #print(f'seq pattern len {seq.keyevts}')\n",
    "        for i in range (0,len(seq.keyevts)+1):\n",
    "            fdist={}\n",
    "            #print(f'i: {i}, len {len(seq.keyevts)}')\n",
    "            for  ind, s in enumerate(seq.incomingSequences):\n",
    "                #print(f's.seqIndices: {s.seqIndices}')\n",
    "                evtHashes= s.getHashList(attr)\n",
    "                l=0 if i==0 else   s.seqIndices[i - 1] + 1\n",
    "                r=len(evtHashes) if i==len(seq.keyevts) else s.seqIndices[i]\n",
    "                \n",
    "                \n",
    "                #print(f'l index: {l}, r index {r}')\n",
    "                #print(f'evt Hash: {evtHashes}')\n",
    "                duplicate=[]\n",
    "                for j in range (l,r):\n",
    "                    w=evtHashes[j]\n",
    "                    #print(w)\n",
    "                    if w in duplicate:\n",
    "                        continue\n",
    "                    duplicate.append(w)\n",
    "                    if w not in fdist:\n",
    "                        fdist[w] = s.getVolume()\n",
    "                    else:\n",
    "                        fdist[w]+= s.getVolume()\n",
    "                \n",
    "                maxw=\"\"\n",
    "                maxc=0\n",
    "                for w in fdist.keys():\n",
    "                    value= fdist[w]\n",
    "                    \n",
    "                    if value < maxSupport and value > maxc:\n",
    "                        maxw= str(w)\n",
    "                        maxc= value\n",
    "                \n",
    "                if maxc > count:\n",
    "                    pos=i\n",
    "                    word=maxw\n",
    "                    count=maxc\n",
    "        #print(f'{word}: word')\n",
    "        #print(f'{maxc}: count')\n",
    "                    \n",
    "        s0=GraphNode()\n",
    "        s1=GraphNode()\n",
    "        \n",
    "        #print(f'this.pattern s0 in growseq: {s0.pattern}')\n",
    "        #print(f'this.pattern s1 in growseq: {s1.pattern}')\n",
    "        \n",
    "        #print(f'minSupport {minSupport} count {count}')    \n",
    "        if count >= minSupport:\n",
    "            words=seq.keyevts\n",
    "            for t in seq.incomingSequences:\n",
    "                l=0 if pos==0 else t.seqIndices[pos - 1] + 1\n",
    "                r= len(t.events) if pos == len(words) else  t.seqIndices[pos]\n",
    "                try:\n",
    "                    i = t.getHashList(attr).index(word,l,r)\n",
    "                    #print(f'position: {i}')\n",
    "                    #i+=l\n",
    "                    \n",
    "                    t.seqIndices.insert(pos,i)\n",
    "                    s1.incomingSequences.append(t)\n",
    "                    s1.seqCount+=t.getVolume()\n",
    "\n",
    "                except ValueError:\n",
    "                    #print(f'Value error')\n",
    "                    s0.incomingSequences.append(t)\n",
    "                    s0.seqCount+=t.getVolume()\n",
    "                \n",
    "        s0.setSeqCount(Sequence.getSeqVolume(s0.incomingSequences))\n",
    "        s1.setSeqCount(Sequence.getSeqVolume(s1.incomingSequences))\n",
    "        print(f'Not contain: {len(s0.incomingSequences)}')\n",
    "        print(f'contain: {len(s1.incomingSequences)}')\n",
    "        return word, pos, count, s0, s1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe( src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            print(src)\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "\n",
    "    \n",
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/sequence_braiding_refined.csv\n",
      "dict_keys(['Glucose', 'Meal'])\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "#Sequence.create_attr_dict([seq])\n",
    "#seq.getEventPosition('Meal','Lunch')\n",
    "#print(seq.getUniqueValueHashes('Meal'))\n",
    "#print(seq.getHashList('Glucose'))\n",
    "#print(seq.getValueHashes('Glucose'))\n",
    "#print(seq.getEventsHashString('Glucose'))\n",
    "#raw_seq=seq.convertToVMSPReadable('Meal')\n",
    "#print(raw_seq)\n",
    "#print(seq.getPathID())\n",
    "#sequence_braiding[0].attributes.keys()\n",
    "#print(sequence_braiding[0].getAttrVal('Meals'))\n",
    "#print(sequence_braiding[0].type)\n",
    "#for events in sequence_braiding:\n",
    "#    print(events.getAttrVal('Meal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"week\")\n",
    "#seq_list=[]\n",
    "#for seqs in sequence_braiding_split:\n",
    "#    seq_list.append(Sequence(seqs))\n",
    "    \n",
    "#Sequence.create_attr_dict(seq_list)\n",
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Meal') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Lunch', '1': 'Sugar to treat', '2': 'Afternoon snack', '3': 'Other', '4': 'Bedtime snack', '5': 'Breakfast', '6': 'Exercise snack', '7': 'Nothing', '8': 'Dinner'}\n"
     ]
    }
   ],
   "source": [
    "print(sequence_braiding_Es.reverseatttrdict['Meal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118231105811011151115\n",
      "105131582187770801055\n",
      "158015805480582175805\n",
      "820580505810580550\n",
      "360582072075111054100587\n",
      "665830758120758075480585805\n",
      "26051111075230751181610511148110158011585\n",
      "876081031510581360548110581585\n",
      "17705810580511548151157\n",
      "182770577058205176054805180111805\n",
      "87058205058770511051018011\n",
      "118110151861015871551805105820515\n",
      "860514185850486118050115\n",
      "11015111087605415051\n",
      "305805071760111171250\n",
      "76061076051810715115105\n",
      "760511100505183105152\n",
      "050580571051818810151\n",
      "70711510501116058101151\n",
      "813017110371810518805480858\n",
      "1605105158151051580\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabularies- can be emulated from attrdict\n",
    "# itemset- keys of vocabularies\n",
    "#count- seq volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sugar to treat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_list[0].events[0].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[0,1,5]\n",
    "seq_sublist=[seq_list[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breakfast'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_sublist[2].events[7].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq= [seqs.getEventsHashString('Meal') for seqs in seq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqCount: 22\n",
      "this.pattern s : []\n",
      "Not contain: 0\n",
      "contain: 22\n",
      "word: 1, pos: 0, count: 21\n",
      "seqCount: [22, 22]\n",
      "seqCount: 22\n",
      "this.pattern s : ['1']\n",
      "Not contain: 1\n",
      "contain: 21\n",
      "word: 0, pos: 1, count: 21\n",
      "seqCount: [22, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['1', '0']\n",
      "Not contain: 0\n",
      "contain: 21\n",
      "word: 5, pos: 2, count: 21\n",
      "seqCount: [21, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['1', '0', '5']\n",
      "Not contain: 0\n",
      "contain: 21\n",
      "word: 0, pos: 3, count: 21\n",
      "seqCount: [21, 21]\n",
      "seqCount: 21\n",
      "this.pattern s : ['1', '0', '5', '0']\n",
      "Not contain: 2\n",
      "contain: 19\n",
      "word: 5, pos: 4, count: 19\n",
      "seqCount: [21, 19]\n",
      "seqCount: 19\n",
      "this.pattern s : ['1', '0', '5', '0', '5']\n",
      "Not contain: 3\n",
      "contain: 16\n",
      "word: 5, pos: 5, count: 16\n",
      "seqCount: [19, 16]\n",
      "seqCount: 16\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5']\n",
      "Not contain: 5\n",
      "contain: 11\n",
      "word: 1, pos: 6, count: 11\n",
      "seqCount: [16, 11, 5]\n",
      "seqCount: 11\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1']\n",
      "Not contain: 2\n",
      "contain: 9\n",
      "word: 5, pos: 7, count: 9\n",
      "seqCount: [11, 5, 9]\n",
      "seqCount: 9\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1', '5']\n",
      "Not contain: 2\n",
      "contain: 7\n",
      "word: 8, pos: 5, count: 7\n",
      "seqCount: [5, 9, 7]\n",
      "seqCount: 7\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1', '5', '8']\n",
      "Not contain: 1\n",
      "contain: 6\n",
      "word: 0, pos: 6, count: 6\n",
      "seqCount: [5, 7, 6]\n",
      "seqCount: 6\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1', '5', '8', '0']\n",
      "Not contain: 1\n",
      "contain: 5\n",
      "word: 8, pos: 10, count: 5\n",
      "seqCount: [5, 6, 5]\n",
      "seqCount: 5\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5']\n",
      "Not contain: 0\n",
      "contain: 5\n",
      "word: 8, pos: 3, count: 5\n",
      "seqCount: [5, 5, 5]\n",
      "seqCount: 5\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1', '5', '8', '0', '8']\n",
      "Not contain: 0\n",
      "contain: 5\n",
      "word: 5, pos: 11, count: 5\n",
      "seqCount: [5, 5, 5]\n",
      "seqCount: 5\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '8']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 8, pos: 0, count: 3\n",
      "seqCount: [5, 5]\n",
      "seqCount: 5\n",
      "this.pattern s : ['1', '0', '5', '0', '5', '5', '1', '5', '8', '0', '8', '5']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 5, pos: 12, count: 4\n",
      "seqCount: [5]\n"
     ]
    }
   ],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Meal\",root,  expandCnt=30, minSupport=5, maxSupport=len(raw_seq),graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"before\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": {\n",
      "      \"before\": {\n",
      "       \"before\": {\n",
      "        \"before\": {\n",
      "         \"before\": {\n",
      "          \"before\": {\n",
      "           \"before\": {\n",
      "            \"before\": {\n",
      "             \"before\": {\n",
      "              \"before\": null,\n",
      "              \"event_attribute\": \"\",\n",
      "              \"Pattern\": [],\n",
      "              \"value\": 0,\n",
      "              \"After\": null\n",
      "             },\n",
      "             \"event_attribute\": \"Breakfast\",\n",
      "             \"Pattern\": [\n",
      "              \"1\",\n",
      "              \"0\",\n",
      "              \"5\",\n",
      "              \"0\",\n",
      "              \"5\",\n",
      "              \"5\",\n",
      "              \"1\",\n",
      "              \"5\",\n",
      "              \"8\",\n",
      "              \"0\",\n",
      "              \"8\",\n",
      "              \"5\"\n",
      "             ],\n",
      "             \"value\": 5,\n",
      "             \"After\": {\n",
      "              \"before\": null,\n",
      "              \"event_attribute\": \"\",\n",
      "              \"Pattern\": [],\n",
      "              \"value\": 0,\n",
      "              \"After\": null\n",
      "             }\n",
      "            },\n",
      "            \"event_attribute\": \"Dinner\",\n",
      "            \"Pattern\": [\n",
      "             \"1\",\n",
      "             \"0\",\n",
      "             \"5\",\n",
      "             \"0\",\n",
      "             \"5\",\n",
      "             \"5\",\n",
      "             \"1\",\n",
      "             \"5\",\n",
      "             \"8\",\n",
      "             \"0\",\n",
      "             \"8\"\n",
      "            ],\n",
      "            \"value\": 5,\n",
      "            \"After\": {\n",
      "             \"before\": null,\n",
      "             \"event_attribute\": \"\",\n",
      "             \"Pattern\": [\n",
      "              \"1\",\n",
      "              \"0\",\n",
      "              \"5\",\n",
      "              \"0\",\n",
      "              \"5\",\n",
      "              \"5\",\n",
      "              \"1\",\n",
      "              \"5\",\n",
      "              \"8\",\n",
      "              \"0\",\n",
      "              \"8\"\n",
      "             ],\n",
      "             \"value\": 0,\n",
      "             \"After\": null\n",
      "            }\n",
      "           },\n",
      "           \"event_attribute\": \"Lunch\",\n",
      "           \"Pattern\": [\n",
      "            \"1\",\n",
      "            \"0\",\n",
      "            \"5\",\n",
      "            \"0\",\n",
      "            \"5\",\n",
      "            \"5\",\n",
      "            \"1\",\n",
      "            \"5\",\n",
      "            \"8\",\n",
      "            \"0\"\n",
      "           ],\n",
      "           \"value\": 6,\n",
      "           \"After\": {\n",
      "            \"before\": null,\n",
      "            \"event_attribute\": \"\",\n",
      "            \"Pattern\": [\n",
      "             \"1\",\n",
      "             \"0\",\n",
      "             \"5\",\n",
      "             \"0\",\n",
      "             \"5\",\n",
      "             \"5\",\n",
      "             \"1\",\n",
      "             \"5\",\n",
      "             \"8\",\n",
      "             \"0\"\n",
      "            ],\n",
      "            \"value\": 1,\n",
      "            \"After\": null\n",
      "           }\n",
      "          },\n",
      "          \"event_attribute\": \"Dinner\",\n",
      "          \"Pattern\": [\n",
      "           \"1\",\n",
      "           \"0\",\n",
      "           \"5\",\n",
      "           \"0\",\n",
      "           \"5\",\n",
      "           \"5\",\n",
      "           \"1\",\n",
      "           \"5\",\n",
      "           \"8\"\n",
      "          ],\n",
      "          \"value\": 7,\n",
      "          \"After\": {\n",
      "           \"before\": null,\n",
      "           \"event_attribute\": \"\",\n",
      "           \"Pattern\": [\n",
      "            \"1\",\n",
      "            \"0\",\n",
      "            \"5\",\n",
      "            \"0\",\n",
      "            \"5\",\n",
      "            \"5\",\n",
      "            \"1\",\n",
      "            \"5\",\n",
      "            \"8\"\n",
      "           ],\n",
      "           \"value\": 1,\n",
      "           \"After\": null\n",
      "          }\n",
      "         },\n",
      "         \"event_attribute\": \"Breakfast\",\n",
      "         \"Pattern\": [\n",
      "          \"1\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"5\",\n",
      "          \"1\",\n",
      "          \"5\"\n",
      "         ],\n",
      "         \"value\": 9,\n",
      "         \"After\": {\n",
      "          \"before\": null,\n",
      "          \"event_attribute\": \"\",\n",
      "          \"Pattern\": [\n",
      "           \"1\",\n",
      "           \"0\",\n",
      "           \"5\",\n",
      "           \"0\",\n",
      "           \"5\",\n",
      "           \"5\",\n",
      "           \"1\",\n",
      "           \"5\"\n",
      "          ],\n",
      "          \"value\": 2,\n",
      "          \"After\": null\n",
      "         }\n",
      "        },\n",
      "        \"event_attribute\": \"Sugar to treat\",\n",
      "        \"Pattern\": [\n",
      "         \"1\",\n",
      "         \"0\",\n",
      "         \"5\",\n",
      "         \"0\",\n",
      "         \"5\",\n",
      "         \"5\",\n",
      "         \"1\"\n",
      "        ],\n",
      "        \"value\": 11,\n",
      "        \"After\": {\n",
      "         \"before\": null,\n",
      "         \"event_attribute\": \"\",\n",
      "         \"Pattern\": [\n",
      "          \"1\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"5\",\n",
      "          \"1\"\n",
      "         ],\n",
      "         \"value\": 2,\n",
      "         \"After\": null\n",
      "        }\n",
      "       },\n",
      "       \"event_attribute\": \"Breakfast\",\n",
      "       \"Pattern\": [\n",
      "        \"1\",\n",
      "        \"0\",\n",
      "        \"5\",\n",
      "        \"0\",\n",
      "        \"5\",\n",
      "        \"5\"\n",
      "       ],\n",
      "       \"value\": 16,\n",
      "       \"After\": {\n",
      "        \"before\": {\n",
      "         \"before\": {\n",
      "          \"before\": null,\n",
      "          \"event_attribute\": \"\",\n",
      "          \"Pattern\": [],\n",
      "          \"value\": 0,\n",
      "          \"After\": null\n",
      "         },\n",
      "         \"event_attribute\": \"Dinner\",\n",
      "         \"Pattern\": [\n",
      "          \"1\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"5\",\n",
      "          \"8\"\n",
      "         ],\n",
      "         \"value\": 5,\n",
      "         \"After\": {\n",
      "          \"before\": null,\n",
      "          \"event_attribute\": \"\",\n",
      "          \"Pattern\": [],\n",
      "          \"value\": 0,\n",
      "          \"After\": null\n",
      "         }\n",
      "        },\n",
      "        \"event_attribute\": \"\",\n",
      "        \"Pattern\": [\n",
      "         \"1\",\n",
      "         \"0\",\n",
      "         \"5\",\n",
      "         \"0\",\n",
      "         \"5\",\n",
      "         \"5\"\n",
      "        ],\n",
      "        \"value\": 5,\n",
      "        \"After\": {\n",
      "         \"before\": null,\n",
      "         \"event_attribute\": \"\",\n",
      "         \"Pattern\": [\n",
      "          \"1\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"0\",\n",
      "          \"5\",\n",
      "          \"5\"\n",
      "         ],\n",
      "         \"value\": 0,\n",
      "         \"After\": null\n",
      "        }\n",
      "       }\n",
      "      },\n",
      "      \"event_attribute\": \"Breakfast\",\n",
      "      \"Pattern\": [\n",
      "       \"1\",\n",
      "       \"0\",\n",
      "       \"5\",\n",
      "       \"0\",\n",
      "       \"5\"\n",
      "      ],\n",
      "      \"value\": 19,\n",
      "      \"After\": {\n",
      "       \"before\": null,\n",
      "       \"event_attribute\": \"\",\n",
      "       \"Pattern\": [\n",
      "        \"1\",\n",
      "        \"0\",\n",
      "        \"5\",\n",
      "        \"0\",\n",
      "        \"5\"\n",
      "       ],\n",
      "       \"value\": 3,\n",
      "       \"After\": null\n",
      "      }\n",
      "     },\n",
      "     \"event_attribute\": \"Lunch\",\n",
      "     \"Pattern\": [\n",
      "      \"1\",\n",
      "      \"0\",\n",
      "      \"5\",\n",
      "      \"0\"\n",
      "     ],\n",
      "     \"value\": 21,\n",
      "     \"After\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": [\n",
      "       \"1\",\n",
      "       \"0\",\n",
      "       \"5\",\n",
      "       \"0\"\n",
      "      ],\n",
      "      \"value\": 2,\n",
      "      \"After\": null\n",
      "     }\n",
      "    },\n",
      "    \"event_attribute\": \"Breakfast\",\n",
      "    \"Pattern\": [\n",
      "     \"1\",\n",
      "     \"0\",\n",
      "     \"5\"\n",
      "    ],\n",
      "    \"value\": 21,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": [\n",
      "      \"1\",\n",
      "      \"0\",\n",
      "      \"5\"\n",
      "     ],\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"Lunch\",\n",
      "   \"Pattern\": [\n",
      "    \"1\",\n",
      "    \"0\"\n",
      "   ],\n",
      "   \"value\": 21,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": [\n",
      "     \"1\",\n",
      "     \"0\"\n",
      "    ],\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"Sugar to treat\",\n",
      "  \"Pattern\": [\n",
      "   \"1\"\n",
      "  ],\n",
      "  \"value\": 22,\n",
      "  \"After\": {\n",
      "   \"before\": null,\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": [\n",
      "    \"1\"\n",
      "   ],\n",
      "   \"value\": 1,\n",
      "   \"After\": null\n",
      "  }\n",
      " },\n",
      " \"event_attribute\": \"\",\n",
      " \"Pattern\": [],\n",
      " \"value\": 22,\n",
      " \"After\": {\n",
      "  \"before\": null,\n",
      "  \"event_attribute\": \"\",\n",
      "  \"Pattern\": [],\n",
      "  \"value\": 0,\n",
      "  \"After\": null\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON Encode data into Python String\n",
      "\"{\\\"links\\\": {\\\"1\\\": [2, 3], \\\"3\\\": [4, 5], \\\"5\\\": [6, 7], \\\"7\\\": [8, 9], \\\"9\\\": [10, 11], \\\"11\\\": [12, 13], \\\"13\\\": [14, 15], \\\"15\\\": [16, 17], \\\"17\\\": [18, 19], \\\"19\\\": [20, 21], \\\"21\\\": [22, 23], \\\"14\\\": [24, 25], \\\"23\\\": [26, 27], \\\"25\\\": [28, 29], \\\"27\\\": [30, 31], \\\"default_factory\\\": {\\\"py/type\\\": \\\"builtins.set\\\"}}, \\\"nodes\\\": [{\\\"nid\\\": 1, \\\"seqCount\\\": 22, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 3, \\\"seqCount\\\": 22, \\\"value\\\": \\\"Sugar to treat\\\"}, {\\\"nid\\\": 2, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 5, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Lunch\\\"}, {\\\"nid\\\": 4, \\\"seqCount\\\": 1, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 7, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Breakfast\\\"}, {\\\"nid\\\": 6, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 9, \\\"seqCount\\\": 21, \\\"value\\\": \\\"Lunch\\\"}, {\\\"nid\\\": 8, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 11, \\\"seqCount\\\": 19, \\\"value\\\": \\\"Breakfast\\\"}, {\\\"nid\\\": 10, \\\"seqCount\\\": 2, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 13, \\\"seqCount\\\": 16, \\\"value\\\": \\\"Breakfast\\\"}, {\\\"nid\\\": 12, \\\"seqCount\\\": 3, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 15, \\\"seqCount\\\": 11, \\\"value\\\": \\\"Sugar to treat\\\"}, {\\\"nid\\\": 14, \\\"seqCount\\\": 5, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 17, \\\"seqCount\\\": 9, \\\"value\\\": \\\"Breakfast\\\"}, {\\\"nid\\\": 16, \\\"seqCount\\\": 2, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 19, \\\"seqCount\\\": 7, \\\"value\\\": \\\"Dinner\\\"}, {\\\"nid\\\": 18, \\\"seqCount\\\": 2, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 21, \\\"seqCount\\\": 6, \\\"value\\\": \\\"Lunch\\\"}, {\\\"nid\\\": 20, \\\"seqCount\\\": 1, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 23, \\\"seqCount\\\": 5, \\\"value\\\": \\\"Dinner\\\"}, {\\\"nid\\\": 22, \\\"seqCount\\\": 1, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 25, \\\"seqCount\\\": 5, \\\"value\\\": \\\"Dinner\\\"}, {\\\"nid\\\": 24, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 27, \\\"seqCount\\\": 5, \\\"value\\\": \\\"Breakfast\\\"}, {\\\"nid\\\": 26, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 29, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 28, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 31, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}, {\\\"nid\\\": 30, \\\"seqCount\\\": 0, \\\"value\\\": \\\"\\\"}]}\"\n"
     ]
    }
   ],
   "source": [
    "empJSON = jsonpickle.encode(graph, unpicklable=False)\n",
    "\n",
    "print(\"Writing JSON Encode data into Python String\")\n",
    "employeeJSONData = json.dumps(empJSON, indent=4)\n",
    "print(employeeJSONData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Circular reference detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-36c41616a18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_serialize_dump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0mmarkerid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmarkerid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Circular reference detected"
     ]
    }
   ],
   "source": [
    "x=json.dumps(graph, ensure_ascii=False, default=Graph.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(graph, default=lambda o: o.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(vars(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {32: {33, 34},\n",
       "             34: {35, 36},\n",
       "             33: {37, 38},\n",
       "             36: {39, 40},\n",
       "             35: {41, 42},\n",
       "             38: {43, 44},\n",
       "             40: {45, 46},\n",
       "             44: {47, 48},\n",
       "             48: {49, 50}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Rawnode at 0x7fc9df1489d0>,\n",
       " <__main__.Rawnode at 0x7fc9def9dcd0>,\n",
       " <__main__.Rawnode at 0x7fc9def9dd10>,\n",
       " <__main__.Rawnode at 0x7fc9def9e510>,\n",
       " <__main__.Rawnode at 0x7fc9def9e550>,\n",
       " <__main__.Rawnode at 0x7fc9def9ed10>,\n",
       " <__main__.Rawnode at 0x7fc9def9ed50>,\n",
       " <__main__.Rawnode at 0x7fc9defa0590>,\n",
       " <__main__.Rawnode at 0x7fc9defa05d0>,\n",
       " <__main__.Rawnode at 0x7fc9df0666d0>,\n",
       " <__main__.Rawnode at 0x7fc9df066ad0>,\n",
       " <__main__.Rawnode at 0x7fca083c9390>,\n",
       " <__main__.Rawnode at 0x7fca083c9410>,\n",
       " <__main__.Rawnode at 0x7fc9df143e90>,\n",
       " <__main__.Rawnode at 0x7fc9df143e50>,\n",
       " <__main__.Rawnode at 0x7fc9df069790>,\n",
       " <__main__.Rawnode at 0x7fc9df069390>,\n",
       " <__main__.Rawnode at 0x7fc9defa0f90>,\n",
       " <__main__.Rawnode at 0x7fc9defa0fd0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=[]\n",
    "#merge(h,key=lambda e:e[0],reverse=True)\n",
    "heapq.heappush(h, (200, 1))\n",
    "heapq.heappush(h, (300,2))\n",
    "heapq.heappush(h, (400,3))\n",
    "print(heapq.heappop(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"abcd\".index('c',2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Sample_Dataset.csv\n",
      "dict_keys(['Events', 'Counts'])\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../Sample_Dataset.csv', 0, \"%m/%d/%Y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"day\")\n",
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Events') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'B', '1': 'E', '2': 'F', '3': 'A', '4': 'K', '5': 'C', '6': 'G', '7': 'H', '8': 'L', '9': 'I', ':': 'J', ';': 'D'}\n"
     ]
    }
   ],
   "source": [
    "print(sequence_braiding_Es.reverseatttrdict['Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqCount: 6\n",
      "this.pattern s : []\n",
      "Not contain: 2\n",
      "contain: 4\n",
      "word: 3, pos: 0, count: 4\n",
      "seqCount: [6, 4, 2]\n",
      "seqCount: 4\n",
      "this.pattern s : ['3']\n",
      "Not contain: 2\n",
      "contain: 2\n",
      "word: 5, pos: 1, count: 2\n",
      "seqCount: [4, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : []\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 5, pos: 0, count: 2\n",
      "seqCount: [2, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3', '5']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 5, pos: 2, count: 2\n",
      "seqCount: [2, 2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: ;, pos: 0, count: 1\n",
      "seqCount: [2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['5']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 2, pos: 1, count: 2\n",
      "seqCount: [2, 2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['3', '5', '5']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 5, pos: 0, count: 1\n",
      "seqCount: [2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['5', '2']\n",
      "Not contain: 0\n",
      "contain: 2\n",
      "word: 6, pos: 2, count: 2\n",
      "seqCount: [2, 2]\n",
      "seqCount: 2\n",
      "this.pattern s : ['5', '2', '6']\n",
      "Not contain: 0\n",
      "contain: 0\n",
      "word: 8, pos: 1, count: 1\n",
      "seqCount: [2]\n"
     ]
    }
   ],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Events\",root,  expandCnt=30, minSupport=2, maxSupport=len(seq_list),graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"before\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": [],\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    },\n",
      "    \"event_attribute\": \"C\",\n",
      "    \"Pattern\": [\n",
      "     \"3\",\n",
      "     \"5\",\n",
      "     \"5\"\n",
      "    ],\n",
      "    \"value\": 2,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": [],\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"C\",\n",
      "   \"Pattern\": [\n",
      "    \"3\",\n",
      "    \"5\"\n",
      "   ],\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": [\n",
      "     \"3\",\n",
      "     \"5\"\n",
      "    ],\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"A\",\n",
      "  \"Pattern\": [\n",
      "   \"3\"\n",
      "  ],\n",
      "  \"value\": 4,\n",
      "  \"After\": {\n",
      "   \"before\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": [],\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   },\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": [\n",
      "    \"3\"\n",
      "   ],\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": [],\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  }\n",
      " },\n",
      " \"event_attribute\": \"\",\n",
      " \"Pattern\": [],\n",
      " \"value\": 6,\n",
      " \"After\": {\n",
      "  \"before\": {\n",
      "   \"before\": {\n",
      "    \"before\": {\n",
      "     \"before\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": [],\n",
      "      \"value\": 0,\n",
      "      \"After\": null\n",
      "     },\n",
      "     \"event_attribute\": \"G\",\n",
      "     \"Pattern\": [\n",
      "      \"5\",\n",
      "      \"2\",\n",
      "      \"6\"\n",
      "     ],\n",
      "     \"value\": 2,\n",
      "     \"After\": {\n",
      "      \"before\": null,\n",
      "      \"event_attribute\": \"\",\n",
      "      \"Pattern\": [],\n",
      "      \"value\": 0,\n",
      "      \"After\": null\n",
      "     }\n",
      "    },\n",
      "    \"event_attribute\": \"F\",\n",
      "    \"Pattern\": [\n",
      "     \"5\",\n",
      "     \"2\"\n",
      "    ],\n",
      "    \"value\": 2,\n",
      "    \"After\": {\n",
      "     \"before\": null,\n",
      "     \"event_attribute\": \"\",\n",
      "     \"Pattern\": [\n",
      "      \"5\",\n",
      "      \"2\"\n",
      "     ],\n",
      "     \"value\": 0,\n",
      "     \"After\": null\n",
      "    }\n",
      "   },\n",
      "   \"event_attribute\": \"C\",\n",
      "   \"Pattern\": [\n",
      "    \"5\"\n",
      "   ],\n",
      "   \"value\": 2,\n",
      "   \"After\": {\n",
      "    \"before\": null,\n",
      "    \"event_attribute\": \"\",\n",
      "    \"Pattern\": [\n",
      "     \"5\"\n",
      "    ],\n",
      "    \"value\": 0,\n",
      "    \"After\": null\n",
      "   }\n",
      "  },\n",
      "  \"event_attribute\": \"\",\n",
      "  \"Pattern\": [],\n",
      "  \"value\": 2,\n",
      "  \"After\": {\n",
      "   \"before\": null,\n",
      "   \"event_attribute\": \"\",\n",
      "   \"Pattern\": [],\n",
      "   \"value\": 0,\n",
      "   \"After\": null\n",
      "  }\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
