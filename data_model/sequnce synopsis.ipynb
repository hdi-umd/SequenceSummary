{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf\n",
    "import json\n",
    "import jsonpickle\n",
    "import heapq\n",
    "\n",
    "import pprint\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import jsonpickle\n",
    "from bisect import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common class for all Events\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, eventtype):\n",
    "        self.type=eventtype\n",
    "    \n",
    "    #Return Attribute value given attribute name\n",
    "    def getAttrVal(self, attrName):\n",
    "        return self.attributes.get(attrName,None)\n",
    "\n",
    "    \n",
    "# A class that represents a point event\n",
    "class PointEvent(Event):\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        super().__init__(\"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "        \n",
    "    \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent(Event):\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        super().__init__(\"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    \n",
    "    def __init__(self, eventlist=[]):\n",
    "        self.attrdict={}\n",
    "        self.reverseatttrdict={}\n",
    "        self.events=eventlist\n",
    "\n",
    "    #should be moved to EventStore\n",
    "    # hold the list of events, also the dictionaries\n",
    "    \n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importPointEvents(self, src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != timestampColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = PointEvent(timestamp, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events\n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    #@staticmethod\n",
    "    def importIntervalEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for the start and end times of the event\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events    \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    #@staticmethod\n",
    "    def importMixedEvents(self, src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "            if type(data[endTimeColumnIdx]) is float:\n",
    "                t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                event_type = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "                event_type = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "            attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "            for i in attribute_columns:\n",
    "                attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            if event_type == \"point\":\n",
    "                e = PointEvent(t, attribs)\n",
    "            else:\n",
    "                e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        self.events=events   \n",
    "        #sequence=Sequence(events)\n",
    "        self.create_attr_dict()\n",
    "        #return sequence\n",
    "\n",
    "    #should take an eventlist as input\n",
    "    # Group events by attributeName, and order them by timestamp\n",
    "    #@staticmethod\n",
    "    #should return a list of sequences\n",
    "    def generateSequence(self, attributeName):\n",
    "        eventList=self.events\n",
    "        grouped_by = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in grouped_by for value\n",
    "            if value in grouped_by:\n",
    "                grouped_by[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                grouped_by[value] = [event]\n",
    "        sequences= list(grouped_by.values())\n",
    "        seqlist=[]\n",
    "        for seq in sequences:\n",
    "            seqlist.append(Sequence(seq, self))\n",
    "        return seqlist\n",
    "    \n",
    "    # Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "    # break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "    # “week”, “month”, “quarter”, and “year”.\n",
    "    # For interval events I used the start time of the event to determine its category when splitting it\n",
    "    \n",
    "    #ZINAT- changes\n",
    "    #SequenceList represents a list of objects of type Sequence. The sequences are further splitted into\n",
    "    #sequence objects, this way we can use generate sequences and then splitSequences \n",
    "    @staticmethod\n",
    "    def splitSequences(sequenceLists, timeUnit, record=None):\n",
    "        if not isinstance(sequenceLists, list):\n",
    "            sequenceLists=[sequenceLists]\n",
    "        eventstore=sequenceLists[0].eventstore\n",
    "        results = []\n",
    "        resultlist=[]\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in valid_time_units:\n",
    "            raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "        \n",
    "        for sequence in sequenceLists:\n",
    "            # Sort the events by the timestamp or event start time\n",
    "            sequenceList= sequence.events\n",
    "            sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "\n",
    "            # Process the event sequence based on the given time unit\n",
    "            # Generally, create a map for that time unit and then add each event into that map \n",
    "            # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "            if timeUnit == \"hour\":\n",
    "                hours = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.hour, time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,hours,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "                results = list(hours.values())\n",
    "\n",
    "            elif timeUnit == \"day\":\n",
    "                days = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,days,event)\n",
    "                    #print(days)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                results = list(days.values())\n",
    "\n",
    "            elif timeUnit == \"month\":\n",
    "                months = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.month,time.year)\n",
    "                    insert_event_into_dict(key,months,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(months.values())\n",
    "\n",
    "            elif timeUnit == \"week\":\n",
    "                weeks = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    week_num = time.isocalendar()[1]\n",
    "                    key = (year,week_num)\n",
    "                    insert_event_into_dict(key,weeks,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "                results = list(weeks.values())\n",
    "\n",
    "            elif timeUnit == \"year\":\n",
    "                years = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = time.year\n",
    "                    insert_event_into_dict(key,years,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key)\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "                results = list(years.values())\n",
    "\n",
    "            elif timeUnit == \"quarter\":\n",
    "                quarters = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    month = time.month\n",
    "                    # Determine the year, quarter pair/key for quarter dict\n",
    "                    # January, February, and March (Q1)\n",
    "                    if month in range(1, 4):\n",
    "                        key = (year, \"Q1\")\n",
    "                    # April, May, and June (Q2)\n",
    "                    elif month in range(4, 7):\n",
    "                        key = (year, \"Q2\")\n",
    "                    # July, August, and September (Q3)\n",
    "                    elif month in range(7,10):\n",
    "                        key = (year, \"Q3\")\n",
    "                    # October, November, and December (Q4)\n",
    "                    elif month in range(10,13):\n",
    "                        key = (year, \"Q4\")\n",
    "                    # Put the event in the dictionary\n",
    "                    insert_event_into_dict(key,quarters,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(quarters.values())\n",
    "            resultlist.extend(results)\n",
    "        resultlists= [Sequence(x, eventstore) for x in resultlist]\n",
    "\n",
    "        return resultlists\n",
    "    \n",
    "    def getUniqueValues(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return l\n",
    "    \n",
    "    #Assuming we are given a list of events and from those events we create \n",
    "    #the mapping and reverse mapping dictionary\n",
    "    def create_attr_dict(self):\n",
    "        attr_list=self.events[0].attributes.keys()\n",
    "        print(attr_list)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            a=48\n",
    "            unique_list=[]\n",
    "            unique_list.extend(self.getUniqueValues(attr))\n",
    "            unique_list=list(set(unique_list))\n",
    "            #unique_list.clear()\n",
    "            \n",
    "            unicode_dict={}\n",
    "            reverse_dict={}\n",
    "            for uniques in unique_list:\n",
    "                unicode_dict[uniques]=chr(a)\n",
    "                reverse_dict[chr(a)]=uniques\n",
    "                a=a+1\n",
    "            self.attrdict[attr]=unicode_dict\n",
    "            self.reverseatttrdict[attr]=reverse_dict\n",
    "            #unicode_dict.clear()                    \n",
    "    \n",
    "    def get_event_name(self, attr, eventlist):\n",
    "        #for val in eventlist:\n",
    "        #    print(self.reverseatttrdict[attr][val])\n",
    "        return \"\".join(self.reverseatttrdict[attr][val] for val in eventlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence():\n",
    "    _ids = count(0)\n",
    "    \n",
    "\n",
    "    def __init__(self,  eventlist, eventstore,sid=None):\n",
    "        # sequence id\n",
    "        if sid is None:\n",
    "            self.sid = next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "        \n",
    "        self.events = eventlist\n",
    "        self.eventstore=eventstore\n",
    "        self.volume=1\n",
    "        self.seqAttributes={}\n",
    "        self.seqIndices=[]\n",
    "    def getEventPosition(self, attr, hash_val):\n",
    "        for count,event in enumerate(self.events):\n",
    "            #if event.getAttrVal(attr)==hash_val:\n",
    "            if self.eventstore.attrdict[attr][event.getAttrVal(attr)]==hash_val:\n",
    "                return count\n",
    "        return -1\n",
    "    \n",
    "    def setVolume(self, intValue):\n",
    "        self.volume=intValue\n",
    "        \n",
    "    def getVolume(self):\n",
    "        return self.volume\n",
    "    \n",
    "    def increaseVolume(self):\n",
    "        self.volume += 1 \n",
    "    \n",
    "    \n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        uniquelist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        return uniquelist\n",
    "    \n",
    "    #Not sure this will always result in same index, will change if \n",
    "    #dictionary is updated\n",
    "    #since python is unordered\n",
    "    \n",
    "    def getHashList(self, attr):\n",
    "        #l=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        l=[event.getAttrVal(attr) for event in self.events]\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "    \n",
    "    def getValueHashes(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "        \n",
    "    \n",
    "    def getEventsHashString(self, attr):\n",
    "        s=\"\"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"\".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #print(s)\n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadablenum(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" -1 \".join(str(self.eventstore.attrdict[attr][elem]) for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\" -2\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" \".join(self.eventstore.attrdict[attr][elem] for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\".\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def getPathID(self):\n",
    "        return self.sid\n",
    "    \n",
    "    def matchPathAttribute(self, attr, val):\n",
    "        # should i use eq?!\n",
    "        if this.seqAttributes.get(attr)==(val):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def setSequenceAttribute(self,attr, value):\n",
    "        self.seqAttributes[attr]=value\n",
    "        \n",
    "         \n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)    \n",
    "    def getSeqVolume(seqlist):\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "    \n",
    "    \n",
    "    # Method equivalent to public String getEvtAttrValue(String attr, int hash) in DataManager.java\n",
    "    def getEvtAttrValue(self, attr, hashval):\n",
    "        return self.eventstore.reverseatttrdict[attr][hashval]\n",
    "        \n",
    "    # Method equivalent to public List<String> getEvtAttrValues(String attr) in DataManager.java    \n",
    "    def getEvtAttrValues(self, attr):\n",
    "        l=[event.getAttrVal(attr) for event in self.events]\n",
    "        return l\n",
    "        #hashlist=[self.eventstore.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        #return list(self.eventstore.reverseatttrdict[attr].values())\n",
    "    \n",
    "    # Method equivalent to int getEvtAttrValueCount(String attr) in DataManager.java    \n",
    "    def getEvtAttrValueCount(self, attr):\n",
    "        return len(self.eventstore.reverseatttrdict[attr])\n",
    "    \n",
    "    def getEventsString(self, attr):\n",
    "        s=\"\"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"-\".join(elem for elem in l)\n",
    "        #print(s)\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    \n",
    "    def getUniqueEvents(seqlist):\n",
    "        l=list(set(event.getAttrVal(attr) for event in seq for seq in seqlist))\n",
    "        return l\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattern:\n",
    "    _pids = count(1)\n",
    "\n",
    "    def __init__(self, events=[]):\n",
    "        #pattern id\n",
    "        self.id = next(self._pids)\n",
    "        \n",
    "        self.keyEvts = events\n",
    "        \n",
    "        self.medianPos=[]\n",
    "        self.meanPos=[]\n",
    "        \n",
    "        self.sids=[]\n",
    "        \n",
    "        self.support=0\n",
    "        self.supPercent=None\n",
    "        self.cluster=None\n",
    "        self.medianPathLength=0\n",
    "        self.meanPathLength=0\n",
    "        \n",
    "        self.parnetSegment=None\n",
    "        self.segSizes=None\n",
    "        \n",
    "    def filterPaths(self, paths, evtType):\n",
    "        print(\"filtering \"+ str(len(paths))+\" paths by \"+str(len(self.keyEvts))+\" checkpoints\")\n",
    "        \n",
    "        for sequences in paths:\n",
    "            if(self.matchMilestones(sequences.getValueHashes(evtType),self.keyEvts)==False):\n",
    "                continue\n",
    "            self.sids.append(sequences)\n",
    "            \n",
    "        print(str(len(self.sids))+\" matching paths\")\n",
    "\n",
    "        \n",
    "    def matchMilestones(self, arr, milestones):\n",
    "        ja=arr\n",
    "        idx=-1\n",
    "        for elems in milestones:\n",
    "            try:\n",
    "                idx=arr[idx+1:].index(elems)\n",
    "                #print(idx)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def getMedianSpacing(self):\n",
    "        l=[y - x for x,y in zip(self.medianPos,self.medianPos[1:])]\n",
    "        if(len(l)<=1):\n",
    "            return 100\n",
    "        l=l.sort()\n",
    "        middle=int(len(l)/2)\n",
    "        if(len(l)%2==0):\n",
    "            return ((l[middle-1]+l[middle])/2.0)\n",
    "        else:\n",
    "            return l[middle]\n",
    "        return np.median(np.asarray(l))\n",
    "    \n",
    "    def addKeyEvent(self, hashval):\n",
    "        self.keyEvts.append(hashval)\n",
    "        \n",
    "    def addToSupportSet(self, seq):\n",
    "        #print(seq.sid)\n",
    "        self.sids.append(seq)\n",
    "        self.support+=seq.getVolume()\n",
    "        \n",
    "    def getSequences(self):\n",
    "        return self.sids\n",
    "    \n",
    "    def setMedianPathLength(self, median):\n",
    "        self.medianPathLength=median\n",
    "    \n",
    "    def setMeanPathLength(self, mean):\n",
    "        self.meanPathLength=mean\n",
    "        \n",
    "    def getMedianPathLength(self):\n",
    "        return self.medianPathLength\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "    \n",
    "    def getEvents(self):\n",
    "        return self.keyEvts\n",
    "    \n",
    "    def getEventMeanPos(self):\n",
    "        return self.meanPos\n",
    "    \n",
    "    def getEventMedianPos(self):\n",
    "        return self.medianPos\n",
    "    \n",
    "    #Do we need to preserve order here??\n",
    "\n",
    "    def getUniqueEventsString(self):\n",
    "        #return \"-\".join(str(x) for x in list(set(self.keyEvts)))\n",
    "        return \"-\".join(str(x) for x in list(dict.fromkeys(self.keyEvts)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def getPositions(events, path):\n",
    "        sequence=path\n",
    "        pos=[]\n",
    "        idx=-1\n",
    "        offset=0\n",
    "        \n",
    "        for elems in events:\n",
    "            \n",
    "            offset+=idx+1\n",
    "            try:\n",
    "                idx=path[offset:].index(elems)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            pos.append(offset+idx)\n",
    "        return pos\n",
    "    \n",
    "    def getMedian(self, data):\n",
    "        #middle=len(data)/2\n",
    "        #if(len(data)%2==0 and len(data)>1):\n",
    "        #    return (data[middle-1]+data[middle])/2.0\n",
    "        #else: \n",
    "        #    return data[middle]\n",
    "        return np.median(data)\n",
    "    \n",
    "    def computePatternStats(self, evtAttr):\n",
    "        pathsOfStrings=[]\n",
    "        #print(f' sids {self.sids}')\n",
    "        for path in self.sids:\n",
    "            pageSequence=path.getHashList(evtAttr)\n",
    "            pathsOfStrings.append(pageSequence)\n",
    "        \n",
    "        #print(f'path of string {pathsOfStrings}')\n",
    "        medians=[]\n",
    "        means=[]\n",
    "        \n",
    "        ## swap the loops for better readability\n",
    "        for i,events in enumerate(self.keyEvts):\n",
    "            numSteps=[]\n",
    "            \n",
    "            for idx,paths in enumerate(pathsOfStrings):\n",
    "                if(self.matchMilestones(paths, self.keyEvts[0:i+1])):\n",
    "                    pos=self.getPositions(self.keyEvts[0:i+1], paths)\n",
    "                    if i==0:\n",
    "                        #add position value of first element id sequence\n",
    "                        numSteps.append(pos[i])\n",
    "                    else:\n",
    "                        #in other cases add the difference\n",
    "                        numSteps.append(pos[i]-pos[i-1])\n",
    "            sum_steps=sum(numSteps)\n",
    "            \n",
    "            median= self.getMedian(numSteps)\n",
    "            \n",
    "            medians.append(median)\n",
    "            means.append(sum_steps*1.0/ len(numSteps))\n",
    "                \n",
    "            \n",
    "                \n",
    "        #list(accumulate(means))\n",
    "        means=np.cumsum(np.asarray(means))\n",
    "        medians=np.cumsum(np.asarray(medians))\n",
    "        \n",
    "        self.setMedianPositions(medians)\n",
    "        self.setMeanPositions(means)\n",
    "        \n",
    "        trailingSteps=[0]*len(self.sids)\n",
    "        for i,path in enumerate(self.sids):\n",
    "            pos=self.getPositions(self.keyEvts, path.getHashList(evtAttr))\n",
    "            #the difference between the last event in thesequence and the last key event\n",
    "            trailingSteps[i]= len(path.events)- pos[-1]-1\n",
    "        \n",
    "        trailStepSum=sum(trailingSteps)\n",
    "        median= self.getMedian(trailingSteps)\n",
    "        mean= trailStepSum/len(trailingSteps)\n",
    "        \n",
    "        self.setMedianPathLength(median+medians[-1])\n",
    "        self.setMeanPathLength(mean+means[-1])\n",
    "                                  \n",
    "    def getMedianPositions(self, allPos, pids):\n",
    "        median=[]\n",
    "        for k in range(0, len(pid)):\n",
    "            posInPaths=allPos[k]\n",
    "            median.append(self.getMedian(posInPaths))\n",
    "        #return list(self.getMedian(posInPaths) for posInPaths in allPos)\n",
    "        return median\n",
    "    \n",
    "    def getMeanPositions(self, allPos, pids):\n",
    "        mean=[]\n",
    "        for k in range(0, len(allPos)):\n",
    "            mean.append(sum(allPos[k])*1.0/(len(allPos[k])))\n",
    "        return mean\n",
    "    \n",
    "    def setMedianPositions(self, median):\n",
    "        self.medianPos=median\n",
    "        \n",
    "    def setMeanPositions(self, mean):\n",
    "        self.meanPos=mean\n",
    "        \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4)\n",
    "    \n",
    "    def getSupport(self):\n",
    "        return self.support\n",
    "    \n",
    "    def setCluster(self, cluster):\n",
    "        self.cluster=cluster\n",
    "        \n",
    "    def setParent(self, parent, segment):\n",
    "        self.parent=parent\n",
    "        self.parentSegment=segment\n",
    "    \n",
    "    \n",
    "    # How to implement this with BitArray?\n",
    "    #def getEventBitSet(self)\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    \n",
    "    def getParentSegment(self):\n",
    "        return self.parentSegment\n",
    "    \n",
    "    def setMeanPathLength(self,d):\n",
    "        self.meanPathLength=d\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "        \n",
    "    def setSupport(self, sup, total):\n",
    "        self.support=sup\n",
    "        self.supPercent= sup*1.0/total\n",
    "        \n",
    "    def getEvtAttrValue(self, attr, eventstore):\n",
    "        return \"\".join(eventstore.reverseatttrdict[attr][hashval] for hashval in self.keyEvts)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster:\n",
    "    \n",
    "    def __init__(self, pat, seq):\n",
    "        self.pattern = pat\n",
    "        self.seqList = seq\n",
    "    \n",
    "def print_clust(clust_d, attr):\n",
    "    print(\"------key val pairs----\")\n",
    "    for key, value in clust_d.items():\n",
    "        print(f'Pattern {key.keyEvts}')\n",
    "        for v in value:\n",
    "            print(f'Sequences {v.getHashList(attr)}')\n",
    "\n",
    "class SequenceSynopsis:\n",
    "    \n",
    "    def minDL(self, Sequences, attr):\n",
    "        # Initialization Phase\n",
    "        clust_dict={}\n",
    "        clust_dict={Pattern(seq.getHashList(attr)): [seq] for seq in Sequences}\n",
    "        print(\"Initial clusts\")\n",
    "        print_clust(clust_dict, attr)\n",
    "        #print(clust_dict.values())\n",
    "\n",
    "        PriorituQueue=[]\n",
    "\n",
    "        for c_i in clust_dict.items():\n",
    "            for c_j in clust_dict.items():\n",
    "                #print(f'c_i: {c_i}')\n",
    "                if (c_i[0].id== c_j[0].id):\n",
    "                    continue\n",
    "                del_L, c_star=self.merge(c_i,c_j, attr)\n",
    "                if del_L>0:\n",
    "                    PriorituQueue.append(tuple((del_L,c_i,c_j,c_star)))\n",
    "        #print(f'Priority {PriorituQueue}')\n",
    "\n",
    "        while PriorituQueue:\n",
    "            PriorituQueue=sorted(PriorituQueue, key= lambda x: x[0], reverse=True)# sort on del_L\n",
    "            #print(f'Priority {PriorituQueue}')\n",
    "            del_L,c_i,c_j,c_star= PriorituQueue[0]\n",
    "            c_new=c_star\n",
    "            #print(f'PQ values {del_L}, {c_i}, {c_j}, {c_star}')\n",
    "            #print(f'c_i: {c_i[0]}')\n",
    "            #clust_dict.pop(c_i[0], None)\n",
    "            #clust_dict.pop(c_j[0], None)\n",
    "            print_clust(clust_dict, attr)\n",
    "            del clust_dict[c_i[0]]\n",
    "            del clust_dict[c_j[0]]\n",
    "\n",
    "            clust_dict[c_new[0]]=c_new[1]\n",
    "            print(f'c_i: {c_i}')\n",
    "            print(f'c_j: {c_j}')\n",
    "            delete_indices=[]\n",
    "\n",
    "            #print(f'PQ before deletion {PriorituQueue}')\n",
    "            for val, entries in enumerate(PriorituQueue):\n",
    "                #print(f'val {val} entries {entries}')\n",
    "                #print(f'entries[1][0] {entries[1][0]} entries[2][0] {entries[2][0]}')\n",
    "                if c_i[0]==entries[1][0] or c_j[0]==entries[1][0]:\n",
    "                    #print(f'metched c_i {c_i}')\n",
    "                    delete_indices.append(val)\n",
    "                    #del entries\n",
    "                elif c_i[0]==entries[2][0] or c_j[0]==entries[2][0]:\n",
    "                    #print(f'metched c_i {c_j}')\n",
    "                    delete_indices.append(val)\n",
    "                    #del entries\n",
    "            #print(f'PQ {PriorituQueue}')\n",
    "\n",
    "            for index in sorted(delete_indices, reverse=True):\n",
    "                del PriorituQueue[index]\n",
    "            #print(f'PQ after deletion {PriorituQueue}')\n",
    "            #del PriorituQueue[delete_indices]    \n",
    "            for c in clust_dict.items():\n",
    "                if c==c_new:\n",
    "                    continue\n",
    "\n",
    "                del_L, c_star= self.merge(c, c_new, attr)\n",
    "\n",
    "                if del_L>0:\n",
    "                    PriorituQueue.append(tuple((del_L,c,c_new, c_star)))\n",
    "                    #clust_dict[]\n",
    "        return clust_dict\n",
    "    \n",
    "    def merge(self, pair1, pair2,attr, alpha=1, lambda_val=0):\n",
    "        key1, val1=pair1\n",
    "        key2, val2=pair2\n",
    "        #print(f'key {key1.keyEvts}')\n",
    "        P_star= Pattern(lcs(key1.keyEvts,key2.keyEvts))\n",
    "        print(f'Pattern 1 {key1.keyEvts}')\n",
    "        print(f'Pattern 2 {key2.keyEvts}')\n",
    "        print(f'Pattern LCS {P_star.keyEvts}')\n",
    "        E_c=list(((Counter(key1.keyEvts)-Counter(P_star.keyEvts))|(Counter(key2.keyEvts)-Counter(P_star.keyEvts))).elements())\n",
    "        #print(f'cpunter 1{(Counter(key1.keyEvts)-Counter(P_star.keyEvts))}')\n",
    "        #print(f'cpunter 2{(Counter(key2.keyEvts)-Counter(P_star.keyEvts))}')\n",
    "        #print(f'cpunter together{(Counter(key1.keyEvts)-Counter(P_star.keyEvts))|(Counter(key2.keyEvts)-Counter(P_star.keyEvts))}')\n",
    "        #E_c= ((Counter(key1.keyEvts)-Counter(P_star.keyEvts))|(Counter(key2.keyEvts)-Counter(P_star.keyEvts))).elements()\n",
    "        #print(f'Counter {E_c}')\n",
    "        #sort Ec by frequency in desc order;\n",
    "        #E_c_counter = sort_by_frequency(val1, val2, E_c)\n",
    "        E_c_counter = Counter(E_c)\n",
    "        E_c_counter=sorted(E_c_counter, key=E_c_counter.get, reverse=True)\n",
    "        #print(f'Counter {E_c_counter}')\n",
    "        del_L=-1\n",
    "\n",
    "        #P_star= Pattern(P_star)\n",
    "        lcs_pos_p_i= Pattern.getPositions(P_star.keyEvts,key1.keyEvts )\n",
    "        lcs_pos_p_j= Pattern.getPositions(P_star.keyEvts,key2.keyEvts )\n",
    "\n",
    "        print(f'positions p_i {lcs_pos_p_i}')\n",
    "        print(f'positions p_j {lcs_pos_p_j}')\n",
    "\n",
    "        for e in E_c_counter:\n",
    "            #temp_P=P_star\n",
    "\n",
    "            print(f'candidate {e}')\n",
    "\n",
    "            indices_key1 = [i for i, x in enumerate(key1.keyEvts) if x == e]\n",
    "            indices_key2 = [i for i, x in enumerate(key2.keyEvts) if x == e]\n",
    "\n",
    "            #print(f'indices {indices_key1}')\n",
    "            #print(f'indices 2{indices_key2}')\n",
    "\n",
    "            #indices_key1.extend(indices_key2)\n",
    "            #print(f'all indices {indices_key1}')\n",
    "            candidate_pos=[]\n",
    "            for ind in indices_key1:\n",
    "                if e in P_star.keyEvts and ind in lcs_pos_p_i:\n",
    "                    continue\n",
    "\n",
    "                temp_P=P_star\n",
    "                index=bisect(lcs_pos_p_i, ind)\n",
    "                #print(f'ind {ind} bisect {index}')\n",
    "                candidate_pos.append(index)\n",
    "\n",
    "            for ind in indices_key2:\n",
    "                if e in P_star.keyEvts and ind in lcs_pos_p_j:\n",
    "                    continue\n",
    "\n",
    "                temp_P=P_star\n",
    "                index=bisect(lcs_pos_p_j, ind)\n",
    "                #print(f'ind {ind} bisect {index}')\n",
    "                candidate_pos.append(index)\n",
    "\n",
    "            candidate_pos= list(set(candidate_pos))\n",
    "\n",
    "            print(f'candidates {candidate_pos}')\n",
    "\n",
    "            for index in candidate_pos:\n",
    "                temp_P.keyEvts.insert(index,e)\n",
    "\n",
    "                #temp_P.addKeyEvent(e)\n",
    "                #print(f'temporary pattern {temp_P}')\n",
    "                del_L_prime= len(key1.keyEvts)+ len(key2.keyEvts)- len(temp_P.keyEvts)+lambda_val\n",
    "                del_L_prime+= sum(alpha*calc_dist(v1.getHashList(attr), key1.keyEvts) for v1 in val1)\n",
    "                del_L_prime+= sum(alpha*calc_dist(v2.getHashList(attr), key2.keyEvts) for v2 in val2) #alpha*calc_dist(val2, key2.keyEvts)\n",
    "                del_L_prime-= sum(alpha*calc_dist(v.getHashList(attr),temp_P.keyEvts) for v in val1+val2)\n",
    "\n",
    "                print(f'del L prime {del_L_prime}')\n",
    "                if(del_L_prime<0 or del_L_prime< del_L):\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    del_L= del_L_prime\n",
    "                    P_star=temp_P\n",
    "                print(f'del L  {del_L}')\n",
    "                print(f'P_star {P_star.keyEvts}')\n",
    "                del temp_P.keyEvts[index]\n",
    "\n",
    "\n",
    "            #return del_L, (P_star, list(set(val1) & set(val2)))\n",
    "            return del_L, (P_star, val1+val2)\n",
    "\n",
    "\n",
    "\n",
    "def lcs(a, b):\n",
    "    tbl = [[0 for _ in range(len(b) + 1)] for _ in range(len(a) + 1)]\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            tbl[i + 1][j + 1] = tbl[i][j] + 1 if x == y else max(\n",
    "                tbl[i + 1][j], tbl[i][j + 1])\n",
    "    res = []\n",
    "    i, j = len(a), len(b)\n",
    "    while i and j:\n",
    "        if tbl[i][j] == tbl[i - 1][j]:\n",
    "            i -= 1\n",
    "        elif tbl[i][j] == tbl[i][j - 1]:\n",
    "            j -= 1\n",
    "        else:\n",
    "            res.append(a[i - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return res[::-1]\n",
    "\n",
    "def sort_by_frequency(v1, v2, selectedevents):\n",
    "    counter=Counter()\n",
    "    for item in (v1,v2):\n",
    "        for seqs in item:\n",
    "            for event in seqs.events:\n",
    "                if event in selectedevents:\n",
    "                    counter[event]+=1\n",
    "    print(counter)\n",
    "    return sorted(counter, key=counter.get, reverse=True)\n",
    "    \n",
    "def levenshtein(s1, s2):\n",
    "    #From Wikipedia\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    #print(previous_row)\n",
    "    return previous_row[-1]    \n",
    "\n",
    "\n",
    "def calc_dist(seqs, pattern):\n",
    "    #total=0\n",
    "    #print(f'val seq {seqs}')\n",
    "    #print(f'val pattern {pattern}')\n",
    "    #total= sum(levenshtein(val.events, pattern) for val in seqs)\n",
    "    total= levenshtein(seqs, pattern)\n",
    "    #print(f'total {total}')\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe( src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            print(src)\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "\n",
    "    \n",
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../corelow_paper_test.csv\n",
      "dict_keys(['Event', 'Category'])\n"
     ]
    }
   ],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../corelow_paper_test.csv', 1, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "seq_list=sequence_braiding_Es.generateSequence(\"Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'K', '1': 'E', '2': 'H', '3': 'L', '4': 'D', '5': 'C', '6': 'J', '7': 'B', '8': 'A', '9': 'G', ':': 'F', ';': 'I'}\n"
     ]
    }
   ],
   "source": [
    "print(sequence_braiding_Es.reverseatttrdict['Event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial clusts\n",
      "------key val pairs----\n",
      "Pattern ['4', '1', '7', '7', '8']\n",
      "Sequences ['4', '1', '7', '7', '8']\n",
      "Pattern ['5', '6', '4', ':', '7', '9', '7']\n",
      "Sequences ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern ['8', '7', '5', '7', '5', '4', '1']\n",
      "Sequences ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern ['5', '8', '5', '5', '8']\n",
      "Sequences ['5', '8', '5', '5', '8']\n",
      "Pattern ['5', '3', ':', '9', '2']\n",
      "Sequences ['5', '3', ':', '9', '2']\n",
      "Pattern [';', '0', '8', '1']\n",
      "Sequences [';', '0', '8', '1']\n",
      "Pattern 1 ['4', '1', '7', '7', '8']\n",
      "Pattern 2 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern LCS ['4', '7', '7']\n",
      "positions p_i [0, 2, 3]\n",
      "positions p_j [2, 4, 6]\n",
      "candidate 1\n",
      "candidates [1]\n",
      "del L prime 3\n",
      "del L  3\n",
      "P_star ['4', '1', '7', '7']\n",
      "Pattern 1 ['4', '1', '7', '7', '8']\n",
      "Pattern 2 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern LCS ['4', '1']\n",
      "positions p_i [0, 1]\n",
      "positions p_j [5, 6]\n",
      "candidate 7\n",
      "candidates [0, 2]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['7', '4', '1']\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['4', '1', '7']\n",
      "Pattern 1 ['4', '1', '7', '7', '8']\n",
      "Pattern 2 ['5', '8', '5', '5', '8']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [4]\n",
      "positions p_j [1]\n",
      "candidate 5\n",
      "candidates [0, 1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '8']\n",
      "del L prime 0\n",
      "Pattern 1 ['4', '1', '7', '7', '8']\n",
      "Pattern 2 ['5', '3', ':', '9', '2']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7']\n",
      "Pattern 1 ['4', '1', '7', '7', '8']\n",
      "Pattern 2 [';', '0', '8', '1']\n",
      "Pattern LCS ['1']\n",
      "positions p_i [1]\n",
      "positions p_j [3]\n",
      "candidate 7\n",
      "candidates [1]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['1', '7']\n",
      "Pattern 1 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern 2 ['4', '1', '7', '7', '8']\n",
      "Pattern LCS ['4', '7', '7']\n",
      "positions p_i [2, 4, 6]\n",
      "positions p_j [0, 2, 3]\n",
      "candidate 5\n",
      "candidates [0]\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['5', '4', '7', '7']\n",
      "Pattern 1 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern 2 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern LCS ['5', '4']\n",
      "positions p_i [0, 2]\n",
      "positions p_j [2, 5]\n",
      "candidate 7\n",
      "candidates [0, 1, 2]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['7', '5', '4']\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['5', '7', '4']\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['5', '4', '7']\n",
      "Pattern 1 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern 2 ['5', '8', '5', '5', '8']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [0]\n",
      "candidate 7\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '7']\n",
      "Pattern 1 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern 2 ['5', '3', ':', '9', '2']\n",
      "Pattern LCS ['5', ':', '9']\n",
      "positions p_i [0, 3, 5]\n",
      "positions p_j [0, 2, 3]\n",
      "candidate 7\n",
      "candidates [2, 3]\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['5', ':', '7', '9']\n",
      "del L prime 3\n",
      "del L  3\n",
      "P_star ['5', ':', '9', '7']\n",
      "Pattern 1 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern 2 [';', '0', '8', '1']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7']\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 ['4', '1', '7', '7', '8']\n",
      "Pattern LCS ['7', '7']\n",
      "positions p_i [1, 3]\n",
      "positions p_j [2, 3]\n",
      "candidate 5\n",
      "candidates [1, 2]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['7', '5', '7']\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['7', '7', '5']\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern LCS ['5', '7']\n",
      "positions p_i [2, 3]\n",
      "positions p_j [0, 4]\n",
      "candidate 8\n",
      "candidates [0]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['8', '5', '7']\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 ['5', '8', '5', '5', '8']\n",
      "Pattern LCS ['8', '5', '5']\n",
      "positions p_i [0, 2, 4]\n",
      "positions p_j [1, 2, 3]\n",
      "candidate 7\n",
      "candidates [1, 2]\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['8', '7', '5', '5']\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['8', '5', '7', '5']\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 ['5', '3', ':', '9', '2']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [2]\n",
      "positions p_j [0]\n",
      "candidate 7\n",
      "candidates [0, 1]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7', '5']\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '7']\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 [';', '0', '8', '1']\n",
      "Pattern LCS ['8', '1']\n",
      "positions p_i [0, 6]\n",
      "positions p_j [2, 3]\n",
      "candidate 7\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['8', '7', '1']\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 ['4', '1', '7', '7', '8']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [1]\n",
      "positions p_j [4]\n",
      "candidate 5\n",
      "candidates [0, 1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '8']\n",
      "del L prime 0\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [0]\n",
      "candidate 5\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '5']\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern LCS ['8', '5', '5']\n",
      "positions p_i [1, 2, 3]\n",
      "positions p_j [0, 2, 4]\n",
      "candidate 7\n",
      "candidates [1, 2]\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['8', '7', '5', '5']\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['8', '5', '7', '5']\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 ['5', '3', ':', '9', '2']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [0]\n",
      "candidate 5\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '5']\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 [';', '0', '8', '1']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [1]\n",
      "positions p_j [2]\n",
      "candidate 5\n",
      "candidates [0, 1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '8']\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['8', '5']\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['4', '1', '7', '7', '8']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7']\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern LCS ['5', ':', '9']\n",
      "positions p_i [0, 2, 3]\n",
      "positions p_j [0, 3, 5]\n",
      "candidate 7\n",
      "candidates [2, 3]\n",
      "del L prime 2\n",
      "del L  2\n",
      "P_star ['5', ':', '7', '9']\n",
      "del L prime 3\n",
      "del L  3\n",
      "P_star ['5', ':', '9', '7']\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [2]\n",
      "candidate 7\n",
      "candidates [0, 1]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7', '5']\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '7']\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['5', '8', '5', '5', '8']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [0]\n",
      "candidate 5\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '5']\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 [';', '0', '8', '1']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 5\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['5']\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['4', '1', '7', '7', '8']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [2]\n",
      "positions p_j [4]\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['7', '8']\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7']\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern LCS ['8', '1']\n",
      "positions p_i [2, 3]\n",
      "positions p_j [0, 6]\n",
      "candidate 7\n",
      "candidates [1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['8', '7', '1']\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['5', '8', '5', '5', '8']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [2]\n",
      "positions p_j [1]\n",
      "candidate 5\n",
      "candidates [0, 1]\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['5', '8']\n",
      "del L prime 1\n",
      "del L  1\n",
      "P_star ['8', '5']\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['5', '3', ':', '9', '2']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate ;\n",
      "candidates [0]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star [';']\n",
      "------key val pairs----\n",
      "Pattern ['4', '1', '7', '7', '8']\n",
      "Sequences ['4', '1', '7', '7', '8']\n",
      "Pattern ['5', '6', '4', ':', '7', '9', '7']\n",
      "Sequences ['5', '6', '4', ':', '7', '9', '7']\n",
      "Pattern ['8', '7', '5', '7', '5', '4', '1']\n",
      "Sequences ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern ['5', '8', '5', '5', '8']\n",
      "Sequences ['5', '8', '5', '5', '8']\n",
      "Pattern ['5', '3', ':', '9', '2']\n",
      "Sequences ['5', '3', ':', '9', '2']\n",
      "Pattern [';', '0', '8', '1']\n",
      "Sequences [';', '0', '8', '1']\n",
      "c_i: (<__main__.Pattern object at 0x7ff1e38e3990>, [<__main__.Sequence object at 0x7ff1e388b050>])\n",
      "c_j: (<__main__.Pattern object at 0x7ff1e38e3650>, [<__main__.Sequence object at 0x7ff1e388b590>])\n",
      "Pattern 1 ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern 2 ['4', '7', '7']\n",
      "Pattern LCS ['7', '7']\n",
      "positions p_i [1, 3]\n",
      "positions p_j [1, 2]\n",
      "candidate 5\n",
      "candidates [1, 2]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7', '5', '7']\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['7', '7', '5']\n",
      "Pattern 1 ['5', '8', '5', '5', '8']\n",
      "Pattern 2 ['4', '7', '7']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 5\n",
      "candidates [0]\n",
      "del L prime -2\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['4', '7', '7']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime -2\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['4', '7', '7']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime -2\n",
      "------key val pairs----\n",
      "Pattern ['8', '7', '5', '7', '5', '4', '1']\n",
      "Sequences ['8', '7', '5', '7', '5', '4', '1']\n",
      "Pattern ['5', '8', '5', '5', '8']\n",
      "Sequences ['5', '8', '5', '5', '8']\n",
      "Pattern ['5', '3', ':', '9', '2']\n",
      "Sequences ['5', '3', ':', '9', '2']\n",
      "Pattern [';', '0', '8', '1']\n",
      "Sequences [';', '0', '8', '1']\n",
      "Pattern ['4', '7', '7']\n",
      "Sequences ['4', '1', '7', '7', '8']\n",
      "Sequences ['5', '6', '4', ':', '7', '9', '7']\n",
      "c_i: (<__main__.Pattern object at 0x7ff1e38e3450>, [<__main__.Sequence object at 0x7ff1e388bc90>])\n",
      "c_j: (<__main__.Pattern object at 0x7ff1e38e3710>, [<__main__.Sequence object at 0x7ff1e388bfd0>])\n",
      "Pattern 1 ['5', '3', ':', '9', '2']\n",
      "Pattern 2 ['8', '5', '5']\n",
      "Pattern LCS ['5']\n",
      "positions p_i [0]\n",
      "positions p_j [1]\n",
      "candidate 3\n",
      "candidates [1]\n",
      "del L prime -1\n",
      "Pattern 1 [';', '0', '8', '1']\n",
      "Pattern 2 ['8', '5', '5']\n",
      "Pattern LCS ['8']\n",
      "positions p_i [2]\n",
      "positions p_j [0]\n",
      "candidate 5\n",
      "candidates [1]\n",
      "del L prime 0\n",
      "del L  0\n",
      "P_star ['8', '5']\n",
      "Pattern 1 ['4', '7', '7']\n",
      "Pattern 2 ['8', '5', '5']\n",
      "Pattern LCS []\n",
      "positions p_i []\n",
      "positions p_j []\n",
      "candidate 7\n",
      "candidates [0]\n",
      "del L prime -4\n"
     ]
    }
   ],
   "source": [
    "syn=SequenceSynopsis()\n",
    "G=syn.minDL(seq_list, \"Event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys['5', '3', ':', '9', '2']\n",
      "5-3-:-9-2\n",
      "CLFGH\n",
      "['C', 'L', 'F', 'G', 'H']\n",
      "keys[';', '0', '8', '1']\n",
      ";-0-8-1\n",
      "IKAE\n",
      "['I', 'K', 'A', 'E']\n",
      "keys['4', '7', '7']\n",
      "4-7\n",
      "DBB\n",
      "['D', 'E', 'B', 'B', 'A']\n",
      "['C', 'J', 'D', 'F', 'B', 'G', 'B']\n",
      "keys['8', '5', '5']\n",
      "8-5\n",
      "ACC\n",
      "['A', 'B', 'C', 'B', 'C', 'D', 'E']\n",
      "['C', 'A', 'C', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "for key, value in G.items():\n",
    "    print(f'keys{key.keyEvts}')\n",
    "    print(key.getUniqueEventsString())\n",
    "    #print(key.getEvtAttrValue(sequence_braiding_Es,\"Event\"))\n",
    "    print(sequence_braiding_Es.get_event_name(\"Event\",key.keyEvts))\n",
    "    for v in value:\n",
    "        #print(v.getHashList(\"Event\"))\n",
    "        print(v.getEvtAttrValues(\"Event\"))\n",
    "\n",
    "#for key, value in G.items():\n",
    "#    print(key.keyEvts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_default_dump(obj)-> dict:\n",
    "        return {\n",
    "            \"Pattern\": [x.keyEvts for x in obj.keys()],\n",
    "            \"links\":obj.values()\n",
    "\n",
    "        }\n",
    "\n",
    "def json_serialize_dump(obj):\n",
    "\n",
    "    if hasattr(obj, \"json_default_dump\"):\n",
    "\n",
    "        return obj.json_default_dump()\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    return None #obj.__dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{<__main__.Pattern object at 0x7ff1e38e3250>: [<__main__.Sequence object at 0x7ff1e388b090>], <__main__.Pattern object at 0x7ff1e38e3050>: [<__main__.Sequence object at 0x7ff1e388b790>], <__main__.Pattern object at 0x7ff1e3921490>: [<__main__.Sequence object at 0x7ff1e388b050>, <__main__.Sequence object at 0x7ff1e388b590>], <__main__.Pattern object at 0x7ff1e3921ad0>: [<__main__.Sequence object at 0x7ff1e388bc90>, <__main__.Sequence object at 0x7ff1e388bfd0>]}\n",
      "Here\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b090>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b090>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b790>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b790>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b050>, <__main__.Sequence object at 0x7ff1e388b590>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b050>\n",
      "here\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b590>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388bc90>, <__main__.Sequence object at 0x7ff1e388bfd0>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388bc90>\n",
      "here\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388bfd0>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'dict'>\n",
      "{<__main__.Pattern object at 0x7ff1e38e3250>: [<__main__.Sequence object at 0x7ff1e388b090>], <__main__.Pattern object at 0x7ff1e38e3050>: [<__main__.Sequence object at 0x7ff1e388b790>], <__main__.Pattern object at 0x7ff1e3921490>: [<__main__.Sequence object at 0x7ff1e388b050>, <__main__.Sequence object at 0x7ff1e388b590>], <__main__.Pattern object at 0x7ff1e3921ad0>: [<__main__.Sequence object at 0x7ff1e388bc90>, <__main__.Sequence object at 0x7ff1e388bfd0>]}\n",
      "Here\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b090>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b090>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b790>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b790>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388b050>, <__main__.Sequence object at 0x7ff1e388b590>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b050>\n",
      "here\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388b590>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n",
      "<class 'list'>\n",
      "[<__main__.Sequence object at 0x7ff1e388bc90>, <__main__.Sequence object at 0x7ff1e388bfd0>]\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388bc90>\n",
      "here\n",
      "<class '__main__.Sequence'>\n",
      "<__main__.Sequence object at 0x7ff1e388bfd0>\n",
      "here\n",
      "<class '__main__.Pattern'>\n",
      "X\n"
     ]
    }
   ],
   "source": [
    "def key_to_json(data):\n",
    "    print(type(data))\n",
    "    if data is None or isinstance(data, (bool, int, str)):\n",
    "        return data\n",
    "    if isinstance(data, Pattern):\n",
    "        print(\"X\")\n",
    "        return \"-\".join(str(x) for x in data.keyEvts)\n",
    "    raise TypeError\n",
    "\n",
    "def to_json(data):\n",
    "    print(type(data))\n",
    "    print(data)\n",
    "    if data is None or isinstance(data, (bool, int, tuple, range, str)):\n",
    "        return data\n",
    "    if isinstance(data, (set, frozenset)):\n",
    "        return sorted(data)\n",
    "    if isinstance(data, Sequence):\n",
    "        print(\"here\")\n",
    "        return data.getEventsHashString(\"Event\")\n",
    "    if isinstance(data, list):\n",
    "        return [to_json(x) for x in data]\n",
    "    if isinstance(data, dict):\n",
    "        print(\"Here\")\n",
    "        return {key_to_json(key): to_json(data[key]) for key in data}\n",
    "    raise TypeError\n",
    "\n",
    "data = G\n",
    "json.dumps(to_json(data))\n",
    "with open('outfile_sequence_synopsis.json', 'w') as the_file2:\n",
    "        the_file2.write(json.dumps(to_json(data), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not Pattern",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-582c395f9612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mjson_serialize_dump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 raise TypeError(f'keys must be str, int, float, bool or None, '\n\u001b[0m\u001b[1;32m    377\u001b[0m                                 f'not {key.__class__.__name__}')\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not Pattern"
     ]
    }
   ],
   "source": [
    "x=json.dumps(G, ensure_ascii=False, default= json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=lcs([1,2,4,5],[1,4,6,7])\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=levenshtein([1,2,4,5],[1,4,6,7])\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=minDL(seq_list, 'Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in G.items():\n",
    "    print(f'keys{key.keyEvts}')\n",
    "    for v in value:\n",
    "        #print(v.getHashList(\"Meal\"))\n",
    "        print(v.getEvtAttrValues(\"Meal\"))\n",
    "\n",
    "#for key, value in G.items():\n",
    "#    print(key.keyEvts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'ac': 1, 'dc': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in a:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
