{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf\n",
    "import json\n",
    "import jsonpickle\n",
    "import heapq\n",
    "\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    \"\"\"Base Event class, holds the types.\"\"\"\n",
    "\n",
    "    def __init__(self, eventtype):\n",
    "        self.type = eventtype\n",
    "        self.attributes = {}\n",
    "\n",
    "    def addAttribute(self, attr, value):\n",
    "        \"\"\"Add attributes to the Event object.\"\"\"\n",
    "        self.attributes[attr] = value\n",
    "\n",
    "    def getAttrVal(self, attrName):\n",
    "        \"\"\"Return Attribute value given attribute name.\"\"\"\n",
    "        return self.attributes.get(attrName, None)\n",
    "\n",
    "\n",
    "class PointEvent(Event):\n",
    "    \"\"\"Derivative class for Point events\"\"\"\n",
    "\n",
    "    def __init__(self, timestamp):\n",
    "        Event.__init__(self, \"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "\n",
    "class IntervalEvent(Event):\n",
    "    \"\"\"Derivative class for interval events.\"\"\"\n",
    "\n",
    "    def __init__(self, t1, t2):\n",
    "        Event.__init__(self, \"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1, t2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    \"\"\"EventStore class holds all tevents present in the dataset. Also creates a\n",
    "    dictionary of event attribute to unicode mapping and the reverse mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eventlist=None):\n",
    "        if eventlist is None:\n",
    "            eventlist = []\n",
    "        self.attrDict = {}\n",
    "        self.reverseAttrDict = {}\n",
    "        self.events = eventlist\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot\n",
    "    #  auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the dataFrame and\n",
    "    # then use that instead of calling my helper\n",
    "    # for those cases\n",
    "\n",
    "    def importPointEvents(self, src, timestampColumnIdx, timeFormat,\n",
    "                          sep='\\t', local=False, header=None, dataFrame=None):\n",
    "        \"\"\" Returns a list of event objects\n",
    "        src is a url or directory path, if local is false its url else its path\n",
    "        header is list of column names if they are not provided in the dataset.\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        # if the dataFrame is not provided\n",
    "        if dataFrame is None:\n",
    "            dataFrame = getDataframe(src, local, sep, header)\n",
    "        cols = dataFrame.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in dataFrame.iterrows():\n",
    "            data = row[1]\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            evt = PointEvent(timestamp)\n",
    "            for i, _ in enumerate(data):\n",
    "                if i != timestampColumnIdx:\n",
    "                    evt.addAttribute(cols[i], data[i])\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            events.append(evt)\n",
    "        self.events = events\n",
    "        # sequence=Sequence(events)\n",
    "        self.createAttrDict()\n",
    "        # return sequence\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas\n",
    "    # cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function\n",
    "    # the dataFrame and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "\n",
    "    def importIntervalEvents(self, src, startTimeColumnIdx, endTimeColumnIdx,\n",
    "                             timeFormat, sep=\"\\t\", local=False, header=None, dataFrame=None):\n",
    "        \"\"\"Returns a list of event objects\n",
    "        src is a url or directory path, if local is false its url else its path.\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        # if the dataFrame is not provided\n",
    "        if dataFrame is None:\n",
    "            dataFrame = getDataframe(src, local, sep, header)\n",
    "        cols = dataFrame.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in dataFrame.iterrows():\n",
    "            data = row[1]\n",
    "            # create datetime object for the start and end times of the event\n",
    "            timestamp1 = datetime.strptime(\n",
    "                data[startTimeColumnIdx], timeFormat)\n",
    "            timestamp2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            evt = IntervalEvent(timestamp1, timestamp2)\n",
    "            for i, _ in enumerate(data):\n",
    "                if i not in (startTimeColumnIdx, endTimeColumnIdx):\n",
    "                    evt.addAttribute(cols[i], data[i])\n",
    "                    #attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            events.append(evt)\n",
    "        self.events = events\n",
    "        # sequence=Sequence(events)\n",
    "        self.createAttrDict()\n",
    "        # return sequence\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas\n",
    "    # cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the dataFrame and then\n",
    "    # use that instead of calling my helper\n",
    "\n",
    "    def importMixedEvents(self, src, startTimeColumnIdx, endTimeColumnIdx,\n",
    "                          timeFormat, sep=\"\\t\", local=False, header=None, dataFrame=None):\n",
    "        \"\"\"Import a dataset that has both interval and point events\n",
    "        Returns a list of event objects\n",
    "        src is a url or directory path, if local is false its url else its path.\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        # if the dataFrame is not provided\n",
    "        if dataFrame is None:\n",
    "            dataFrame = getDataframe(src, local, sep, header)\n",
    "        cols = dataFrame.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in dataFrame.iterrows():\n",
    "            data = row[1]\n",
    "            # create datetime object for timestamp (if point events)\n",
    "            # or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time\n",
    "            # string then its a point event\n",
    "            # if isinstance(data[endTimeColumnIdx], float):\n",
    "            if data[endTimeColumnIdx] is None or isinstance(data[endTimeColumnIdx], float):\n",
    "                timeStamp = datetime.strptime(\n",
    "                    data[startTimeColumnIdx], timeFormat)\n",
    "                eventType = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                timeStamp1 = datetime.strptime(\n",
    "                    data[startTimeColumnIdx], timeFormat)\n",
    "                timeStamp2 = datetime.strptime(\n",
    "                    data[endTimeColumnIdx], timeFormat)\n",
    "                eventType = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            # list of indices to be ignored\n",
    "            ignore = [startTimeColumnIdx, endTimeColumnIdx]\n",
    "            attributeColumns = [ind for ind in range(\n",
    "                len(data)) if ind not in ignore]\n",
    "            if eventType == \"point\":\n",
    "                evt = PointEvent(timeStamp)\n",
    "            else:\n",
    "                evt = IntervalEvent(timeStamp1, timeStamp2)\n",
    "            for i in attributeColumns:\n",
    "                evt.addAttribute(cols[i], data[i])\n",
    "                #attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            events.append(evt)\n",
    "        self.events = events\n",
    "        # sequence=Sequence(events)\n",
    "        self.createAttrDict()\n",
    "        # return sequence\n",
    "\n",
    "    def generateSequence(self, attributeName):\n",
    "        \"\"\"Group events by attributeName, and order them by timestamp\n",
    "        returns a list of sequences.\n",
    "        \"\"\"\n",
    "        eventList = self.events\n",
    "        groupedBy = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=getTimeToSortBy)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in groupedBy for value\n",
    "            if value in groupedBy:\n",
    "                groupedBy[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                groupedBy[value] = [event]\n",
    "        sequences = list(groupedBy.values())\n",
    "        seqlist = []\n",
    "        for seq in sequences:\n",
    "            seqlist.append(Sequence(seq, self))\n",
    "        return seqlist\n",
    "\n",
    "    def getUniqueValues(self, attr):\n",
    "        \"\"\"returns the unique values of a certain attribute\n",
    "         present in the dataset.\n",
    "         \"\"\"\n",
    "        uniqVals = list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return uniqVals\n",
    "\n",
    "    def getEventValue(self, attr, hashlist):\n",
    "        \"\"\"Given a list of hash values, return the original value of event.\"\"\"\n",
    "        return [self.reverseAttrDict[attr][val] for val in hashlist]\n",
    "\n",
    "    def createAttrDict(self):\n",
    "        \"\"\" Assuming we are given a list of events and from those events we create\n",
    "        the mapping and reverse mapping dictionary.\n",
    "        \"\"\"\n",
    "        attrList = self.events[0].attributes.keys()\n",
    "        print(attrList)\n",
    "\n",
    "        for attr in attrList:\n",
    "            unicode = 48\n",
    "            uniqueList = []\n",
    "            uniqueList.extend(self.getUniqueValues(attr))\n",
    "            uniqueList = list(set(uniqueList))\n",
    "            # uniqueList.clear()\n",
    "\n",
    "            unicodeDict = {}\n",
    "            reverseDict = {}\n",
    "            for uniques in uniqueList:\n",
    "                unicodeDict[uniques] = chr(unicode)\n",
    "                reverseDict[chr(unicode)] = uniques\n",
    "                unicode = unicode+1\n",
    "            self.attrDict[attr] = unicodeDict\n",
    "            self.reverseAttrDict[attr] = reverseDict\n",
    "            # unicodeDict.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence():\n",
    "    \"\"\"Collection of events sharing similar property.\"\"\"\n",
    "\n",
    "    _ids = count(0)\n",
    "\n",
    "    def __init__(self, eventlist, eventstore, sid=None):\n",
    "        # sequence id\n",
    "        if sid is None:\n",
    "            self.sid = self  # next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "\n",
    "        self.events = eventlist\n",
    "        self.eventstore = eventstore\n",
    "        self.volume = 1\n",
    "        self.seqAttributes = {}\n",
    "        self.seqIndices = []\n",
    "\n",
    "    def getEventPosition(self, attr, hashVal):\n",
    "        \"\"\"Returns the position of first event where the value of attr matches the\n",
    "        given hash value.\n",
    "        \"\"\"\n",
    "        for pos, event in enumerate(self.events):\n",
    "            # if event.getAttrVal(attr)==hashVal:\n",
    "            if self.eventstore.attrDict[attr][event.getAttrVal(attr)] == hashVal:\n",
    "                return pos\n",
    "        return -1\n",
    "\n",
    "    def setVolume(self, intValue):\n",
    "        \"\"\"Assigns the volume value to intValue.\"\"\"\n",
    "        self.volume = intValue\n",
    "\n",
    "    def getVolume(self):\n",
    "        \"\"\"Returns the volume value for this object.\"\"\"\n",
    "        return self.volume\n",
    "\n",
    "    def increaseVolume(self):\n",
    "        \"\"\"Increases volume value by 1.\"\"\"\n",
    "        self.volume += 1\n",
    "\n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        \"\"\"Returns Hash values for unique attribute types for the specified attribute\n",
    "         in the dictionary\n",
    "         \"\"\"\n",
    "        lst = list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        uniquelist = [self.eventstore.attrDict[attr][elem] for elem in lst]\n",
    "        return uniquelist\n",
    "\n",
    "    # Not sure this will always result in same index, will change if\n",
    "    #dictionary is updated\n",
    "    # since python is unordered\n",
    "\n",
    "    def getHashList(self, attr):\n",
    "        \"\"\"\"Returns a list of index positions of the specified attribute for all\n",
    "        the events  in the sequence\n",
    "        \"\"\"\n",
    "        #lst=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        lst = [event.getAttrVal(attr) for event in self.events]\n",
    "        hashlist = [self.eventstore.attrDict[attr][elem] for elem in lst]\n",
    "\n",
    "        return hashlist\n",
    "\n",
    "    def getValueHashes(self, attr):\n",
    "        \"\"\"Returns a list of values of the specified attribute for all the\n",
    "        events in the sequence\n",
    "        \"\"\"\n",
    "        lst = list(event.getAttrVal(attr) for event in self.events)\n",
    "        hashlist = [self.eventstore.attrDict[attr][elem] for elem in lst]\n",
    "\n",
    "        return hashlist\n",
    "\n",
    "    def getEventsHashString(self, attr):\n",
    "        \"\"\"Returns a string containing a list of values of the specified attribute\n",
    "        for all the events in the sequence\n",
    "        \"\"\"\n",
    "        string = attr + \": \"\n",
    "        lst = list(event.getAttrVal(attr) for event in self.events)\n",
    "        # for count,event in enumerate(self.events):\n",
    "        #    string+=str(event.getAttrVal(attr))+\" \"\n",
    "        string += \"\".join(str(self.eventstore.attrDict[attr][elem])\n",
    "                          for elem in lst)\n",
    "        return string\n",
    "\n",
    "    def convertToVMSPReadablenum(self, attr):\n",
    "        \"\"\"Returns a VMSP readable string of numbers containing a list of values of\n",
    "        the specified attribute for all the events in the sequence\n",
    "        \"\"\"\n",
    "        lst = list(event.getAttrVal(attr) for event in self.events)\n",
    "        string = \" -1 \".join(str(self.eventstore.attrDict[attr][elem])\n",
    "                             for elem in lst)\n",
    "        # string=\"\"\n",
    "        # for count,event in enumerate(self.events):\n",
    "        #    string+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        string += \" -2\"\n",
    "\n",
    "        return string\n",
    "\n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        \"\"\"Returns a VMSP readable string containing a list of values of\n",
    "        the specified attribute for all the events in the sequence\n",
    "        \"\"\"\n",
    "        lst = list(event.getAttrVal(attr) for event in self.events)\n",
    "        string = \" \".join(self.eventstore.attrDict[attr][elem] for elem in lst)\n",
    "        # string=\"\"\n",
    "        # for count,event in enumerate(self.events):\n",
    "        #    string+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        string += \".\"\n",
    "\n",
    "        return string\n",
    "\n",
    "    def getPathID(self):\n",
    "        \"\"\"Returns the sequence ID value for this object.\"\"\"\n",
    "        return self.sid\n",
    "\n",
    "    def matchPathAttribute(self, attr, val):\n",
    "        \"\"\"Returns True if the value for specified sequence attribute matches the specified value\"\"\"\n",
    "        # should i use eq?!\n",
    "        return bool(self.seqAttributes.get(attr) == (val))\n",
    "\n",
    "    def setSequenceAttribute(self, attr, value):\n",
    "        \"\"\"Assigns the value of the specified attribute to the specified Value\"\"\"\n",
    "        self.seqAttributes[attr] = value\n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)\n",
    "    @staticmethod\n",
    "    def getSeqVolume(seqlist):\n",
    "        \"\"\"Return aggregated value of total volume of all the sequences\n",
    "        in the given List of Sequences.\n",
    "        \"\"\"\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "\n",
    "    @staticmethod\n",
    "    def getUniqueEvents(seqlist, attr):\n",
    "        \"\"\"Get all possible event types\"\"\"\n",
    "        # return self.eventstore.reverseAttrDict[attr].values()\n",
    "        return list(set(event.getAttrVal(attr) for event in seq for seq in seqlist))\n",
    "\n",
    "    # Method equivalent to public String getEvtAttrValue(String attr, int hash) in DataManager.java\n",
    "\n",
    "    def getEvtAttrValue(self, attr, hashVal):\n",
    "        \"\"\"Given hashVal, return original value for the specified attr\"\"\"\n",
    "        return self.eventstore.reverseAttrDict[attr][hashVal]\n",
    "\n",
    "    # Method equivalent to public List<String> getEvtAttrValues(String attr) in DataManager.java\n",
    "    def getEvtAttrValues(self, attr):\n",
    "        \"\"\"Given attr name, return all possible values for that attribute\"\"\"\n",
    "        return [event.getAttrVal(attr) for event in self.events]\n",
    "\n",
    "    # Method equivalent to int getEvtAttrValueCount(String attr) in DataManager.java\n",
    "    def getEvtAttrValueCount(self, attr):\n",
    "        \"\"\"return the number of distinct types present given an attribute\"\"\"\n",
    "        return len(self.eventstore.reverseAttrDict[attr])\n",
    "\n",
    "    def getEventsString(self, attr):\n",
    "        \"\"\"Convert the sequence events to a string.\"\"\"\n",
    "        return \" \".join(elem for elem in self.getEvtAttrValues(attr))\n",
    "\n",
    "    #ZINAT- changes\n",
    "    # SequenceList represents a list of objects of type Sequence.\n",
    "    # The sequences are further splitted into\n",
    "    # sequence objects, this way we can use generate sequences and then splitSequences\n",
    "    @staticmethod\n",
    "    def splitSequences(sequenceLists, timeUnit, record=None):\n",
    "        \"\"\"Split a long sequence into shorter ones by timeUnit. For example, a sequence\n",
    "        may span several days and we want to break it down into daily sequences. The argument\n",
    "        timeUnit can be one of the following strings: “hour”, “day”, “week”, “month”, “quarter”,\n",
    "        and “year”. For interval events the start time of the event  is used to determine its\n",
    "        category when splitting it\n",
    "        \"\"\"\n",
    "        if not isinstance(sequenceLists, list):\n",
    "            sequenceLists = [sequenceLists]\n",
    "        eventstore = sequenceLists[0].eventstore\n",
    "        results = []\n",
    "        resultlist = []\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        validTimeUnits = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in validTimeUnits:\n",
    "            raise ValueError(\n",
    "                \"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "\n",
    "        for sequence in sequenceLists:\n",
    "            # Sort the events by the timestamp or event start time\n",
    "            sequenceList = sequence.events\n",
    "            sequenceList = sorted(sequenceList, key=getTimeToSortBy)\n",
    "\n",
    "            # Process the event sequence based on the given time unit\n",
    "            # Generally, create a map for that time unit and then add each event into that map\n",
    "            # (key=time such as May 2021 in case of month, value=sequence) and then return the\n",
    "            # values of the map as a list\n",
    "            if timeUnit == \"hour\":\n",
    "                hours = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    key = (time.hour, time.day, time.month, time.year)\n",
    "                    insertEventIntoDict(key, hours, event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = ' '.join(\n",
    "                            [str(k) for k in key])\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "                results = list(hours.values())\n",
    "\n",
    "            elif timeUnit == \"day\":\n",
    "                days = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    key = (time.day, time.month, time.year)\n",
    "                    insertEventIntoDict(key, days, event)\n",
    "                    # print(days)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = datetime(\n",
    "                            *(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                results = list(days.values())\n",
    "\n",
    "            elif timeUnit == \"month\":\n",
    "                months = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    key = (time.month, time.year)\n",
    "                    insertEventIntoDict(key, months, event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(months.values())\n",
    "\n",
    "            elif timeUnit == \"week\":\n",
    "                weeks = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    year = time.year\n",
    "                    weekNum = time.isocalendar()[1]\n",
    "                    key = (year, weekNum)\n",
    "                    insertEventIntoDict(key, weeks, event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = str(\n",
    "                            key[0])+\"W\"+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "                results = list(weeks.values())\n",
    "\n",
    "            elif timeUnit == \"year\":\n",
    "                years = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    key = time.year\n",
    "                    insertEventIntoDict(key, years, event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = str(key)\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+str(key)\n",
    "                results = list(years.values())\n",
    "\n",
    "            elif timeUnit == \"quarter\":\n",
    "                quarters = {}\n",
    "                for event in sequenceList:\n",
    "                    time = getTimeToSortBy(event)\n",
    "                    year = time.year\n",
    "                    month = time.month\n",
    "                    # Determine the year, quarter pair/key for quarter dict\n",
    "                    # January, February, and March (Q1)\n",
    "                    if month in range(1, 4):\n",
    "                        key = (year, \"Q1\")\n",
    "                    # April, May, and June (Q2)\n",
    "                    elif month in range(4, 7):\n",
    "                        key = (year, \"Q2\")\n",
    "                    # July, August, and September (Q3)\n",
    "                    elif month in range(7, 10):\n",
    "                        key = (year, \"Q3\")\n",
    "                    # October, November, and December (Q4)\n",
    "                    elif month in range(10, 13):\n",
    "                        key = (year, \"Q4\")\n",
    "                    # Put the event in the dictionary\n",
    "                    insertEventIntoDict(key, quarters, event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"] = str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record] = str(\n",
    "                            event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(quarters.values())\n",
    "            resultlist.extend(results)\n",
    "        resultlists = [Sequence(x, eventstore) for x in resultlist]\n",
    "\n",
    "        return resultlists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"Base Node class holds information of the branching patterns in sequences\"\"\"\n",
    "\n",
    "    nodeCounter = count(1)\n",
    "    nodeHash = {}\n",
    "\n",
    "    def __init__(self, name=\"\", count_=0, value=\"\", attr=\"\"):\n",
    "        super().__init__()\n",
    "        self.nid = next(self.nodeCounter)\n",
    "        self.name = name\n",
    "        self.seqCount = count_\n",
    "        # What's the difference between name and value?\n",
    "        self.value = value\n",
    "        self.hash = -1\n",
    "        self.pos = []\n",
    "        self.meanStep = 0\n",
    "        self.medianStep = 0\n",
    "        # self.zipCompressRatio=0\n",
    "        self.incomingBranchUniqueEvts = None\n",
    "        # self.incomingBranchSimMean=None\n",
    "        # self.incomingBranchSimMedian=None\n",
    "        # self.incomingBranchSimVariance=None\n",
    "        self.keyevts = []\n",
    "        self.sequences = []\n",
    "        self.incomingSequences = []\n",
    "        self.outgoingSequences = []\n",
    "\n",
    "        self.meanRelTimestamp = 0\n",
    "        self.medianRelTimestamp = 0\n",
    "\n",
    "        self.attr = attr\n",
    "\n",
    "        TreeNode.nodeHash[self.nid] = self\n",
    "\n",
    "    def getNode(self, nodeId):\n",
    "        \"\"\"Returns the node for given node if in nodeHash table.\"\"\"\n",
    "        return self.nodeHash[nodeId]\n",
    "\n",
    "    def clearHash(self):\n",
    "        \"\"\"Clears the nodeHash dictionary\"\"\"\n",
    "        self.nodeHash.clear()\n",
    "\n",
    "    def getIncomingSequences(self):\n",
    "        \"\"\"Returns the list of incoming sequences\"\"\"\n",
    "        return self.incomingSequences\n",
    "\n",
    "    def getSeqCount(self):\n",
    "        \"\"\"returns the sequence count\"\"\"\n",
    "        return self.seqCount\n",
    "\n",
    "    def setSeqCount(self, seqCount):\n",
    "        \"\"\"Assigns the sequence coun\"\"\"\n",
    "        self.seqCount = seqCount\n",
    "\n",
    "    def getName(self):\n",
    "        \"\"\"Returns name of the node\"\"\"\n",
    "        return self.name\n",
    "\n",
    "    def setName(self, name):\n",
    "        \"\"\"Assigns name of the node\"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def getMeanStep(self):\n",
    "        \"\"\"Returns the value of meanStep\"\"\"\n",
    "        return self.meanStep\n",
    "\n",
    "    # need a better implementation\n",
    "    def toJSONObject(self):\n",
    "        \"\"\"converts the node to siple JSON object\"\"\"\n",
    "        # ,sort_keys=True, indent=4)\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)\n",
    "\n",
    "    def toString(self):\n",
    "        \"\"\"Returns name and seqCount for the node\"\"\"\n",
    "        return self.name+\": \"+self.seqCount\n",
    "\n",
    "    def setPositions(self, lst):\n",
    "        \"\"\"set meanStep and medianStep\"\"\"\n",
    "        self.pos = lst\n",
    "        print(f'positions {self.pos}')\n",
    "        self.pos.sort()\n",
    "        sumVal = sum(self.pos)+len(self.pos)\n",
    "        #mid = len(self.pos)/2\n",
    "\n",
    "        if len(self.pos) == 0:\n",
    "            self.meanStep = 0\n",
    "            self.medianStep = 0\n",
    "        else:\n",
    "            # WHY WE ARE ADDING 1 to mean and medianStep?\n",
    "            self.meanStep = sumVal/(len(self.pos))-1\n",
    "            # ((self.pos[mid-1]+self.pos[mid])/2.0)+1 if len(self.pos)%2==0 else self.pos[mid]+1\n",
    "            self.medianStep = np.median(self.pos)\n",
    "\n",
    "    def getValue(self):\n",
    "        \"\"\"Returns value of the node.\"\"\"\n",
    "        return self.value\n",
    "\n",
    "    def setValue(self, value):\n",
    "        \"\"\"Assigns value to the node.\"\"\"\n",
    "        self.value = value\n",
    "\n",
    "    def getMedianStep(self):\n",
    "        \"\"\"Returns medianStep of the node\"\"\"\n",
    "        return self.medianStep\n",
    "\n",
    "    # def getZipCompressRatio(self):\n",
    "    #    return self.zipCompressRatio\n",
    "\n",
    "    # def setZipCompressRatio(self, zipcompressratio):\n",
    "    #    self.zipCompressRatio=zipcompressratio\n",
    "\n",
    "    def getIncomingBranchUniqueEvts(self):\n",
    "        \"\"\"returns Unique events for the incoming branch\"\"\"\n",
    "        return self.incomingBranchUniqueEvts\n",
    "\n",
    "    def setIncomingBranchUniqueEvts(self, incomingBranchUniqueEvts):\n",
    "        \"\"\"Assigns value to incomingBranchUniqueEvts\"\"\"\n",
    "        self.incomingBranchUniqueEvts = incomingBranchUniqueEvts\n",
    "\n",
    "    # def setIncomingBranchSimilarityStats(self, mean, median, variance):\n",
    "    #    self.incomingBranchSimMean=mean\n",
    "    #    self.incomingBranchSimMedian=median\n",
    "    #    self.incomingBranchSimVariance=variance\n",
    "\n",
    "    def setIncomingSequences(self, incomingBranchSeqs):\n",
    "        \"\"\"Assigns value to incomingSequences\"\"\"\n",
    "        self.incomingSequences = incomingBranchSeqs\n",
    "\n",
    "    def setRelTimeStamps(self, relTimeStamps):\n",
    "        \"\"\"Assigns value to  meanRelTimestamp and medianRelTimestamp\"\"\"\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        relTimeStamps.sort()\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "\n",
    "        sumVal = sum(relTimeStamps, timedelta())\n",
    "\n",
    "        #mid = len(reltimestamps)/2\n",
    "\n",
    "        if len(relTimeStamps) == 0:\n",
    "            self.meanRelTimestamp = 0\n",
    "            self.medianRelTimestamp = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.meanRelTimestamp = sumVal*1.0/len(relTimeStamps)\n",
    "            # (reltimestamps[mid-1]+reltimestamps[mid])/2.0\n",
    "            # if len(reltimestamps%2==0) else reltimestamps[mid]\n",
    "            self.medianRelTimestamp = np.median(relTimeStamps)\n",
    "\n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "\n",
    "    def getPatternString(self):\n",
    "        \"\"\"Returns the pattern string for this node\"\"\"\n",
    "        return \"-\".join(str(\n",
    "            self.incomingSequences[0].eventstore.reverseAttrDict[self.attr][hashVal])\n",
    "                        for hashVal in self.keyevts if self.incomingSequences)\n",
    "\n",
    "    def getHash(self):\n",
    "        \"\"\"Returns hash value for this node.\"\"\"\n",
    "        return self.hash\n",
    "\n",
    "    def setHash(self, value):\n",
    "        \"\"\"Assigns hash value for this node\"\"\"\n",
    "        self.hash = value\n",
    "\n",
    "    # def jsonSerialize(self):\n",
    "    #    json.dump(self, indent=4, default= TreeNode.jsonDefaultDump)\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        \"\"\"dummy method- implemented in derived class\"\"\"\n",
    "\n",
    "    def jsonSerialize(self) -> None:\n",
    "        \"\"\"dummy method- implemented in derived class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def jsonSerializeDump(obj):\n",
    "        \"\"\"dummy method- implemented in derived class\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(Node):\n",
    "    \"\"\"Class to visualize Coreflow-like Tree data structures\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"\", count_val=0, value=\"\", attr=\"\"):\n",
    "        super().__init__(name, count_val, value)\n",
    "        self.children = []\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        return {\n",
    "            \"event_attribute\": self.value,\n",
    "            \"Pattern\": self.getPatternString(),\n",
    "            \"value\": self.seqCount,\n",
    "            \"median_index\": self.medianStep,\n",
    "            \"average_index\": self.meanStep,\n",
    "\n",
    "            \"children\": [TreeNode.jsonSerializeDump(x) for x in self.children]\n",
    "\n",
    "        }\n",
    "\n",
    "    def jsonSerialize(self) -> None:\n",
    "        json.dumps(self, indent=4, default=TreeNode.jsonSerializeDump)\n",
    "\n",
    "    @staticmethod\n",
    "    def jsonSerializeDump(obj):\n",
    "\n",
    "        if hasattr(obj, \"jsonDefaultDump\"):\n",
    "\n",
    "            return obj.jsonDefaultDump()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(Node):\n",
    "\n",
    "    \"\"\"Class to support graphs where multiple branching of nodes are possible\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"\", count_val=0, value=\"\", attr=\"\"):\n",
    "        super().__init__(name, count_val, value, attr)\n",
    "        self.before = []\n",
    "        self.after = []\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        return {\n",
    "            \"before\": GraphNode.jsonSerializeDump(self.before),\n",
    "            \"event_attribute\": self.value,\n",
    "            \"Pattern\": self.getPatternString(),\n",
    "            \"value\": self.seqCount,\n",
    "            \"After\": GraphNode.jsonSerializeDump(self.after)\n",
    "\n",
    "        }\n",
    "\n",
    "    def jsonSerialize(self) -> None:\n",
    "        json.dumps(self, indent=4, default=GraphNode.jsonSerializeDump)\n",
    "\n",
    "    @staticmethod\n",
    "    def jsonSerializeDump(obj):\n",
    "\n",
    "        if hasattr(obj, \"jsonDefaultDump\"):\n",
    "\n",
    "            return obj.jsonDefaultDump()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creates the RawNode, Links and Graph class.\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "class RawNode:\n",
    "    \"\"\"RawNode contains selected attributes from Node class for json conversion.\"\"\"\n",
    "\n",
    "    def __init__(self, node):\n",
    "        self.nid = node.nid\n",
    "        self.seqCount = node.seqCount\n",
    "        self.value = node.value\n",
    "        self.pattern = node.getPatternString()\n",
    "        self.meanStep = node.meanStep\n",
    "        self.medianStep = node.medianStep\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        \"\"\"creates the Json format output for the class RawNode.\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self.nid,\n",
    "            \"event_attribute\": self.value,\n",
    "            \"Pattern\": self.pattern,\n",
    "            \"value\": self.seqCount,\n",
    "            \"median_index\": self.medianStep,\n",
    "            \"average_index\": self.meanStep\n",
    "        }\n",
    "\n",
    "    def printNode(self):\n",
    "        \"\"\" Prints details for a node.\"\"\"\n",
    "        print(f'node {self.nid}, value {self.value}, Pattern {self.pattern}, meanStep {self.meanStep} seqcount {self.seqCount}')\n",
    "\n",
    "    @staticmethod\n",
    "    def printNodes(nodeList):\n",
    "        \"\"\"Print all nodes in the list.\"\"\"\n",
    "        for node in nodeList:\n",
    "            node.printNode()\n",
    "\n",
    "\n",
    "\n",
    "class Links:\n",
    "    \"\"\"Links class contains information regarding which node is connected to which one\"\"\"\n",
    "\n",
    "    def __init__(self, node1, node2, count):\n",
    "        self.source = node1\n",
    "        self.target = node2\n",
    "        self.count = count\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        \"\"\"creates the Json format output for the class Links.\"\"\"\n",
    "        return {\n",
    "            \"source\": self.source,\n",
    "            \"target\": self.target,\n",
    "            \"count\": self.count\n",
    "        }\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"Graph class consusts of Links and Nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.links = []  # defaultdict(set)\n",
    "        self.nodes = []\n",
    "\n",
    "    def jsonDefaultDump(self) -> dict:\n",
    "        \"\"\"creates the Json format output for the class Graph.\"\"\"\n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"links\": self.links\n",
    "\n",
    "        }\n",
    "\n",
    "    def jsonSerialize(self) -> None:\n",
    "        \"\"\"Default JSON serializer\"\"\"\n",
    "        json.dumps(self, indent=4, default=Graph.jsonSerializeDump)\n",
    "\n",
    "    @staticmethod\n",
    "    def jsonSerializeDump(obj):\n",
    "        \"\"\"static method to call jsonDefaultDump on all custom objects\"\"\"\n",
    "        if hasattr(obj, \"jsonDefaultDump\"):\n",
    "\n",
    "            return obj.jsonDefaultDump()\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return None  # obj.__dict__\n",
    "\n",
    "    def printGraph(self):\n",
    "        \"\"\"Print the node ids.\"\"\"\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            print(f'node {node.nid}, index {i}')\n",
    "        for i, link in enumerate(self.links):\n",
    "            print(f'links {link.source} {link.target}, index {i}')\n",
    "\n",
    "    def collapseNode(self):\n",
    "        \"\"\"Gets rid of extra nrange(lenodes and links\"\"\"\n",
    "\n",
    "        RawNode.printNodes(self.nodes)\n",
    "        delNodes = []\n",
    "        delLinks = []\n",
    "        newLinks = []\n",
    "\n",
    "        for node in self.nodes:\n",
    "            if node.value == -2:\n",
    "                # ideally. there should be one source\n",
    "                linkArrSrc = [\n",
    "                    x for x in self.links if x.target == node.nid]\n",
    "                print(f'source1 {[lnk.source for lnk in linkArrSrc]}')\n",
    "                print(f'target1 {[lnk.target for lnk in linkArrSrc]}')\n",
    "                # if len(linkArrSrc) == 1:\n",
    "                #    linkArrSrc = linkArrSrc[0]\n",
    "\n",
    "                linkArrTrgt = [\n",
    "                    x for x in self.links if x.source == node.nid]\n",
    "                print(f'source2 {[lnk.source for lnk in linkArrTrgt]}')\n",
    "                print(f'target2 {[lnk.target for lnk in linkArrTrgt]}')\n",
    "\n",
    "                if len(linkArrSrc) > 1 and len(linkArrTrgt) > 1:\n",
    "                    print(f'len source {len(linkArrSrc)}, len target {len(linkArrTrgt)}')\n",
    "                    #continue\n",
    "                    #[x.printNode() for x in self.nodes if x.node_id in ]\n",
    "                    raise ValueError('multiple source and target')\n",
    "\n",
    "\n",
    "                for j, _ in enumerate(linkArrSrc):\n",
    "                    for i, _ in enumerate(linkArrTrgt):\n",
    "                        newLinks.append(\n",
    "                            Links(linkArrSrc[j].source,\n",
    "                                  linkArrTrgt[i].target, linkArrTrgt[i].count))\n",
    "                        delLinks.append(linkArrTrgt[i])\n",
    "                    delLinks.append(linkArrSrc[j])\n",
    "\n",
    "\n",
    "                delNodes.append(node)\n",
    "\n",
    "        #print(f'Node delete {[node. nid for node in delNodes]}')\n",
    "        # print(\n",
    "        #    f'Link delete {[((link.source, link.target)) for link in delLinks]}')\n",
    "\n",
    "        # print(self.printGraph())\n",
    "\n",
    "        delNodeIndices = [self.nodes.index(x) for x in delNodes]\n",
    "        delLinkIndices = [self.links.index(x) for x in delLinks]\n",
    "\n",
    "        #print(f'Node delete {delNodeIndices}')\n",
    "        #print(f'Link delete {delLinkIndices}')\n",
    "        print(delLinkIndices)\n",
    "        for idx in sorted(delNodeIndices, reverse=True):\n",
    "            del self.nodes[idx]\n",
    "\n",
    "        # To sure uniqueness we use list(set) operation here\n",
    "        for idx in sorted(list(set(delLinkIndices)), reverse=True):\n",
    "            print(f'index {idx}')\n",
    "            del self.links[idx]\n",
    "\n",
    "        self.links.extend(newLinks)\n",
    "\n",
    "    def allignNodes(self):\n",
    "        \"\"\" Align  nodes according to their position in sequence. \"\"\"\n",
    "        print(\"nodes sorted\")\n",
    "        node = sorted(self.nodes, key=lambda x: x.meanStep)\n",
    "        RawNode.printNodes(node)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RankingFunction:\n",
    "    \"\"\" Class to perform ranking and tiebreaker among events.\"\"\"\n",
    "\n",
    "    def __init__(self, maxSup):\n",
    "        self.fdist = {}\n",
    "        self.fdistInd = {}\n",
    "        self.pos = -1\n",
    "        self.word = \"\"\n",
    "        self.count = 0\n",
    "        self.maxSupport = maxSup\n",
    "        self.rankingFunc = self.numberOfSequence\n",
    "        self.tieBreaker = self.performRankingMedianIndex#self.\n",
    "        \n",
    "    def setRankingFunc(self, method1):\n",
    "        \"\"\"Set ranking function.\"\"\"\n",
    "        self.rankingFunc = method1\n",
    "\n",
    "    def setTieBreaker(self, method1):\n",
    "        \"\"\"Set tie breaker.\"\"\"\n",
    "        self.tieBreaker = method1\n",
    "\n",
    "    def clearfdists(self):\n",
    "        \"\"\"clear fdist and fdistInd.\"\"\"\n",
    "        self.fdist.clear()\n",
    "        self.fdistInd.clear()\n",
    "\n",
    "    def initValues(self):\n",
    "        \"\"\"Initialize pos, word and count.\"\"\"\n",
    "        self.pos = -1\n",
    "        self.word = \"\"\n",
    "        self.count = 0\n",
    "\n",
    "    def performRankingNaive(self, index, _minpos):\n",
    "        \"\"\"Naive ranking of events, does not consider index.\"\"\"\n",
    "        maxWord = \"\"\n",
    "        maxCount = 0\n",
    "        for word in self.fdist:\n",
    "            value = self.fdist[word]\n",
    "\n",
    "            if maxCount < value <= self.maxSupport:\n",
    "                maxWord = str(word)\n",
    "                maxCount = value\n",
    "\n",
    "        if maxCount > self.count:\n",
    "            self.pos = index\n",
    "            self.word = maxWord\n",
    "            self.count = maxCount\n",
    "\n",
    "    def performRankingMeanIndex(self, index, minPos):\n",
    "        \"\"\"If two events have the same number of Occurrences tie breake\n",
    "        based on minimum Mean Index value.\n",
    "        \"\"\"\n",
    "        maxWord = \"\"\n",
    "        maxCount = 0\n",
    "\n",
    "        for word in self.fdist:\n",
    "            value = self.fdist[word]\n",
    "\n",
    "            meanPos = sum(self.fdistInd[word]) / len(self.fdistInd[word])\n",
    "\n",
    "            if maxCount < value <= self.maxSupport:\n",
    "                maxWord = str(word)\n",
    "                maxCount = value\n",
    "                minPos = meanPos\n",
    "\n",
    "            if value == maxCount and meanPos < minPos:\n",
    "                maxWord = str(word)\n",
    "                maxCount = value\n",
    "                minPos = meanPos\n",
    "\n",
    "        if maxCount > self.count or (maxCount == self.count and self.pos < index):\n",
    "            self.pos = index\n",
    "            self.word = maxWord\n",
    "            self.count = maxCount\n",
    "\n",
    "\n",
    "    def performRankingMedianIndex(self, index, minPos):\n",
    "        \"\"\"If two events have the same number of Occurrences tie breake\n",
    "        based on minimum Mean Index value.\n",
    "        \"\"\"\n",
    "        maxWord = \"\"\n",
    "        maxCount = 0\n",
    "\n",
    "        for word in self.fdist.keys():\n",
    "            value = self.fdist[word]\n",
    "\n",
    "            meadianPos = np.median(self.fdistInd[word])\n",
    "\n",
    "            if maxCount < value <= self.maxSupport:\n",
    "                maxWord = str(word)\n",
    "                maxCount = value\n",
    "                minPos = meadianPos\n",
    "\n",
    "            if value == maxCount and meadianPos < minPos:\n",
    "                maxWord = str(word)\n",
    "                maxCount = value\n",
    "                minPos = meadianPos\n",
    "\n",
    "        if maxCount > self.count or (maxCount == self.count and self.pos < index):\n",
    "            self.pos = index\n",
    "            self.word = maxWord\n",
    "            self.count = maxCount\n",
    "\n",
    "    def numberOfSequence(self, evtHashes, startPos, endPos, seq):\n",
    "        \"\"\"Choose the event present in maximum number of sequences\n",
    "        as the next Pattern event.\n",
    "        \"\"\"\n",
    "\n",
    "        duplicate = []\n",
    "        for j in range(startPos, endPos):\n",
    "            word = evtHashes[j]\n",
    "            # print(word)\n",
    "            if word in duplicate:\n",
    "                continue\n",
    "            duplicate.append(word)\n",
    "            if word not in self.fdist:\n",
    "                self.fdist[word] = seq.getVolume()\n",
    "                self.fdistInd[word] = [j]\n",
    "            else:\n",
    "                self.fdist[word] += seq.getVolume()\n",
    "                self.fdistInd[word].append(j)\n",
    "\n",
    "    def allOccurrence(self, evtHashes, startPos, endPos, seq):\n",
    "        \"\"\"Choose the event present maximum number of time across sequences\n",
    "        as the next Pattern event.\n",
    "        \"\"\"\n",
    "\n",
    "        for j in range(startPos, endPos):\n",
    "            word = evtHashes[j]\n",
    "            # print(word)\n",
    "            if word not in self.fdist:\n",
    "                self.fdist[word] = seq.getVolume()\n",
    "                self.fdistInd[word] = [j]\n",
    "            else:\n",
    "                self.fdist[word] += seq.getVolume()\n",
    "                self.fdistInd[word].append(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenTree Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenTreeMiner:\n",
    "    \n",
    "    def __init__ (self , minSup, maxSup):\n",
    "        self.minSupport = minSup\n",
    "        self.maxSupport = maxSup\n",
    "        \n",
    "        self.ranker = RankingFunction(maxSup)\n",
    "        self.ranker.setRankingFunc(self.ranker.numberOfSequence)\n",
    "        self.ranker.setTieBreaker(self.ranker.performRankingMedianIndex)\n",
    "\n",
    "    def expandSeqTree(self, attr, rootNode, expandCnt, graph):\n",
    "        \"\"\"Chooses which branch of the tree to expand next.\"\"\"\n",
    "        # if len(rootSeq.eventlist>0):\n",
    "        expandCnt -= len(rootNode.keyevts)\n",
    "        seqs = []\n",
    "        seqs.append(rootNode)\n",
    "        rootNode.setSeqCount(Sequence.getSeqVolume(rootNode.incomingSequences))\n",
    "        rootNode.attr = attr\n",
    "        leafSeqs = []\n",
    "\n",
    "        graph.nodes.append(RawNode(rootNode))\n",
    "        while seqs and expandCnt > 0:\n",
    "            currentSeq = max(seqs, key=lambda x: x.seqCount)\n",
    "            print(f'seqCount: {currentSeq.seqCount}')\n",
    "\n",
    "            seq0 = currentSeq.after\n",
    "            seq1 = currentSeq.before\n",
    "\n",
    "            print(f'this.pattern currentSeq : {currentSeq.keyevts}')\n",
    "\n",
    "            if not seq1 and not seq0:\n",
    "                word, pos, count, seq0, seq1 = self.growSeq(\n",
    "                    attr, currentSeq)\n",
    "                print(f'event: {word}, pos: {pos}, count: {count}')\n",
    "\n",
    "                if count < self.minSupport:\n",
    "                    leafSeqs.append(currentSeq)\n",
    "                else:\n",
    "\n",
    "                    seq1.setHash(word)\n",
    "                    seq1.setValue(\n",
    "                        currentSeq.incomingSequences[0].getEvtAttrValue(attr, word))\n",
    "                    seq1.keyevts = currentSeq.keyevts[:]  # deep copy\n",
    "                    seq0.keyevts = currentSeq.keyevts[:]\n",
    "\n",
    "                    seq1.keyevts.insert(pos, word)\n",
    "\n",
    "            if seq1 and seq1.seqCount >= self.minSupport:\n",
    "                expandCnt -= 1\n",
    "                seqs.append(seq1)\n",
    "                graph.nodes.append(RawNode(seq1))\n",
    "                graph.links.append(\n",
    "                    Links(currentSeq.nid, seq1.nid, seq1.seqCount))\n",
    "\n",
    "            currentSeq.before = seq1\n",
    "            currentSeq.after = seq0\n",
    "\n",
    "            if seq0 and seq0.seqCount >= self.minSupport:\n",
    "                seqs.append(seq0)\n",
    "                graph.nodes.append(RawNode(seq0))\n",
    "                graph.nodes[-1].value = -2  # dummy node value\n",
    "                graph.links.append(\n",
    "                    Links(currentSeq.nid, seq0.nid, seq0.seqCount))\n",
    "\n",
    "            print(f'seqCount: {[s.seqCount for s in seqs]}')\n",
    "\n",
    "            del seqs[seqs.index(currentSeq)]\n",
    "\n",
    "        #graph.collapseNode()\n",
    "        #graph.allignNodes()\n",
    "\n",
    "        return leafSeqs.append(seqs)\n",
    "\n",
    "    \n",
    "    \n",
    "    def growSeq(self, attr, seq):\n",
    "        \"\"\"Expands the current max Pattern by another event.\"\"\"\n",
    "        self.ranker.initValues()\n",
    "\n",
    "        for i in range(0, len(seq.keyevts)+1):\n",
    "            self.ranker.clearfdists()\n",
    "\n",
    "            for elem in seq.incomingSequences:\n",
    "\n",
    "                evtHashes = elem.getHashList(attr)\n",
    "                startPos = 0 if i == 0 else elem.seqIndices[i - 1] + 1\n",
    "                endPos = len(evtHashes) if i == len(\n",
    "                    seq.keyevts) else elem.seqIndices[i]\n",
    "\n",
    "                self.ranker.rankingFunc(evtHashes, startPos,\n",
    "                                        endPos, elem)\n",
    "\n",
    "            minPos = max(len(x.events) for x in seq.incomingSequences)\n",
    "\n",
    "            self.ranker.tieBreaker(i, minPos)\n",
    "\n",
    "        seq0 = GraphNode(attr=attr)\n",
    "        seq1 = GraphNode(attr=attr)\n",
    "\n",
    "        if self.ranker.count >= self.minSupport:\n",
    "            words = seq.keyevts\n",
    "            for elem in seq.incomingSequences:\n",
    "                startPos = 0 if self.ranker.pos == 0 else elem.seqIndices[self.ranker.pos - 1] + 1\n",
    "                endPos = len(elem.events) if self.ranker.pos == len(\n",
    "                    words) else elem.seqIndices[self.ranker.pos]\n",
    "                try:\n",
    "                    i = elem.getHashList(attr).index(self.ranker.word, startPos, endPos)\n",
    "                    # sequence index value for the word being inserted. e.g. A-C-G seq indice 1,4,8\n",
    "                    elem.seqIndices.insert(self.ranker.pos, i)\n",
    "                    seq1.incomingSequences.append(elem)\n",
    "                    seq1.seqCount += elem.getVolume()\n",
    "\n",
    "                except ValueError:\n",
    "                    seq0.incomingSequences.append(elem)\n",
    "                    seq0.seqCount += elem.getVolume()\n",
    "            # calculate average index\n",
    "            posArr = [seq.seqIndices[self.ranker.pos] for seq in seq1.incomingSequences]\n",
    "            seq1.setPositions(posArr)\n",
    "            seq0.setSeqCount(Sequence.getSeqVolume(seq0.incomingSequences))\n",
    "            seq1.setSeqCount(Sequence.getSeqVolume(seq1.incomingSequences))\n",
    "            seq0.sequences = seq0.incomingSequences\n",
    "            seq1.sequences = seq1.incomingSequences\n",
    "\n",
    "            print(f'Not contain: {len(seq0.incomingSequences)}')\n",
    "            print(f'contain: {len(seq1.incomingSequences)}')\n",
    "\n",
    "        return self.ranker.word, self.ranker.pos, self.ranker.count, seq0, seq1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def getDataframe(src, local=False, sep=\"\\t\", header=None):\n",
    "    \"\"\"Helper function to return a data frame\n",
    "    Local is boolean, if local then source should be path to the file\n",
    "    Otherwise it should be a URL to the the file\n",
    "    \"\"\"\n",
    "\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        urlContent = req.content\n",
    "        csvFile = open('data.txt', 'wb')\n",
    "        csvFile.write(urlContent)\n",
    "        csvFile.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if header is None:\n",
    "            dataFrame = pd.read_csv(\"data.txt\", sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            dataFrame = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        # return dataFrame\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            print(src)\n",
    "            dataFrame = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            dataFrame = pd.read_csv(src, sep, names=header)\n",
    "    return dataFrame    \n",
    "    \n",
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "def getTimeToSortBy(evt):\n",
    "    \"\"\"Helper function for generateSequence to use when sorting events to get\n",
    "    what time field to sort by. Also used in splitSequences to give the time of\n",
    "    an event when splitting the events up\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if isinstance(evt, IntervalEvent):\n",
    "        return evt.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    return evt.timestamp\n",
    "\n",
    "\n",
    "    \n",
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insertEventIntoDict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../datasets/sequence_braiding/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "#Sequence.create_attr_dict([seq])\n",
    "#seq.getEventPosition('Meal','Lunch')\n",
    "#print(seq.getUniqueValueHashes('Meal'))\n",
    "#print(seq.getHashList('Glucose'))\n",
    "#print(seq.getValueHashes('Glucose'))\n",
    "#print(seq.getEventsHashString('Glucose'))\n",
    "#raw_seq=seq.convertToVMSPReadable('Meal')\n",
    "#print(raw_seq)\n",
    "#print(seq.getPathID())\n",
    "#sequence_braiding[0].attributes.keys()\n",
    "#print(sequence_braiding[0].getAttrVal('Meals'))\n",
    "#print(sequence_braiding[0].type)\n",
    "#for events in sequence_braiding:\n",
    "#    print(events.getAttrVal('Meal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=Sequence.splitSequences(seq, \"week\")\n",
    "#seq_list=[]\n",
    "#for seqs in sequence_braiding_split:\n",
    "#    seq_list.append(Sequence(seqs))\n",
    "    \n",
    "#Sequence.create_attr_dict(seq_list)\n",
    "raw_seq=\"\\n\".join( seqs.getEventsString('Meal') for seqs in seq_list)\n",
    "raw_seq2=[seqs.getEventsString('Meal') for seqs in seq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_braiding_Es.reverseAttrDict['Meal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('demo3.tsv', 'w') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    writer.writerow([\"id\", \"text\", \"count\"])\n",
    "    for index, elem in enumerate(raw_seq2):\n",
    "        writer.writerow([index, elem, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabularies- can be emulated from attrdict\n",
    "# itemset- keys of vocabularies\n",
    "#count- seq volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list[0].events[0].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[0,1,5]\n",
    "seq_sublist=[seq_list[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_sublist[2].events[7].getAttrVal('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq= [seqs.getEventsHashString('Meal') for seqs in seq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stm= SentenTreeMiner(minSup=5, maxSup=len(raw_seq))\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Meal\",root,  expandCnt=30, graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.jsonSerializeDump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empJSON = jsonpickle.encode(graph, unpicklable=False)\n",
    "#empJSON = jsonpickle.encode(graph)\n",
    "print(\"Writing JSON Encode data into Python String\")\n",
    "employeeJSONData = json.dumps(empJSON, indent=4)\n",
    "print(employeeJSONData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=json.dumps(graph, ensure_ascii=False, default=Graph.jsonSerializeDump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "my_dict = {'name': 'flare',\n",
    "           'children': [{'name': k,\n",
    "                         'children': [{'name': child} for child in v]}\n",
    "                            for k, v in my_defaultdict.items()]}\n",
    "\n",
    "json_data = json.dumps(my_dict, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(graph, default=lambda o: o.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=[]\n",
    "#merge(h,key=lambda e:e[0],reverse=True)\n",
    "heapq.heappush(h, (200, 1))\n",
    "heapq.heappush(h, (300,2))\n",
    "heapq.heappush(h, (400,3))\n",
    "print(heapq.heappop(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"abcd\".index('c',2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../Sample_Dataset.csv', 0, \"%m/%d/%Y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=sequence_braiding_Es.splitSequences(seq, \"day\")\n",
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Events') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_braiding_Es.reverseAttrDict['Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Events\",root,  expandCnt=30, minSupport=2, maxSupport=len(seq_list)*5,graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding_Es= EventStore()\n",
    "sequence_braiding_Es.importPointEvents('../corelow_paper_test.csv', 1, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=Sequence(sequence_braiding_Es.events, sequence_braiding_Es)\n",
    "seq_list=sequence_braiding_Es.generateSequence(\"Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_list=sequence_braiding_Es.splitSequences(seq, \"day\")\n",
    "raw_seq=\"\\n\".join( seqs.getEventsString('Event') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq=\"\\n\".join( seqs.getEventsHashString('Event') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_braiding_Es.reverseAttrDct['Event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stm= SentenTreeMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=GraphNode()\n",
    "root.incomingSequences=seq_list\n",
    "graph=Graph()\n",
    "visibleGroups=stm.expandSeqTree(\"Event\",root,  expandCnt=30, minSupport=1, maxSupport=len(seq_list)*5,graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=json.dumps(root, ensure_ascii=False, default=GraphNode.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=json.dumps(graph, ensure_ascii=False, default=Graph.json_serialize_dump, indent=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(vars(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
