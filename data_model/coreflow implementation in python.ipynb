{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "from itertools import accumulate\n",
    "\n",
    "from spmf import Spmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common class for all Events\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, eventtype):\n",
    "        self.type=eventtype\n",
    "    \n",
    "    #Return Attribute value given attribute name\n",
    "    def getAttrVal(self, attrName):\n",
    "        return self.attributes.get(attrName,None)\n",
    "\n",
    "    \n",
    "# A class that represents a point event\n",
    "class PointEvent(Event):\n",
    "    def __init__(self, timestamp, attributes):\n",
    "        Event.__init__(self,\"point\")\n",
    "        #self.type = \"point\"\n",
    "        self.timestamp = timestamp \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes \n",
    "        \n",
    "    \n",
    "\n",
    "# class to represent an interval event\n",
    "class IntervalEvent(Event):\n",
    "    def __init__(self, t1, t2, attributes):\n",
    "        Event.__init__(self,\"interval\")\n",
    "        #self.type = \"interval\"\n",
    "        self.time = [t1,t2] \n",
    "        # dictionary: key=attribute value=attribute value\n",
    "        self.attributes = attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence:\n",
    "    _ids = count(0)\n",
    "    \n",
    "    attrdict={}\n",
    "    reverseatttrdict={}\n",
    "    def __init__(self, events, sid=None):\n",
    "        # sequence id\n",
    "        if sid is None:\n",
    "            self.sid = next(self._ids)\n",
    "        else:\n",
    "            self.sid = sid\n",
    "        \n",
    "        self.events = events\n",
    "        self.volume=1\n",
    "        self.seqAttributes={}\n",
    "    def getEventPosition(self, attr, hash_val):\n",
    "        for count,event in enumerate(self.events):\n",
    "            #if event.getAttrVal(attr)==hash_val:\n",
    "            if Sequence.attrdict[attr][event.getAttrVal(attr)]==hash_val:\n",
    "                return count\n",
    "        return -1\n",
    "    \n",
    "    def setVolume(self, intValue):\n",
    "        self.volume=intValue\n",
    "        \n",
    "    def getVolume(self):\n",
    "        return self.volume\n",
    "    \n",
    "    def increaseVolume(self):\n",
    "        self.volume += 1 \n",
    "    \n",
    "    \n",
    "    def getUniqueValues(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        return l\n",
    "    \n",
    "    def getUniqueValueHashes(self, attr):\n",
    "        l=list(set(event.getAttrVal(attr) for event in self.events))\n",
    "        uniquelist=[Sequence.attrdict[attr][elem] for elem in l]\n",
    "        return uniquelist\n",
    "    \n",
    "    #Not sure this will always result in same index, will change if \n",
    "    #dictionary is updated\n",
    "    #since python is unordered\n",
    "    \n",
    "    def getHashList(self, attr):\n",
    "        #l=list(list(event.attributes.keys()).index(attr) for event in self.events)\n",
    "        l=[event.getAttrVal(attr) for event in self.events]\n",
    "        hashlist=[Sequence.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "    \n",
    "    def getValueHashes(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        hashlist=[Sequence.attrdict[attr][elem] for elem in l]\n",
    "        \n",
    "        return hashlist\n",
    "        \n",
    "    \n",
    "    def getEventsHashString(self, attr):\n",
    "        s=attr+\": \"\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" \"\n",
    "        s+=\"\".join(str(Sequence.attrdict[attr][elem]) for elem in l)\n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadablenum(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" -1 \".join(str(Sequence.attrdict[attr][elem]) for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\" -2\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def convertToVMSPReadable(self, attr):\n",
    "        l=list(event.getAttrVal(attr) for event in self.events)\n",
    "        s=\" \".join(Sequence.attrdict[attr][elem] for elem in l)\n",
    "        #s=\"\"\n",
    "        #for count,event in enumerate(self.events):\n",
    "        #    s+=str(event.getAttrVal(attr))+\" -1 \"\n",
    "        s+=\".\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def getPathID(self):\n",
    "        return self.sid\n",
    "    \n",
    "    def matchPathAttribute(self, attr):\n",
    "        # should i use eq?!\n",
    "        if this.seqAttributes.get(attr)==(val):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def setSequenceAttribute(self,attr, value):\n",
    "        self.seqAttributes[attr]=value\n",
    "        \n",
    "         \n",
    "\n",
    "    # equivalent to method signature public static int getVolume(List<Sequence> seqs)    \n",
    "    def getSeqVolume(seqlist):\n",
    "        return sum(seq.getVolume() for seq in seqlist)\n",
    "    \n",
    "    \n",
    "    # Method equivalent to public String getEvtAttrValue(String attr, int hash) in DataManager.java\n",
    "    def getEvtAttrValue(attr, hashval):\n",
    "        return Sequence.reverseatttrdict[attr][hashval]\n",
    "        \n",
    "    # Method equivalent to public List<String> getEvtAttrValues(String attr) in DataManager.java    \n",
    "    def getEvtAttrValues(attr):\n",
    "        return list(Sequence.reverseatttrdict[attr].values())\n",
    "    \n",
    "    # Method equivalent to int getEvtAttrValueCount(String attr) in DataManager.java    \n",
    "    def getEvtAttrValueCount(attr):\n",
    "        return len(Sequence.reverseatttrdict[attr])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattern:\n",
    "    _pids = count(1)\n",
    "\n",
    "    def __init__(self, events=[]):\n",
    "        #pattern id\n",
    "        self.id = next(self._pids)\n",
    "        \n",
    "        self.keyEvts = events\n",
    "        \n",
    "        self.medianPos=[]\n",
    "        self.meanPos=[]\n",
    "        \n",
    "        self.sids=[]\n",
    "        \n",
    "        self.support=0\n",
    "        self.supPercent=None\n",
    "        self.cluster=None\n",
    "        self.medianPathLength=0\n",
    "        self.meanPathLength=0\n",
    "        \n",
    "        self.parnetSegment=None\n",
    "        self.segSizes=None\n",
    "        \n",
    "    def filterPaths(self, paths, evtType):\n",
    "        print(\"filtering \"+ str(len(paths))+\" paths by \"+str(len(self.keyEvts))+\" checkpoints\")\n",
    "        \n",
    "        for sequences in paths:\n",
    "            if(self.matchMilestones(sequences.getValueHashes(evtType),self.keyEvts)==False):\n",
    "                continue\n",
    "            self.sids.append(sequences)\n",
    "            \n",
    "        print(str(len(self.sids))+\" matching paths\")\n",
    "\n",
    "        \n",
    "    def matchMilestones(self, arr, milestones):\n",
    "        ja=arr\n",
    "        idx=-1\n",
    "        for elems in milestones:\n",
    "            try:\n",
    "                idx=arr[idx+1:].index(elems)\n",
    "                #print(idx)\n",
    "            except ValueError:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def getMedianSpacing(self):\n",
    "        l=[y - x for x,y in zip(self.medianPos,self.medianPos[1:])]\n",
    "        if(len(l)<=1):\n",
    "            return 100\n",
    "        l=l.sort()\n",
    "        middle=int(len(l)/2)\n",
    "        if(len(l)%2==0):\n",
    "            return ((l[middle-1]+l[middle])/2.0)\n",
    "        else:\n",
    "            return l[middle]\n",
    "        return np.median(np.asarray(l))\n",
    "    \n",
    "    def addKeyEvent(self, hashval):\n",
    "        self.keyEvts.append(hashval)\n",
    "        \n",
    "    def addToSupportSet(self, seq):\n",
    "        #print(seq.sid)\n",
    "        self.sids.append(seq)\n",
    "        self.support+=seq.getVolume()\n",
    "        \n",
    "    def getSequences(self):\n",
    "        return self.sids\n",
    "    \n",
    "    def setMedianPathLength(self, median):\n",
    "        self.medianPathLength=median\n",
    "    \n",
    "    def setMeanPathLength(self, mean):\n",
    "        self.meanPathLength=mean\n",
    "        \n",
    "    def getMedianPathLength(self):\n",
    "        return self.medianPathLength\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "    \n",
    "    def getEvents(self):\n",
    "        return self.keyEvts\n",
    "    \n",
    "    def getEventMeanPos(self):\n",
    "        return self.meanPos\n",
    "    \n",
    "    def getEventMedianPos(self):\n",
    "        return self.medianPos\n",
    "    \n",
    "    #Do we need to preserve order here??\n",
    "\n",
    "    def getUniqueEventsString(self):\n",
    "        #return \"-\".join(str(x) for x in list(set(self.keyEvts)))\n",
    "        return \"-\".join(str(x) for x in list(dict.fromkeys(self.keyEvts)))\n",
    "    \n",
    "    def getPositions(self, events, path):\n",
    "        sequence=path\n",
    "        pos=[]\n",
    "        idx=-1\n",
    "        offset=0\n",
    "        \n",
    "        for elems in events:\n",
    "            \n",
    "            offset+=idx+1\n",
    "            try:\n",
    "                idx=path[offset:].index(elems)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            pos.append(offset+idx)\n",
    "        return pos\n",
    "    \n",
    "    def getMedian(self, data):\n",
    "        #middle=len(data)/2\n",
    "        #if(len(data)%2==0 and len(data)>1):\n",
    "        #    return (data[middle-1]+data[middle])/2.0\n",
    "        #else: \n",
    "        #    return data[middle]\n",
    "        return np.median(data)\n",
    "    \n",
    "    def computePatternStats(self, evtAttr):\n",
    "        pathsOfStrings=[]\n",
    "        #print(f' sids {self.sids}')\n",
    "        for path in self.sids:\n",
    "            pageSequence=path.getHashList(evtAttr)\n",
    "            pathsOfStrings.append(pageSequence)\n",
    "        \n",
    "        #print(f'path of string {pathsOfStrings}')\n",
    "        medians=[]\n",
    "        means=[]\n",
    "        \n",
    "        ## swap the loops for better readability\n",
    "        for i,events in enumerate(self.keyEvts):\n",
    "            numSteps=[]\n",
    "            \n",
    "            for idx,paths in enumerate(pathsOfStrings):\n",
    "                if(self.matchMilestones(paths, self.keyEvts[0:i+1])):\n",
    "                    pos=self.getPositions(self.keyEvts[0:i+1], paths)\n",
    "                    if i==0:\n",
    "                        #add position value of first element id sequence\n",
    "                        numSteps.append(pos[i])\n",
    "                    else:\n",
    "                        #in other cases add the difference\n",
    "                        numSteps.append(pos[i]-pos[i-1])\n",
    "            sum_steps=sum(numSteps)\n",
    "            \n",
    "            median= self.getMedian(numSteps)\n",
    "            \n",
    "            medians.append(median)\n",
    "            means.append(sum_steps*1.0/ len(numSteps))\n",
    "                \n",
    "            \n",
    "                \n",
    "        #list(accumulate(means))\n",
    "        means=np.cumsum(np.asarray(means))\n",
    "        medians=np.cumsum(np.asarray(medians))\n",
    "        \n",
    "        self.setMedianPositions(medians)\n",
    "        self.setMeanPositions(means)\n",
    "        \n",
    "        trailingSteps=[0]*len(self.sids)\n",
    "        for i,path in enumerate(self.sids):\n",
    "            pos=self.getPositions(self.keyEvts, path.getHashList(evtAttr))\n",
    "            #the difference between the last event in thesequence and the last key event\n",
    "            trailingSteps[i]= len(path.events)- pos[-1]-1\n",
    "        \n",
    "        trailStepSum=sum(trailingSteps)\n",
    "        median= self.getMedian(trailingSteps)\n",
    "        mean= trailStepSum/len(trailingSteps)\n",
    "        \n",
    "        self.setMedianPathLength(median+medians[-1])\n",
    "        self.setMeanPathLength(mean+means[-1])\n",
    "                                  \n",
    "    def getMedianPositions(self, allPos, pids):\n",
    "        median=[]\n",
    "        for k in range(0, len(pid)):\n",
    "            posInPaths=allPos[k]\n",
    "            median.append(self.getMedian(posInPaths))\n",
    "        #return list(self.getMedian(posInPaths) for posInPaths in allPos)\n",
    "        return median\n",
    "    \n",
    "    def getMeanPositions(self, allPos, pids):\n",
    "        mean=[]\n",
    "        for k in range(0, len(allPos)):\n",
    "            mean.append(sum(allPos[k])*1.0/(len(allPos[k])))\n",
    "        return mean\n",
    "    \n",
    "    def setMedianPositions(self, median):\n",
    "        self.medianPos=median\n",
    "        \n",
    "    def setMeanPositions(self, mean):\n",
    "        self.meanPos=mean\n",
    "        \n",
    "    def toJson(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4)\n",
    "    \n",
    "    def getSupport(self):\n",
    "        return self.support\n",
    "    \n",
    "    def setCluster(self, cluster):\n",
    "        self.cluster=cluster\n",
    "        \n",
    "    def setParent(self, parent, segment):\n",
    "        self.parent=parent\n",
    "        self.parentSegment=segment\n",
    "    \n",
    "    \n",
    "    # How to implement this with BitArray?\n",
    "    #def getEventBitSet(self)\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    \n",
    "    def getParentSegment(self):\n",
    "        return self.parentSegment\n",
    "    \n",
    "    def setMeanPathLength(self,d):\n",
    "        self.meanPathLength=d\n",
    "    \n",
    "    def getMeanPathLength(self):\n",
    "        return self.meanPathLength\n",
    "        \n",
    "    def setSupport(self, sup, total):\n",
    "        self.support=sup\n",
    "        self.supPercent= sup*1.0/total\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreeNode Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    NID=count(1)\n",
    "    nodeHash={}\n",
    "    \n",
    "    \n",
    "    def __init__(self, name=\"\", count=0, value=\"\"):\n",
    "        \n",
    "        self.nid=next(self.NID)\n",
    "        self.name=name\n",
    "        self.seqCount=count\n",
    "        ## What's the difference between name and value?\n",
    "        self.value=value\n",
    "        self.hash=-1\n",
    "        self.pos=[]\n",
    "        self.meanStep=0\n",
    "        self.medianStep=0\n",
    "        #self.zipCompressRatio=0\n",
    "        self.incomingBranchUniqueEvts=None\n",
    "        #self.incomingBranchSimMean=None\n",
    "        #self.incomingBranchSimMedian=None\n",
    "        #self.incomingBranchSimVariance=None\n",
    "        \n",
    "        self.incomingSequences=[]\n",
    "        self.outgoingSequences=[]\n",
    "        \n",
    "        self.meanRelTimestamp=0\n",
    "        self.medianRelTimestamp=0\n",
    "        \n",
    "        TreeNode.nodeHash[self.nid]=self\n",
    "        self.children = []\n",
    "        \n",
    "    def getNode(self, node_id):\n",
    "        return nodeHash[node_id]\n",
    "    \n",
    "    def clearHash(self):\n",
    "        nodeHash.clear()\n",
    "        \n",
    "    def getIncomingSequences(self):\n",
    "        return self.incomingSequences\n",
    "    \n",
    "    def getSeqCount(self):\n",
    "        return self.seqCount\n",
    "    \n",
    "    def setSeqCount(self, seqCount):\n",
    "        self.seqCount=seqCount\n",
    "        \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def setName(self, name):\n",
    "        self.name=name\n",
    "        \n",
    "    def getMeanStep(self):\n",
    "        return self.meanStep\n",
    "    \n",
    "    #need a better implementation\n",
    "    def toJSONObject(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__)#,sort_keys=True, indent=4) \n",
    "    \n",
    "    def toString(self):\n",
    "        return self.name+\": \"+self.seqCount\n",
    "    \n",
    "    def setPositions(self, l):\n",
    "        self.pos=l\n",
    "        self.pos.sort()\n",
    "        d=sum(self.pos)+len(self.pos)\n",
    "        mid=len(self.pos)/2\n",
    "        \n",
    "        if len(self.pos)==0:\n",
    "            self.meanStep=0\n",
    "            slf.medianStep=0\n",
    "        else:\n",
    "            #WHY WE ARE ADDING 1 to mean and medianStep?\n",
    "            self.meanStep=d/len(self.pos)\n",
    "            self.medianStep= np.median(self.pos)+1#((self.pos[mid-1]+self.pos[mid])/2.0)+1 if len(self.pos)%2==0 else self.pos[mid]+1\n",
    "            \n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "    def setValue(self, value):\n",
    "        self.value=value\n",
    "        \n",
    "    def getMedianStep(self):\n",
    "        return self.medianStep\n",
    "    \n",
    "    #def getZipCompressRatio(self):\n",
    "    #    return self.zipCompressRatio\n",
    "    \n",
    "    #def setZipCompressRatio(self, zipcompressratio):\n",
    "    #    self.zipCompressRatio=zipcompressratio\n",
    "        \n",
    "    def getIncomingBranchUniqueEvts(self):\n",
    "        return self.incomingBranchUniqueEvts\n",
    "    \n",
    "    def setIncomingBranchUniqueEvts(self, incomingbranchuniqueevts):\n",
    "        self.incomingBranchUniqueEvts=incomingbranchuniqueevts\n",
    "        \n",
    "    #def setIncomingBranchSimilarityStats(self, mean, median, variance):\n",
    "    #    self.incomingBranchSimMean=mean\n",
    "    #    self.incomingBranchSimMedian=median\n",
    "    #    self.incomingBranchSimVariance=variance\n",
    "        \n",
    "    \n",
    "    def setIncomingSequences(self, incomingbrancseqs, evtattr):\n",
    "        self.incomingSequences=incomingbrancseqs\n",
    "        \n",
    "    def setRelTimeStamps(self, reltimestamps):\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        reltimestamps.sort()\n",
    "        #print(f'Time Stamp {reltimestamps}')\n",
    "        #print(f'Time Stamp {type(reltimestamps[0])}')\n",
    "        \n",
    "        d=sum(reltimestamps, timedelta())\n",
    "        \n",
    "        mid=len(reltimestamps)/2\n",
    "        \n",
    "        if(len(reltimestamps)==0):\n",
    "            self.meanRelTimestamp=0\n",
    "            self.medianRelTimestamp=0\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            self.meanRelTimestamp=d*1.0/len(reltimestamps)\n",
    "            self.medianRelTimestamp=np.median(reltimestamps) #(reltimestamps[mid-1]+reltimestamps[mid])/2.0 if len(reltimestamps%2==0) else reltimestamps[mid]\n",
    "        \n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        #print(f'Time Stamp {self.meanRelTimestamp}')\n",
    "        \n",
    "    def getHash():\n",
    "        return self.hash\n",
    "        \n",
    "    def setHash(self, value):\n",
    "        self.hash=value\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcccurrencesMeanRankingFunction:\n",
    "    def __init__(self):\n",
    "        self.topRankedEvtValues=[]\n",
    "        self.evtAttr=\"\"\n",
    "        \n",
    "    def setEvtAttr(self, evtAttr):\n",
    "        self.evtAttr=evtAttr\n",
    "        #print(f'evtattr {self.evtAttr}')\n",
    "\n",
    "    def getTopEventSet(self):\n",
    "        if not self.topRankedEvtValues:\n",
    "            return None\n",
    "        elif len(self.topRankedEvtValues)==1:\n",
    "            return self.topRankedEvtValues[0]\n",
    "        else:\n",
    "            #for k in self.topRankedEvtValues:\n",
    "            #    print(f'top key {k.keyEvts}')\n",
    "\n",
    "            for p in self.topRankedEvtValues:\n",
    "                p.computePatternStats(self.evtAttr)\n",
    "            \n",
    "            #for k in self.topRankedEvtValues:\n",
    "            #    print(f'sorted key {k.keyEvts}')\n",
    "\n",
    "            self.topRankedEvtValues=sorted(self.topRankedEvtValues, key=lambda x: x.getEventMeanPos()[0])\n",
    "        \n",
    "        return self.topRankedEvtValues[0]\n",
    "    \n",
    "    def performRanking(self, seqs, maxSup, excludedEvts):\n",
    "        result={}\n",
    "        evtHashes=[]\n",
    "        evtValueKey=\"\"\n",
    "        \n",
    "        for seq in seqs:\n",
    "            # get hashlist for each individual sequence\n",
    "            evtHashes= seq.getHashList(self.evtAttr)\n",
    "            #print(f'evthash {evtHashes}')\n",
    "            for hashval in evtHashes:\n",
    "                \n",
    "                if hashval in excludedEvts:\n",
    "                    continue\n",
    "                evtValueKey=str(hashval)\n",
    "                \n",
    "                #create a pattern for all hash values\n",
    "                if evtValueKey  not in result.keys():\n",
    "                    #print(f'evtValueKey {evtValueKey}')\n",
    "                    p=Pattern([evtValueKey])\n",
    "                    #p.addKeyEvent(hashval)\n",
    "                    result[evtValueKey]=p\n",
    "                    \n",
    "                result[evtValueKey].addToSupportSet(seq)\n",
    "        #print(result.keys())\n",
    "        #print(result.values())\n",
    "        \n",
    "        \n",
    "        s=[]\n",
    "        #print(f'maxSup {maxSup}')\n",
    "        \n",
    "        for itr in result.values():\n",
    "            #print(itr.keyEvts)\n",
    "            if(itr.getSupport()>maxSup):\n",
    "                continue\n",
    "            s.append(itr)\n",
    "        \n",
    "        if not s:\n",
    "            return\n",
    "        \n",
    "        #for patterns in s:\n",
    "            #print(f'pat before sort {patterns.keyEvts}')\n",
    "        s=sorted(s, key= lambda x: x.getSupport())\n",
    "        \n",
    "        self.topRankedEvtValues=[]\n",
    "        \n",
    "        maxval= s[0].getSupport()\n",
    "        #print(f'maxval {maxval}')\n",
    "        \n",
    "        for patterns in s:\n",
    "            #print(f'pat {patterns.keyEvts}')\n",
    "            #print(f'support {patterns.getSupport()}')\n",
    "            if patterns.getSupport() < maxval:\n",
    "                break\n",
    "            self.topRankedEvtValues.append(patterns)\n",
    "        #print(len(s))\n",
    "        #print(len(self.topRankedEvtValues))\n",
    "        #for k in self.topRankedEvtValues:\n",
    "        #    print(f'key {k.keyEvts}')\n",
    "\n",
    "        #print(f'top ranked {*(k.keyEvts for k in self.topRankedEvtValues)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyMedianRankingFunction:\n",
    "    def __init__(self):\n",
    "        self.topRankedEvtValues=[]\n",
    "        self.evtAttr=\"\"\n",
    "        \n",
    "    def setEvtAttr(self, evtAttr):\n",
    "        self.evtAttr=evtAttr\n",
    "        #print(f'evtattr {self.evtAttr}')\n",
    "\n",
    "    def getTopEventSet(self):\n",
    "        if not self.topRankedEvtValues:\n",
    "            return None\n",
    "        elif len(self.topRankedEvtValues)==1:\n",
    "            return self.topRankedEvtValues[0]\n",
    "        else:\n",
    "            #for k in self.topRankedEvtValues:\n",
    "            #    print(f'top key {k.keyEvts}')\n",
    "\n",
    "            for p in self.topRankedEvtValues:\n",
    "                p.computePatternStats(self.evtAttr)\n",
    "            \n",
    "            #for k in self.topRankedEvtValues:\n",
    "            #    print(f'sorted key {k.keyEvts}')\n",
    "\n",
    "            self.topRankedEvtValues=sorted(self.topRankedEvtValues, key=lambda x: x.getEventMedianPos()[0])\n",
    "        \n",
    "        return self.topRankedEvtValues[0]\n",
    "    \n",
    "    def performRanking(self, seqs, maxSup, excludedEvts):\n",
    "        result={}\n",
    "        \n",
    "        evtValueKey=\"\"\n",
    "        uniqueHashes=[]\n",
    "        \n",
    "        for seq in seqs:\n",
    "            # get hashlist for each individual sequence\n",
    "            uniqueHashes= seq.getUniqueValueHashes(self.evtAttr)\n",
    "            #print(f'evthash {evtHashes}')\n",
    "            for hashval in uniqueHashes:\n",
    "                \n",
    "                if hashval in excludedEvts:\n",
    "                    continue\n",
    "                evtValueKey=str(hashval)\n",
    "                \n",
    "                #create a pattern for all hash values\n",
    "                if evtValueKey  not in result.keys():\n",
    "                    #print(f'evtValueKey {evtValueKey}')\n",
    "                    p=Pattern([evtValueKey])\n",
    "                    #p.addKeyEvent(hashval)\n",
    "                    result[evtValueKey]=p\n",
    "                    \n",
    "                result[evtValueKey].addToSupportSet(seq)\n",
    "        #print(result.keys())\n",
    "        #print(result.values())\n",
    "        \n",
    "        \n",
    "        s=[]\n",
    "        #print(f'maxSup {maxSup}')\n",
    "        \n",
    "        for itr in result.values():\n",
    "            #print(itr.keyEvts)\n",
    "            if(itr.getSupport()>maxSup):\n",
    "                continue\n",
    "            s.append(itr)\n",
    "        \n",
    "        if not s:\n",
    "            return\n",
    "        \n",
    "        #for patterns in s:\n",
    "            #print(f'pat before sort {patterns.keyEvts}')\n",
    "        s=sorted(s, key= lambda x: x.getSupport(), reverse=True)\n",
    "        \n",
    "        self.topRankedEvtValues=[]\n",
    "        \n",
    "        maxval= s[0].getSupport()\n",
    "        #print(f'maxval {maxval}')\n",
    "        \n",
    "        for patterns in s:\n",
    "            #print(f'pat {patterns.keyEvts}')\n",
    "            #print(f'support {patterns.getSupport()}')\n",
    "            if patterns.getSupport() < maxval:\n",
    "                break\n",
    "            self.topRankedEvtValues.append(patterns)\n",
    "        #print(len(s))\n",
    "        #print(len(self.topRankedEvtValues))\n",
    "        #for k in self.topRankedEvtValues:\n",
    "        #    print(f'key {k.keyEvts}')\n",
    "\n",
    "        #print(f'top ranked {*(k.keyEvts for k in self.topRankedEvtValues)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreflow Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreFlowMiner:\n",
    "    rf=FrequencyMedianRankingFunction()\n",
    "    # Implement CoreFlow algo which takes a list of sequences, a TreeNode (root), and a bunch of CoreFlow parameters \n",
    "\n",
    "    def __init__(self):\n",
    "        self.branchSequences={}\n",
    "        \n",
    "    \n",
    "    def checkForStop(seqs, minval, checkpoints):\n",
    "        pass\n",
    "    \n",
    "    def adjustMin(seqs, minval):\n",
    "        if minval<50 :\n",
    "            return minval\n",
    "        \n",
    "        while(Sequence.getSeqVolume(seqs)<minval and minval>50):\n",
    "            minval=minval/2\n",
    "        \n",
    "        return minval\n",
    "    \n",
    "    def bundleToExit(self, seqs, parent, attr, exitNodeHash):\n",
    "        if len(seqs)==0:\n",
    "            return\n",
    "        \n",
    "        node=TreeNode()\n",
    "        \n",
    "        if exitNodeHash==-1:\n",
    "            node.setName(\"Exit\")\n",
    "            node.setValue(\"Exit\")\n",
    "            node.setHash(-1)\n",
    "            \n",
    "        else:\n",
    "            node.setName(str(Sequence.getEvtAttrValue(attr,exitNodeHash)))\n",
    "            \n",
    "            #set attribute value for this sequence\n",
    "            node.setValue(Sequence.getEvtAttrValue(attr,exitNodeHash))\n",
    "            node.setHash(exitNodeHash)\n",
    "        \n",
    "        node.setIncomingSequences(seqs, attr)\n",
    "        node.setSeqCount(Sequence.getSeqVolume(seqs))\n",
    "        print(f'exit node seq count {node.seqCount}')\n",
    "        lengths=[]\n",
    "        for s in seqs:\n",
    "            for i in range(0,s.getVolume()):\n",
    "                lengths.append(len(s.events)-1)\n",
    "        node.setPositions(lengths)\n",
    "        parent.children.append(node)\n",
    "        \n",
    "    #needs properimplementation    \n",
    "    def getNewRootNode(self, numPaths, seqlist):\n",
    "        return TreeNode(\"Start of all \"+ str(len(seqlist))+\" visits\", numPaths, \"-1\")\n",
    "    \n",
    "    \n",
    "    def truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain):\n",
    "        indices=[]\n",
    "        uniqueEvts=[]\n",
    "        relTimestamps=[]\n",
    "        incomingBranchSeqs=[]\n",
    "        \n",
    "        print(f'hashval {hashval}')\n",
    "        for seq in seqs:\n",
    "            i=seq.getEventPosition(evtAttr, hashval)\n",
    "            print(f'Position {i}')\n",
    "            if i<0:\n",
    "                notContain.append(seq)\n",
    "                print(f'not contain {seq.getHashList(evtAttr)}')\n",
    "            else:\n",
    "                if i>=1:\n",
    "                    incomingSeq= Sequence(seq.events[0:i])\n",
    "                    self.branchSequences[incomingSeq.getPathID()]= incomingSeq\n",
    "                    incomingSeq.setVolume(seq.getVolume())\n",
    "                    incomingBranchSeqs.append(incomingSeq)\n",
    "                    print(f'previous {incomingSeq.getHashList(evtAttr)}')\n",
    "                    uniqueEvts.extend(incomingSeq.getUniqueValueHashes(evtAttr))\n",
    "                    \n",
    "                if len(seq.events)>i+1:\n",
    "                    outgoingSeq= Sequence(seq.events[i+1:len(seq.events)])\n",
    "                    self.branchSequences[outgoingSeq.getPathID()]= outgoingSeq\n",
    "                    \n",
    "                    outgoingSeq.setVolume(seq.getVolume())\n",
    "                    trailingSeqSegs.append(outgoingSeq)\n",
    "                    print(f'next {outgoingSeq.getHashList(evtAttr)}')\n",
    "                    \n",
    "                for k in range(0, seq.getVolume()):\n",
    "                    indices.append(i)\n",
    "                relTimestamps.append(seq.events[i].timestamp-seq.events[0].timestamp)\n",
    "        #print(f'Time Stamp {relTimestamps}')\n",
    "        #print(f'unique {uniqueEvts}')\n",
    "        #print(f'unique {set(uniqueEvts)}')\n",
    "        #print(f'unique {len(set(uniqueEvts))}')\n",
    "                \n",
    "        node.setIncomingBranchUniqueEvts( len(set(uniqueEvts)) )\n",
    "        node.setSeqCount(Sequence.getSeqVolume(incomingBranchSeqs))\n",
    "        node.setPositions(indices)\n",
    "        node.setRelTimeStamps(relTimestamps)\n",
    "        node.setIncomingSequences(incomingBranchSeqs, evtAttr)\n",
    "        print(f'seq count {node.getSeqCount()}')\n",
    "        print(f' pos {node.pos}')\n",
    "        print(f'Seq len trailing {len(trailingSeqSegs)}')\n",
    "        print(f'Seq len not contain {len(notContain)}')\n",
    "\n",
    "            \n",
    "    def run(self, seqs, evtAttr, parent, minval, maxval, checkpoints, excludedEvts, exitNodeHash ):\n",
    "        if len(checkpoints)>0:\n",
    "            containSegs=[]\n",
    "            notContain=[]\n",
    "            \n",
    "            node= TreeNode()\n",
    "            \n",
    "            #First integer event\n",
    "            hashval=checkpoints[0]\n",
    "            print(f'hashval {hashval}')\n",
    "            eVal=Sequence.getEvtAttrValue(evtAttr, hashval)\n",
    "            print(f'eVal {eVal}')\n",
    "            node.setName(str(eVal)) #NOT sure\n",
    "            node.setValue(eVal)\n",
    "            node.setHash(hashval)\n",
    "            del checkpoints[0]\n",
    "            self.truncateSequences(seqs, hashval, evtAttr, node, containSegs, notContain)\n",
    "            \n",
    "            parent.children.append(node)\n",
    "            \n",
    "            self.run(containSegs, evtAttr, node, minval, maxval, checkpoints, excludedEvts, exitNodeHash)\n",
    "            self.run(notContain, evtAttr, parent, minval, maxval, checkpoints, excludedEvts, exitNodeHash)\n",
    "            \n",
    "        else:\n",
    "            #print(f'minval {minval}')\n",
    "            #print(f'Seq volume {Sequence.getSeqVolume(seqs)}')\n",
    "            if Sequence.getSeqVolume(seqs)<minval:\n",
    "                self.bundleToExit(seqs, parent, evtAttr, exitNodeHash)\n",
    "                return \n",
    "            \n",
    "            else:\n",
    "                self.rf.setEvtAttr(evtAttr)\n",
    "                #print(f'maxval {maxval}')\n",
    "                self.rf.performRanking(seqs, maxval, excludedEvts)\n",
    "                topPattern=self.rf.getTopEventSet()\n",
    "                print(f'topPattern {topPattern.keyEvts}')\n",
    "        \n",
    "                if topPattern is None:\n",
    "                    print(\"no patterns found\")\n",
    "                    self.bundleToExit(seqs, parent, evtAttr, exitNodeHash)\n",
    "                \n",
    "                containSegs=[]\n",
    "                notContain=[]\n",
    "                \n",
    "                node= TreeNode()\n",
    "                hashval=topPattern.getEvents()[0]\n",
    "                eVal=Sequence.getEvtAttrValue(evtAttr, hashval)\n",
    "                node.setName(str(eVal)) #NOT sure\n",
    "                node.setValue(eVal)\n",
    "                node.setHash(hashval)\n",
    "\n",
    "                self.truncateSequences(seqs, hashval, evtAttr, node, containSegs, notContain)\n",
    "                node.setSeqCount(Sequence.getSeqVolume(containSegs))\n",
    "                \n",
    "                if node.getSeqCount()>minval:\n",
    "                    parent.children.append(node)\n",
    "                    self.run(containSegs, evtAttr, node, minval, maxval, checkpoints, excludedEvts, exitNodeHash)\n",
    "                    self.run(notContain, evtAttr, parent, minval, maxval, checkpoints, excludedEvts, exitNodeHash)\n",
    "                \n",
    "                else:\n",
    "                    self.bundleToExit(seqs, parent, evtAttr, exitNodeHash)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.root=root\n",
    "        self.numnodes=0\n",
    "        self.uniqueNodes={}\n",
    "        self.outDegree=[]\n",
    "        self.depth=0\n",
    "    def run(currentnode, numnodes, uniquenodes,outdegree, depth):\n",
    "        numnodes=numnodes+1\n",
    "        uniquenodes[currentnode.getValue()]=True\n",
    "        outdegree.append(len(currentnode.children))\n",
    "        print(f'depth {depth}')\n",
    "        print(f'numnodes {numnodes}')\n",
    "        print(f'children {len(currentnode.children)}')\n",
    "            \n",
    "        for node in currentnode.children:\n",
    "            depth=depth+1\n",
    "            \n",
    "            TreeAnalyzer.run(node, numnodes, uniquenodes, outdegree, depth)\n",
    "        #self.numnodes+=1\n",
    "        #uniqueNodes[node.getValue()]=True\n",
    "        return numnodes, uniquenodes, outdegree, depth\n",
    "    def traverse(root, path):\n",
    "        path=\"{\\n\"\n",
    "        path+=root.name\n",
    "        for node in root.children:\n",
    "            TreeAnalyzer.traverse(node, path)\n",
    "        if len(root.children)==0:\n",
    "            print(path+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return a data frame\n",
    "# Local is boolean, if local then source should be path to the file\n",
    "# Otherwise it should be a URL to the the file\n",
    "def get_dataframe( src, local=False, sep=\"\\t\", header=[]):\n",
    "    if not local:\n",
    "        # To force a dropbox link to download change the dl=0 to 1\n",
    "        if \"dropbox\" in src:\n",
    "            src = src.replace('dl=0', 'dl=1')\n",
    "        # Download the CSV at url\n",
    "        req = requests.get(src)\n",
    "        url_content = req.content\n",
    "        csv_file = open('data.txt', 'wb') \n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()\n",
    "        # Read the CSV into pandas\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            df = pd.read_csv(\"data.txt\", sep)\n",
    "        #else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(\"data.txt\", sep, names=header)\n",
    "        # Delete the csv file\n",
    "        os.remove(\"data.txt\")\n",
    "        return df\n",
    "    # Dataset is local\n",
    "    else:\n",
    "        # If header list is empty, the dataset provides header so ignore param\n",
    "        if not header:\n",
    "            print(src)\n",
    "            df = pd.read_csv(src, sep)\n",
    "        # else use header param for column names\n",
    "        else:\n",
    "            df = pd.read_csv(src, sep, names=header)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# Helper function for generateSequence to use when sorting events to get what time field to sort by\n",
    "# Also used in splitSequences to give the time of an event when splitting the events up\n",
    "\n",
    "def get_time_to_sort_by(e):\n",
    "    # Sort by starting time of event if its an interval event\n",
    "    if type(e) == IntervalEvent:\n",
    "        return e.time[0]\n",
    "    # Otherwise use the timestamp\n",
    "    else:\n",
    "        return e.timestamp\n",
    "\n",
    "\n",
    "    \n",
    "# Helper to insert an event into a map\n",
    "# Params are key=unique id for that time, map of key to event list, event object\n",
    "def insert_event_into_dict(key, dictionary, event):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(event)\n",
    "    else:\n",
    "        dictionary[key] = [event]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing events functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    #should be moved to EventStore\n",
    "    # hold the list of events, also the dictionaries\n",
    "    \n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # header is list of column names if they are not provided in the dataset\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    @staticmethod\n",
    "    def importPointEvents(src, timestampColumnIdx, timeFormat, sep='\\t', local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            timestamp = datetime.strptime(data[timestampColumnIdx], timeFormat)\n",
    "            # for all attributes other tahn time, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != timestampColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = PointEvent(timestamp, attribs)\n",
    "            events.append(e)\n",
    "        sequence=Sequence(events)\n",
    "        EventStore.create_attr_dict(sequence)\n",
    "        return sequence\n",
    "\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    # for those cases\n",
    "    @staticmethod\n",
    "    def importIntervalEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for the start and end times of the event\n",
    "            t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "            t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            for i in range(len(data)):\n",
    "                if i != startTimeColumnIdx and i != endTimeColumnIdx:\n",
    "                    attribs[cols[i]] = data[i]\n",
    "            # use time stamp and attributes map to construct event object\n",
    "            e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        sequence=Sequence(events)\n",
    "        EventStore.create_attr_dict(sequence)\n",
    "        return sequence\n",
    "\n",
    "    # Import a dataset that has both interval and point events\n",
    "    # Returns a list of event objects\n",
    "    # src is a url or directory path, if local is false its url else its path\n",
    "    # The foursquare datasets are all using a differnet encoding that pandas cannot auto identify so for those\n",
    "    # I thought the simplest thing was just to give this function the df and then use that instead of calling my helper\n",
    "    @staticmethod\n",
    "    def importMixedEvents(src, startTimeColumnIdx, endTimeColumnIdx, timeFormat, sep=\"\\t\", local=False, header=[], df=None):\n",
    "        events = []\n",
    "        # if the df is not provided\n",
    "        if df is None:\n",
    "            df = get_dataframe(src, local, sep, header)\n",
    "        cols = df.columns\n",
    "        # For each event in the csv construct an event object\n",
    "        for row in df.iterrows():\n",
    "            data = row[1]\n",
    "            attribs = {}\n",
    "            # create datetime object for timestamp (if point events) or t1 and t2 (if interval event)\n",
    "            # If the endTimeColumnIdx value is NaN ie a float instead of a time string then its a point event\n",
    "            if type(data[endTimeColumnIdx]) is float:\n",
    "                t = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                event_type = \"point\"\n",
    "            # Otherwise its an interval event\n",
    "            else:\n",
    "                t1 = datetime.strptime(data[startTimeColumnIdx], timeFormat)\n",
    "                t2 = datetime.strptime(data[endTimeColumnIdx], timeFormat)\n",
    "                event_type = \"interval\"\n",
    "            # for all attributes other than times, add them to attributes dict\n",
    "            ignore=[startTimeColumnIdx, endTimeColumnIdx] # list of indices to be ignored\n",
    "            attribute_columns = [ind for ind in range(len(data)) if ind not in ignore]\n",
    "            for i in attribute_columns:\n",
    "                attribs[cols[i]] = data[i]\n",
    "            # use time stamp (or t1 and t2) and attributes map to construct event object\n",
    "            if event_type == \"point\":\n",
    "                e = PointEvent(t, attribs)\n",
    "            else:\n",
    "                e = IntervalEvent(t1, t2, attribs)\n",
    "            events.append(e)\n",
    "        sequence=Sequence(events)\n",
    "        EventStore.create_attr_dict(sequence)\n",
    "        return sequence\n",
    "\n",
    "    #should take an eventlist as input\n",
    "    # Group events by attributeName, and order them by timestamp\n",
    "    @staticmethod\n",
    "    #should return a list of sequences\n",
    "    def generateSequence(sequence, attributeName):\n",
    "        eventList=sequence.events\n",
    "        grouped_by = {}\n",
    "        # Sort the event list\n",
    "        eventList = sorted(eventList, key=get_time_to_sort_by)\n",
    "        for event in eventList:\n",
    "            value = event.attributes[attributeName]\n",
    "            # If have seen this value before, append it the list of events in grouped_by for value\n",
    "            if value in grouped_by:\n",
    "                grouped_by[value].append(event)\n",
    "            # otherwise store a new list with just that event\n",
    "            else:\n",
    "                grouped_by[value] = [event]\n",
    "        return list(grouped_by.values())\n",
    "    \n",
    "    # Split a long sequence into shorter ones by timeUnit. For example, a sequence may span several days and we want to \n",
    "    # break it down into daily sequences. The argument timeUnit can be one of the following strings: “hour”, “day”, \n",
    "    # “week”, “month”, “quarter”, and “year”.\n",
    "    # For interval events I used the start time of the event to determine its category when splitting it\n",
    "    \n",
    "    #ZINAT- changes\n",
    "    #SequenceList represents a list of objects of type Sequence. The sequences are further splitted into\n",
    "    #sequence objects, this way we can use generate sequences and then splitSequences \n",
    "    @staticmethod\n",
    "    def splitSequences(sequenceLists, timeUnit, record=None):\n",
    "        if not isinstance(sequenceLists, list):\n",
    "            sequenceLists=[sequenceLists]\n",
    "        results = []\n",
    "        resultlist=[]\n",
    "        timeUnit = timeUnit.lower()\n",
    "        # Check if the time unit is a valid argument\n",
    "        valid_time_units = [\"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"]\n",
    "        if timeUnit not in valid_time_units:\n",
    "            raise ValueError(\"timeUnit must be hour, day, week, month, quarter, or year\")\n",
    "        \n",
    "        for sequence in sequenceLists:\n",
    "            # Sort the events by the timestamp or event start time\n",
    "            sequenceList= sequence.events\n",
    "            sequenceList = sorted(sequenceList, key=get_time_to_sort_by)\n",
    "\n",
    "            # Process the event sequence based on the given time unit\n",
    "            # Generally, create a map for that time unit and then add each event into that map \n",
    "            # (key=time such as May 2021 in case of month, value=sequence) and then return the values of the map as a list\n",
    "            if timeUnit == \"hour\":\n",
    "                hours = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.hour, time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,hours,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=' '.join([str(k) for k in key])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+' '.join([str(k) for k in key])\n",
    "                results = list(hours.values())\n",
    "\n",
    "            elif timeUnit == \"day\":\n",
    "                days = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.day, time.month, time.year)\n",
    "                    insert_event_into_dict(key,days,event)\n",
    "                    #print(days)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+datetime(*(key[::-1])).strftime(\"%Y%m%d\")\n",
    "                results = list(days.values())\n",
    "\n",
    "            elif timeUnit == \"month\":\n",
    "                months = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = (time.month,time.year)\n",
    "                    insert_event_into_dict(key,months,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(months.values())\n",
    "\n",
    "            elif timeUnit == \"week\":\n",
    "                weeks = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    week_num = time.isocalendar()[1]\n",
    "                    key = (year,week_num)\n",
    "                    insert_event_into_dict(key,weeks,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+\"W\"+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+\"W\"+str(key[1])\n",
    "                results = list(weeks.values())\n",
    "\n",
    "            elif timeUnit == \"year\":\n",
    "                years = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    key = time.year\n",
    "                    insert_event_into_dict(key,years,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key)\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key)\n",
    "                results = list(years.values())\n",
    "\n",
    "            elif timeUnit == \"quarter\":\n",
    "                quarters = {}\n",
    "                for event in sequenceList:\n",
    "                    time = get_time_to_sort_by(event)\n",
    "                    year = time.year\n",
    "                    month = time.month\n",
    "                    # Determine the year, quarter pair/key for quarter dict\n",
    "                    # January, February, and March (Q1)\n",
    "                    if month in range(1, 4):\n",
    "                        key = (year, \"Q1\")\n",
    "                    # April, May, and June (Q2)\n",
    "                    elif month in range(4, 7):\n",
    "                        key = (year, \"Q2\")\n",
    "                    # July, August, and September (Q3)\n",
    "                    elif month in range(7,10):\n",
    "                        key = (year, \"Q3\")\n",
    "                    # October, November, and December (Q4)\n",
    "                    elif month in range(10,13):\n",
    "                        key = (year, \"Q4\")\n",
    "                    # Put the event in the dictionary\n",
    "                    insert_event_into_dict(key,quarters,event)\n",
    "                    if record is None:\n",
    "                        event.attributes[\"record\"]=str(key[0])+str(key[1])\n",
    "                    else:\n",
    "                        event.attributes[record]=str(event.attributes[record])+\"_\"+str(key[0])+str(key[1])\n",
    "                results = list(quarters.values())\n",
    "            resultlist.extend(results)\n",
    "        resultlists= [Sequence(x) for x in resultlist]\n",
    "\n",
    "        return resultlists\n",
    "    \n",
    "    #Assuming we are given a list of events and from those events we create \n",
    "    #the mapping and reverse mapping dictionary\n",
    "    def create_attr_dict(seqList):\n",
    "        attr_list=seqList.events[0].attributes.keys()\n",
    "        print(attr_list)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            a=48\n",
    "            unique_list=[]\n",
    "            unique_list.extend(seqList.getUniqueValues(attr))\n",
    "            unique_list=list(set(unique_list))\n",
    "            #unique_list.clear()\n",
    "            \n",
    "            unicode_dict={}\n",
    "            reverse_dict={}\n",
    "            for uniques in unique_list:\n",
    "                unicode_dict[uniques]=chr(a)\n",
    "                reverse_dict[chr(a)]=uniques\n",
    "                a=a+1\n",
    "            Sequence.attrdict[attr]=unicode_dict\n",
    "            Sequence.reverseatttrdict[attr]=reverse_dict\n",
    "            #unicode_dict.clear()                    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Aggregation\n",
    "For aggregateEventsRegex and aggregateEventsDict, see what the files are expected to look like in the repo in DataModel/testFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run the mappings file as a dictionary\n",
    "def give_dictionary_of_mappings_file(fileName):\n",
    "    # Open the file and split the contents on new lines\n",
    "    file = open(fileName, \"r\")\n",
    "    mappings = file.read().split(\"\\n\")\n",
    "    file.close()\n",
    "    # Remove any empty strings from the list of mappings\n",
    "    mappings = list(filter(None, mappings))\n",
    "    # Raise an error if there is an odd number of items in mapping\n",
    "    if (len(mappings) % 2) != 0:\n",
    "        raise ValueError(\"There must be an even number of lines in the mappings file.\")\n",
    "    # Create a dictionary based on read in mappings\n",
    "    aggregations = {}\n",
    "    for i in range(len(mappings)):\n",
    "        if i % 2 == 0:\n",
    "            aggregations[mappings[i]] = mappings[i+1]\n",
    "    #print(aggregations)\n",
    "    return aggregations\n",
    "\n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by rules expressed in regular expressions. For example, in the highway incident dataset, we can \n",
    "# replace all events with the pattern “CHART Unit [number] departed” by “CHART Unit departed”. The argument \n",
    "# regexMapping can be a path pointing to a file defining such rules. We can assume each rule occupies two lines: \n",
    "# first line is the regular expression, second line is the merged event name \n",
    "def aggregateEventsRegex(eventList, regexMapping, attributeName): \n",
    "    aggregations = give_dictionary_of_mappings_file(regexMapping)\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # For all the regexes\n",
    "        for regex in aggregations.keys():\n",
    "            # If its a match then replace the attribute value for event with\n",
    "            if re.match(regex, attribute_val):\n",
    "                event.attributes[attributeName] = aggregations[regex]\n",
    "                break\n",
    "    return eventList\n",
    "    \n",
    "# NOTE: this current modifies the events in eventList argument\n",
    "# merge events by a dictionary mapping an event name to the merged name. The argument nameDict can be a path \n",
    "# pointing to a file defining such a dictionary. We can assume each mapping occupies two lines: first line is the \n",
    "# original name, second line is the merged event name.    \n",
    "def aggregateEventsDict(eventList, nameDict, attributeName):\n",
    "    aggregations = give_dictionary_of_mappings_file(nameDict)\n",
    "    # Iterate over all events and replace evevnts in event list with updated attribute name\n",
    "    # if directed to by given mappings\n",
    "    for event in eventList:\n",
    "        # Get the attribute value of interest\n",
    "        attribute_val = event.attributes[attributeName]\n",
    "        # If the attribute value has a mapping then replace the event's current value with the one in give map\n",
    "        if attribute_val in aggregations:\n",
    "            \n",
    "            event.attributes[attributeName] = aggregations[attribute_val]\n",
    "    return eventList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_braiding = EventStore.importPointEvents('../datasets/sequence_braiding_refined.csv', 0, \"%m/%d/%y\", sep=',', local=True)\n",
    "#print(type(sequence_braiding))\n",
    "seq=sequence_braiding\n",
    "#Sequence.create_attr_dict([seq])\n",
    "#seq.getEventPosition('Meal','Lunch')\n",
    "#print(seq.getUniqueValueHashes('Meal'))\n",
    "#print(seq.getHashList('Glucose'))\n",
    "print(seq.getValueHashes('Glucose'))\n",
    "#print(seq.getEventsHashString('Glucose'))\n",
    "raw_seq=seq.convertToVMSPReadable('Meal')\n",
    "print(seq.convertToVMSPReadable('Glucose'))\n",
    "#print(seq.getPathID())\n",
    "#sequence_braiding[0].attributes.keys()\n",
    "#print(sequence_braiding[0].getAttrVal('Meals'))\n",
    "#print(sequence_braiding[0].type)\n",
    "#for events in sequence_braiding:\n",
    "#    print(events.getAttrVal('Meal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list=EventStore.splitSequences(sequence_braiding, \"week\")\n",
    "#seq_list=[]\n",
    "#for seqs in sequence_braiding_split:\n",
    "#    seq_list.append(Sequence(seqs))\n",
    "    \n",
    "#Sequence.create_attr_dict(seq_list)\n",
    "raw_seq=\"\\n\".join( seqs.convertToVMSPReadable('Meal') for seqs in seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=Pattern(['2','6'])\n",
    "print(pat.keyEvts)\n",
    "s=pat.filterPaths(seq_list, 'Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=Pattern([233,309,106,166])\n",
    "print(pat.keyEvts)\n",
    "#print(pat.filterPaths([seq],'Glucose'))\n",
    "#print(pat.getUniqueEventsString())\n",
    "print(pat.getPositions([233,309,80,168],seq.getValueHashes('Glucose')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example_raw = \"\"\"1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2\n",
    "1 4 -1 3 -1 2 3 -1 1 5 -1 -2\n",
    "5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2\n",
    "5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf = Spmf(\"VMSP\", spmf_bin_location_dir=\"../../Tools/coreflow/CoreFlow-backend-src/src/datastructure_python/test_files/\", input_direct=raw_seq,\n",
    "           input_type=\"text\", output_filename=\"output.txt\", arguments=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spmf.to_pandas_dataframe(pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm= CoreFlowMiner()\n",
    "root=cfm.getNewRootNode(Sequence.getSeqVolume(seq_list), seq_list)\n",
    "cfm.run(seq_list, \"Meal\", root, 5 * Sequence.getSeqVolume(seq_list)/100.0, Sequence.getSeqVolume(seq_list), [], {}, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currentnode, numnodes, uniquenodes,outdegree, depth\n",
    "w,x,y,z= TreeAnalyzer.run(root,0,{},[],0)\n",
    "print(w)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=Pattern(['3','0'])\n",
    "print(pat.keyEvts)\n",
    "s=pat.filterPaths(seq_sublist, 'Meal')\n",
    "pat.computePatternStats('Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pat.medianPos)\n",
    "print(pat.meanPos)\n",
    "        \n",
    "print(pat.medianPathLength)\n",
    "print(pat.meanPathLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[0,1,5]\n",
    "seq_sublist=[seq_list[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm= CoreFlowMiner()\n",
    "#cfm.truncateSequences(self, seqs, hashval, evtAttr, node,trailingSeqSegs, notContain)\n",
    "root=cfm.getNewRootNode(Sequence.getSeqVolume(seq_sublist), seq_sublist)\n",
    "cfm.truncateSequences(seq_sublist, '3', 'Meal', root,[], [])\n",
    "#cfm.run(seq_sublist, \"Meal\", cfm.getNewRootNode(Sequence.getSeqVolume(seq_sublist), seq_sublist), 5 * Sequence.getSeqVolume(seq_sublist)/100.0, Sequence.getSeqVolume(seq_sublist), [], {}, -1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm.run(seq_sublist, \"Meal\", root, 5 * Sequence.getSeqVolume(seq_sublist)/100.0, Sequence.getSeqVolume(seq_sublist), [], {}, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeAnalyzer.traverse(root, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
