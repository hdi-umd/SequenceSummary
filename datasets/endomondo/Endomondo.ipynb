{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_values = set()\n",
    "unique_sport_values= set()\n",
    "unique_id_values=set()\n",
    "unique_url_values=set()\n",
    "\n",
    "user_list=[]\n",
    "workout_list=[]\n",
    "workout_details_list=None\n",
    "#device_data = [{k: v for k, v in x.items() if k in ['type','id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute(data, attribute, default_value):\n",
    "    return data.get(attribute) or default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Endomodo/endomondoHR_proper.json\", encoding='latin1') as json_file:\n",
    "    count=0\n",
    "    for line_number, line in enumerate(json_file):\n",
    "        \n",
    "        #print (\"Processing line\", line_number + 1,\"at cursor index:\", cursor)\n",
    "        #line=ast.literal_eval(line)\n",
    "        line = line.replace(\"\\'\", \"\\\"\")\n",
    "        data = json.loads(line)\n",
    "        unique_sport_values.add(data['sport'])\n",
    "        unique_url_values.add(data['url'])\n",
    "        \n",
    "        if(data['userId']) not in unique_user_values:\n",
    "            user_list.append({k: v for k, v in data.items() if k in ['userId','gender']})\n",
    "        if(data['id']) not in unique_id_values:\n",
    "            workout_list.append({k: v for k, v in data.items() if k in ['id','userId','sport','url']})\n",
    "        #count=line_number    \n",
    "        unique_user_values.add(data['userId'])\n",
    "        unique_id_values.add(data['id'])\n",
    "        #print(len(data['timestamp']))\n",
    "        #print(data['speed'])\n",
    "        workout_id=np.repeat(data['id'],len(data['timestamp']))\n",
    "        speed = get_attribute(data, 'speed', np.repeat(0,len(data['timestamp'])))\n",
    "        #print(speed)\n",
    "        workout_details=np.column_stack((workout_id,data['timestamp'],data['altitude'],data['longitude'],data['latitude'],data['heart_rate'],speed))\n",
    "        if workout_details_list is None:\n",
    "            workout_details_list=workout_details\n",
    "        else:\n",
    "            workout_details_list=np.vstack((workout_details_list,workout_details))\n",
    "        #print(workout_details)\n",
    "print(unique_sport_values)\n",
    "print(count)\n",
    "user_df=pd.DataFrame(user_list)\n",
    "work_out_df=pd.DataFrame(workout_list)\n",
    "        #print(data)\n",
    "        #print(data.keys())\n",
    "        #line_as_file = io.StringIO(line)\n",
    "        # Use a new parser for each line\n",
    "        #json_parser = ijson.parse(line_as_file)\n",
    "        #for prefix, type, value in json_parser:\n",
    "        #    print (\"prefix=\",prefix, \"type=\",type, \"value=\",value)\n",
    "        #cursor += len(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "work_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_id_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_user_values)\n",
    "print(len(unique_sport_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "temp=(work_out_df[['userId',\"id\"]].groupby('userId').count())\n",
    "print(temp.head())\n",
    "print(temp.iloc[[temp[\"id\"].argmax()]])\n",
    "\n",
    "print(temp.iloc[[temp[\"id\"].argmin()]])\n",
    "\n",
    "print(temp[\"id\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_out_df.loc[work_out_df['userId'] ==2734298 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(\"Endomodo/endomondoHR_proper.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Endomodo/endomondoHR_proper.json\",'rb') as json_file:      \n",
    "    data = json_file.readlines()\n",
    "    data = data.replace(\"\\'\", \"\\\"\")\n",
    "    # this line below may take at least 8-10 minutes of processing for 4-5 million rows. It converts all strings in list to actual json objects.\n",
    "    print(data)\n",
    "    data = list(map(json.loads, data)) \n",
    "#data = json.load(open(\"Endomodo/processed_endomondoHR_proper.npy\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.memmap(\"Endomodo/processed_endomondoHR_proper.npy\",  mode='r'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Endomondo/\") as json_file:      \n",
    "    data = json_file.readlines()\n",
    "    # this line below may take at least 8-10 minutes of processing for 4-5 million rows. It converts all strings in list to actual json objects. \n",
    "    data = list(map(json.loads, data)) \n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\", encoding='latin1',delimiter = \"\\t\", header=None, names=[\"User ID\", \"Venue ID\", \"UTC Time\", \"Timezone offset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Venue ID\"].unique())\n",
    "print(df[\"Venue ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"User ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UTC Time']=pd.to_datetime(df['UTC Time'], errors=\"coerce\", infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['User ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['User ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "temp=(df[['User ID',\"Venue ID\"]].groupby('User ID').count())\n",
    "print(temp.head())\n",
    "print(temp.iloc[[temp[\"Venue ID\"].argmax()]])\n",
    "\n",
    "print(temp.iloc[[temp[\"Venue ID\"].argmin()]])\n",
    "\n",
    "print(temp[\"Venue ID\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df[['User ID','UTC Time']]\n",
    "temp['diff']=temp.sort_values(['UTC Time']).groupby('User ID')['UTC Time'].diff()\n",
    "#diff=temp.sort_values(['UTC Time']).groupby('User ID')['UTC Time'].diff()\n",
    "#print(temp.sort_values(['UTC Time']).groupby('User ID')['UTC Time'].diff().head())\n",
    "#temp=temp['UTC Time'].diff()\n",
    "#temp.apply(print)\n",
    "#temp['UTC Time'].diff()\n",
    "temp = temp.dropna(subset=['diff'])\n",
    "#print(temp.head())\n",
    "#print(temp.shape)\n",
    "print(temp['diff'].argmin())\n",
    "print(temp['diff'].min())\n",
    "print(temp['diff'].argmax())\n",
    "print(temp['diff'].max())\n",
    "print(temp['diff'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[\"User ID\"].unique():\n",
    "    #print(df.loc[df.Case==i])\n",
    "    print(df.loc[df[\"User ID\"]==i][[\"Venue ID\"]].duplicated().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install ijson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit8e74114c351544819777b51aa88f0aa0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
